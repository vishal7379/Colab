{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 1: SYNTHETIC NL → SQL DATASET (LARGE + CORRECT)\n",
        "# ============================================================\n",
        "\n",
        "import random\n",
        "import json\n",
        "\n",
        "TABLE_SCHEMAS = {\n",
        "    \"employee\": [\"id\", \"name\", \"salary\", \"department\", \"age\"],\n",
        "    \"orders\": [\"order_id\", \"customer_id\", \"amount\", \"order_date\"],\n",
        "    \"customer\": [\"customer_id\", \"name\", \"city\"]\n",
        "}\n",
        "\n",
        "AGGS = [\"SUM\", \"AVG\", \"COUNT\", \"MAX\", \"MIN\"]\n",
        "OPS = [\">\", \"<\", \"=\"]\n",
        "\n",
        "def select_all():\n",
        "    table = random.choice(list(TABLE_SCHEMAS))\n",
        "    return (\n",
        "        f\"show all {table}\",\n",
        "        f\"SELECT * FROM {table}\",\n",
        "        {table: TABLE_SCHEMAS[table]}\n",
        "    )\n",
        "\n",
        "def where_clause():\n",
        "    table = \"employee\"\n",
        "    val = random.choice([30000, 50000, 70000])\n",
        "    op = random.choice(OPS)\n",
        "    return (\n",
        "        f\"show employees where salary {op} {val}\",\n",
        "        f\"SELECT * FROM employee WHERE salary {op} {val}\",\n",
        "        {\"employee\": TABLE_SCHEMAS[\"employee\"]}\n",
        "    )\n",
        "\n",
        "def group_by():\n",
        "    agg = random.choice(AGGS)\n",
        "    return (\n",
        "        f\"{agg.lower()} salary per department\",\n",
        "        f\"SELECT department, {agg}(salary) FROM employee GROUP BY department\",\n",
        "        {\"employee\": TABLE_SCHEMAS[\"employee\"]}\n",
        "    )\n",
        "\n",
        "generators = [select_all, where_clause, group_by]\n",
        "\n",
        "def generate_dataset(n=50000):\n",
        "    data = []\n",
        "    for _ in range(n):\n",
        "        q, sql, schema = random.choice(generators)()\n",
        "        data.append({\n",
        "            \"question\": q,\n",
        "            \"sql\": sql,\n",
        "            \"schema\": schema\n",
        "        })\n",
        "    return data\n",
        "\n",
        "dataset = generate_dataset(50000)\n",
        "\n",
        "with open(\"custom_nl2sql.json\", \"w\") as f:\n",
        "    json.dump(dataset, f, indent=2)\n",
        "\n",
        "print(\"✅ Dataset size:\", len(dataset))\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "id": "DXIWKpsHmPZB",
        "outputId": "cb039230-3438-4161-bd78-9303d4707351",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset size: 50000\n",
            "{'question': 'show all orders', 'sql': 'SELECT * FROM orders', 'schema': {'orders': ['order_id', 'customer_id', 'amount', 'order_date']}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 2: PREPROCESS DATASET\n",
        "# ============================================================\n",
        "\n",
        "import re\n",
        "\n",
        "def normalize(text):\n",
        "    return re.sub(r\"\\s+\", \" \", text.lower()).strip()\n",
        "\n",
        "processed = []\n",
        "\n",
        "for item in dataset:\n",
        "    schema_text = \"\"\n",
        "    for table, cols in item[\"schema\"].items():\n",
        "        schema_text += f\"table {table} columns: {', '.join(cols)} ; \"\n",
        "\n",
        "    encoder_input = schema_text + \" question: \" + normalize(item[\"question\"])\n",
        "    decoder_output = normalize(item[\"sql\"])\n",
        "\n",
        "    processed.append({\n",
        "        \"encoder_input\": encoder_input,\n",
        "        \"decoder_output\": decoder_output\n",
        "    })\n",
        "\n",
        "with open(\"processed_data.json\", \"w\") as f:\n",
        "    json.dump(processed, f, indent=2)\n",
        "\n",
        "print(\"✅ Example:\")\n",
        "print(processed[0])\n"
      ],
      "metadata": {
        "id": "ohx8WxbzmPfA",
        "outputId": "7461013b-0c79-490f-d8ef-9481273fcb32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Example:\n",
            "{'encoder_input': 'table orders columns: order_id, customer_id, amount, order_date ;  question: show all orders', 'decoder_output': 'select * from orders'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 3: BUILD VOCAB\n",
        "# ============================================================\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(sentences, min_freq=1):\n",
        "    counter = Counter()\n",
        "    for s in sentences:\n",
        "        counter.update(s.split())\n",
        "\n",
        "    vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "encoder_texts = [x[\"encoder_input\"] for x in processed]\n",
        "decoder_texts = [x[\"decoder_output\"] for x in processed]\n",
        "\n",
        "src_vocab = build_vocab(encoder_texts)\n",
        "tgt_vocab = build_vocab(decoder_texts)\n",
        "\n",
        "print(\"✅ Encoder vocab:\", len(src_vocab))\n",
        "print(\"✅ Decoder vocab:\", len(tgt_vocab))\n"
      ],
      "metadata": {
        "id": "8y5reQzqmPgr",
        "outputId": "1a0078dc-2f63-4568-f04f-e379cf5bdd5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Encoder vocab: 39\n",
            "✅ Decoder vocab: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 4: DATASET\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "    def __init__(self, data, src_vocab, tgt_vocab, max_len=60):\n",
        "        self.data = data\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def encode(self, text, vocab):\n",
        "        tokens = text.split()\n",
        "        ids = [vocab.get(t, vocab[\"<unk>\"]) for t in tokens]\n",
        "        ids = ids[:self.max_len]\n",
        "        return ids + [vocab[\"<pad>\"]] * (self.max_len - len(ids))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.encode(self.data[idx][\"encoder_input\"], self.src_vocab)\n",
        "        tgt = [self.tgt_vocab[\"<sos>\"]] + \\\n",
        "              self.encode(self.data[idx][\"decoder_output\"], self.tgt_vocab) + \\\n",
        "              [self.tgt_vocab[\"<eos>\"]]\n",
        "\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "dataset = NL2SQLDataset(processed, src_vocab, tgt_vocab)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "x, y = next(iter(loader))\n",
        "print(x.shape, y.shape)\n"
      ],
      "metadata": {
        "id": "XIhQNMKJmPh3",
        "outputId": "2014c720-b59e-4c86-db2d-689b8d24f2a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 60]) torch.Size([32, 62])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 5: ENCODER\n",
        "# ============================================================\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        outputs, hidden = self.rnn(emb)\n",
        "        return outputs, hidden\n"
      ],
      "metadata": {
        "id": "VUQQRr6Wm-BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), 2)))\n",
        "        scores = self.v(energy).squeeze(2)\n",
        "        weights = torch.softmax(scores, dim=1)\n",
        "        context = torch.bmm(weights.unsqueeze(1), encoder_outputs)\n",
        "        return context.squeeze(1)\n"
      ],
      "metadata": {
        "id": "jcOTgL6QnDSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.attn = Attention(hidden_dim)\n",
        "        self.rnn = nn.GRU(emb_dim + hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, encoder_outputs):\n",
        "        emb = self.embedding(x.unsqueeze(1))\n",
        "        context = self.attn(hidden.squeeze(0), encoder_outputs)\n",
        "        rnn_input = torch.cat((emb, context.unsqueeze(1)), 2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        return self.fc(output.squeeze(1)), hidden\n"
      ],
      "metadata": {
        "id": "-J7avgoPnGFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 8: TRAIN\n",
        "# ============================================================\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "encoder = Encoder(len(src_vocab)).to(device)\n",
        "decoder = Decoder(len(tgt_vocab)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(encoder.parameters()) + list(decoder.parameters()),\n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        enc_out, hidden = encoder(src)\n",
        "        dec_input = tgt[:, 0]\n",
        "\n",
        "        loss = 0\n",
        "        for t in range(1, tgt.size(1)):\n",
        "            output, hidden = decoder(dec_input, hidden, enc_out)\n",
        "            loss += criterion(output, tgt[:, t])\n",
        "            dec_input = tgt[:, t]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss:\", total_loss / len(loader))\n"
      ],
      "metadata": {
        "id": "BnrCaY0gnIfa",
        "outputId": "d5a8d616-776d-4cdc-bfd7-063890c93e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1750031525.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 1: SYNTHETIC NL → SQL DATASET\n",
        "# ============================================================\n",
        "\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "TABLE_SCHEMAS = {\n",
        "    \"employee\": [\"id\", \"name\", \"salary\", \"department\", \"age\"],\n",
        "    \"orders\": [\"order_id\", \"customer_id\", \"amount\", \"order_date\"],\n",
        "    \"customer\": [\"customer_id\", \"name\", \"city\"]\n",
        "}\n",
        "\n",
        "AGGS = [\"SUM\", \"AVG\", \"COUNT\", \"MAX\", \"MIN\"]\n",
        "OPS = [\">\", \"<\", \"=\"]\n",
        "\n",
        "def select_all():\n",
        "    table = random.choice(list(TABLE_SCHEMAS))\n",
        "    return (\n",
        "        f\"show all {table}\",\n",
        "        f\"SELECT * FROM {table}\",\n",
        "        {table: TABLE_SCHEMAS[table]}\n",
        "    )\n",
        "\n",
        "def where_clause():\n",
        "    val = random.choice([30000, 50000, 70000])\n",
        "    op = random.choice(OPS)\n",
        "    return (\n",
        "        f\"show employees where salary {op} {val}\",\n",
        "        f\"SELECT * FROM employee WHERE salary {op} {val}\",\n",
        "        {\"employee\": TABLE_SCHEMAS[\"employee\"]}\n",
        "    )\n",
        "\n",
        "def group_by():\n",
        "    agg = random.choice(AGGS)\n",
        "    return (\n",
        "        f\"{agg.lower()} salary per department\",\n",
        "        f\"SELECT department, {agg}(salary) FROM employee GROUP BY department\",\n",
        "        {\"employee\": TABLE_SCHEMAS[\"employee\"]}\n",
        "    )\n",
        "\n",
        "dataset = []\n",
        "for _ in range(50000):\n",
        "    q, sql, schema = random.choice([select_all, where_clause, group_by])()\n",
        "    dataset.append({\"question\": q, \"sql\": sql, \"schema\": schema})\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: PREPROCESS\n",
        "# ============================================================\n",
        "\n",
        "def normalize(text):\n",
        "    return re.sub(r\"\\s+\", \" \", text.lower()).strip()\n",
        "\n",
        "processed = []\n",
        "for item in dataset:\n",
        "    schema_text = \"\"\n",
        "    for t, cols in item[\"schema\"].items():\n",
        "        schema_text += f\"table {t} columns: {', '.join(cols)} ; \"\n",
        "    processed.append({\n",
        "        \"encoder_input\": schema_text + \" question: \" + normalize(item[\"question\"]),\n",
        "        \"decoder_output\": normalize(item[\"sql\"])\n",
        "    })\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3: BUILD VOCAB\n",
        "# ============================================================\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    counter = Counter()\n",
        "    for s in sentences:\n",
        "        counter.update(s.split())\n",
        "    vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
        "    for w in counter:\n",
        "        vocab[w] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "src_vocab = build_vocab([x[\"encoder_input\"] for x in processed])\n",
        "tgt_vocab = build_vocab([x[\"decoder_output\"] for x in processed])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 4: DATASET\n",
        "# ============================================================\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "    def __init__(self, data, src_vocab, tgt_vocab, max_len=60):\n",
        "        self.data = data\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def encode(self, text, vocab):\n",
        "        ids = [vocab.get(t, vocab[\"<unk>\"]) for t in text.split()]\n",
        "        return ids[:self.max_len]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # SOURCE (already fixed length)\n",
        "        src = self.encode(self.data[idx][\"encoder_input\"], self.src_vocab)\n",
        "        src = src + [self.src_vocab[\"<pad>\"]] * (self.max_len - len(src))\n",
        "\n",
        "        # TARGET (FIXED LENGTH NOW)\n",
        "        tgt = self.encode(self.data[idx][\"decoder_output\"], self.tgt_vocab)\n",
        "        tgt = [self.tgt_vocab[\"<sos>\"]] + tgt + [self.tgt_vocab[\"<eos>\"]]\n",
        "\n",
        "        tgt = tgt[:self.max_len]\n",
        "        tgt = tgt + [self.tgt_vocab[\"<pad>\"]] * (self.max_len - len(tgt))\n",
        "\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# ============================================================\n",
        "# CREATE DATALOADER (MISSING STEP)\n",
        "# ============================================================\n",
        "\n",
        "dataset_obj = NL2SQLDataset(processed, src_vocab, tgt_vocab, max_len=60)\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset_obj,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 5–7: MODEL\n",
        "# ============================================================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab, emb=128, hid=256):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, emb, padding_idx=0)\n",
        "        self.rnn = nn.GRU(emb, hid, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e = self.emb(x)\n",
        "        return self.rnn(e)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(hid * 2, hid)\n",
        "        self.v = nn.Linear(hid, 1, bias=False)\n",
        "\n",
        "    def forward(self, h, enc):\n",
        "        h = h.unsqueeze(1).repeat(1, enc.size(1), 1)\n",
        "        e = torch.tanh(self.fc(torch.cat((h, enc), 2)))\n",
        "        w = torch.softmax(self.v(e).squeeze(2), 1)\n",
        "        return torch.bmm(w.unsqueeze(1), enc).squeeze(1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab, emb=128, hid=256):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, emb)\n",
        "        self.attn = Attention(hid)\n",
        "        self.rnn = nn.GRU(emb + hid, hid, batch_first=True)\n",
        "        self.fc = nn.Linear(hid, vocab)\n",
        "\n",
        "    def forward(self, x, h, enc):\n",
        "        e = self.emb(x.unsqueeze(1))\n",
        "        c = self.attn(h.squeeze(0), enc)\n",
        "        out, h = self.rnn(torch.cat((e, c.unsqueeze(1)), 2), h)\n",
        "        return self.fc(out.squeeze(1)), h\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 8: TRAIN (NaN-SAFE)\n",
        "# ============================================================\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "encoder = Encoder(len(src_vocab)).to(device)\n",
        "decoder = Decoder(len(tgt_vocab)).to(device)\n",
        "\n",
        "PAD_IDX = tgt_vocab[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(encoder.parameters()) + list(decoder.parameters()), lr=0.001\n",
        ")\n",
        "\n",
        "for epoch in range(5):\n",
        "    encoder.train(); decoder.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        enc_out, hidden = encoder(src)\n",
        "\n",
        "        dec_input = tgt[:, 0]\n",
        "        loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for t in range(1, tgt.size(1)):\n",
        "            out, hidden = decoder(dec_input, hidden, enc_out)\n",
        "            loss += criterion(out, tgt[:, t])\n",
        "            dec_input = tgt[:, t]\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            list(encoder.parameters()) + list(decoder.parameters()), 1.0\n",
        "        )\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "RsaxyXGBpKdr",
        "outputId": "fc899c51-a151-4714-e08b-5eb7cf1508ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1308424444.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yRvwqktLm-Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VzJvBtjsm-Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3XRpUTJ8m-SX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}