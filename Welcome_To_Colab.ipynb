{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip -d /content/"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbr7H07m5MiX",
        "outputId": "78fba480-2b4e-40ad-86bf-c80b5f7df058"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "  inflating: /content/spider/README.txt  \n",
            "  inflating: /content/spider/database/academic/academic.sqlite  \n",
            "  inflating: /content/spider/database/academic/schema.sql  \n",
            "  inflating: /content/spider/database/activity_1/activity_1.sqlite  \n",
            "  inflating: /content/spider/database/activity_1/schema.sql  \n",
            "  inflating: /content/spider/database/aircraft/aircraft.sqlite  \n",
            "  inflating: /content/spider/database/aircraft/schema.sql  \n",
            "  inflating: /content/spider/database/allergy_1/allergy_1.sqlite  \n",
            "  inflating: /content/spider/database/allergy_1/schema.sql  \n",
            "  inflating: /content/spider/database/apartment_rentals/apartment_rentals.sqlite  \n",
            "  inflating: /content/spider/database/apartment_rentals/schema.sql  \n",
            "  inflating: /content/spider/database/architecture/architecture.sqlite  \n",
            "  inflating: /content/spider/database/architecture/schema.sql  \n",
            "  inflating: /content/spider/database/assets_maintenance/assets_maintenance.sqlite  \n",
            "  inflating: /content/spider/database/assets_maintenance/schema.sql  \n",
            "  inflating: /content/spider/database/baseball_1/baseball_1.sqlite  \n",
            "  inflating: /content/spider/database/baseball_1/schema.sql  \n",
            "  inflating: /content/spider/database/battle_death/battle_death.sqlite  \n",
            "  inflating: /content/spider/database/battle_death/schema.sql  \n",
            "  inflating: /content/spider/database/behavior_monitoring/behavior_monitoring.sqlite  \n",
            "  inflating: /content/spider/database/behavior_monitoring/schema.sql  \n",
            "  inflating: /content/spider/database/bike_1/bike_1.sqlite  \n",
            "  inflating: /content/spider/database/bike_1/schema.sql  \n",
            "  inflating: /content/spider/database/body_builder/body_builder.sqlite  \n",
            "  inflating: /content/spider/database/body_builder/schema.sql  \n",
            "  inflating: /content/spider/database/book_2/book_2.sqlite  \n",
            "  inflating: /content/spider/database/book_2/schema.sql  \n",
            "  inflating: /content/spider/database/browser_web/browser_web.sqlite  \n",
            "  inflating: /content/spider/database/browser_web/schema.sql  \n",
            "  inflating: /content/spider/database/candidate_poll/candidate_poll.sqlite  \n",
            "  inflating: /content/spider/database/candidate_poll/schema.sql  \n",
            "  inflating: /content/spider/database/car_1/annotation.json  \n",
            "  inflating: /content/spider/database/car_1/car_1.json  \n",
            "  inflating: /content/spider/database/car_1/car_1.sql  \n",
            "  inflating: /content/spider/database/car_1/car_1.sqlite  \n",
            "  inflating: /content/spider/database/car_1/data_csv/README.CARS.TXT  \n",
            "  inflating: /content/spider/database/car_1/data_csv/car-makers.csv  \n",
            "  inflating: /content/spider/database/car_1/data_csv/car-names.csv  \n",
            "  inflating: /content/spider/database/car_1/data_csv/cars-data.csv  \n",
            "  inflating: /content/spider/database/car_1/data_csv/cars.desc  \n",
            "  inflating: /content/spider/database/car_1/data_csv/continents.csv  \n",
            "  inflating: /content/spider/database/car_1/data_csv/countries.csv  \n",
            "  inflating: /content/spider/database/car_1/data_csv/model-list.csv  \n",
            "  inflating: /content/spider/database/car_1/link.txt  \n",
            "  inflating: /content/spider/database/car_1/q.txt  \n",
            "  inflating: /content/spider/database/chinook_1/annotation.json  \n",
            "  inflating: /content/spider/database/chinook_1/chinook_1.sqlite  \n",
            "  inflating: /content/spider/database/cinema/cinema.sqlite  \n",
            "  inflating: /content/spider/database/cinema/schema.sql  \n",
            "  inflating: /content/spider/database/city_record/city_record.sqlite  \n",
            "  inflating: /content/spider/database/city_record/schema.sql  \n",
            "  inflating: /content/spider/database/climbing/climbing.sqlite  \n",
            "  inflating: /content/spider/database/climbing/schema.sql  \n",
            "  inflating: /content/spider/database/club_1/club_1.sqlite  \n",
            "  inflating: /content/spider/database/club_1/schema.sql  \n",
            "  inflating: /content/spider/database/coffee_shop/coffee_shop.sqlite  \n",
            "  inflating: /content/spider/database/coffee_shop/schema.sql  \n",
            "  inflating: /content/spider/database/college_1/TinyCollege.sql  \n",
            "  inflating: /content/spider/database/college_1/college_1.sqlite  \n",
            "  inflating: /content/spider/database/college_1/link.txt  \n",
            "  inflating: /content/spider/database/college_2/TextBookExampleSchema.sql  \n",
            "  inflating: /content/spider/database/college_2/college_2.sqlite  \n",
            "  inflating: /content/spider/database/college_2/link.txt  \n",
            "  inflating: /content/spider/database/college_3/college_3.sqlite  \n",
            "  inflating: /content/spider/database/college_3/schema.sql  \n",
            "  inflating: /content/spider/database/company_1/company_1.sqlite  \n",
            "  inflating: /content/spider/database/company_1/link.txt  \n",
            "  inflating: /content/spider/database/company_employee/company_employee.sqlite  \n",
            "  inflating: /content/spider/database/company_employee/schema.sql  \n",
            "  inflating: /content/spider/database/company_office/company_office.sqlite  \n",
            "  inflating: /content/spider/database/company_office/schema.sql  \n",
            "  inflating: /content/spider/database/concert_singer/concert_singer.sqlite  \n",
            "  inflating: /content/spider/database/concert_singer/schema.sql  \n",
            "  inflating: /content/spider/database/county_public_safety/county_public_safety.sqlite  \n",
            "  inflating: /content/spider/database/county_public_safety/schema.sql  \n",
            "  inflating: /content/spider/database/course_teach/course_teach.sqlite  \n",
            "  inflating: /content/spider/database/course_teach/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Doc_Control_Systems/cre_Doc_Control_Systems.sqlite  \n",
            "  inflating: /content/spider/database/cre_Doc_Control_Systems/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Doc_Template_Mgt/cre_Doc_Template_Mgt.sqlite  \n",
            "  inflating: /content/spider/database/cre_Doc_Template_Mgt/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Doc_Tracking_DB/cre_Doc_Tracking_DB.sqlite  \n",
            "  inflating: /content/spider/database/cre_Doc_Tracking_DB/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Docs_and_Epenses/cre_Docs_and_Epenses.sqlite  \n",
            "  inflating: /content/spider/database/cre_Docs_and_Epenses/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Drama_Workshop_Groups/cre_Drama_Workshop_Groups.sqlite  \n",
            "  inflating: /content/spider/database/cre_Drama_Workshop_Groups/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Theme_park/cre_Theme_park.sqlite  \n",
            "  inflating: /content/spider/database/cre_Theme_park/schema.sql  \n",
            "  inflating: /content/spider/database/csu_1/csu_1.sqlite  \n",
            "  inflating: /content/spider/database/csu_1/schema.sql  \n",
            "  inflating: /content/spider/database/culture_company/culture_company.sqlite  \n",
            "  inflating: /content/spider/database/culture_company/schema.sql  \n",
            "  inflating: /content/spider/database/customer_complaints/customer_complaints.sqlite  \n",
            "  inflating: /content/spider/database/customer_complaints/schema.sql  \n",
            "  inflating: /content/spider/database/customer_deliveries/customer_deliveries.sqlite  \n",
            "  inflating: /content/spider/database/customer_deliveries/schema.sql  \n",
            "  inflating: /content/spider/database/customers_and_addresses/customers_and_addresses.sqlite  \n",
            "  inflating: /content/spider/database/customers_and_addresses/schema.sql  \n",
            "  inflating: /content/spider/database/customers_and_invoices/customers_and_invoices.sqlite  \n",
            "  inflating: /content/spider/database/customers_and_invoices/schema.sql  \n",
            "  inflating: /content/spider/database/customers_and_products_contacts/customers_and_products_contacts.sqlite  \n",
            "  inflating: /content/spider/database/customers_and_products_contacts/schema.sql  \n",
            "  inflating: /content/spider/database/customers_campaigns_ecommerce/customers_campaigns_ecommerce.sqlite  \n",
            "  inflating: /content/spider/database/customers_campaigns_ecommerce/schema.sql  \n",
            "  inflating: /content/spider/database/customers_card_transactions/customers_card_transactions.sqlite  \n",
            "  inflating: /content/spider/database/customers_card_transactions/schema.sql  \n",
            "  inflating: /content/spider/database/debate/debate.sqlite  \n",
            "  inflating: /content/spider/database/debate/schema.sql  \n",
            "  inflating: /content/spider/database/decoration_competition/decoration_competition.sqlite  \n",
            "  inflating: /content/spider/database/decoration_competition/schema.sql  \n",
            "  inflating: /content/spider/database/department_management/department_management.sqlite  \n",
            "  inflating: /content/spider/database/department_management/schema.sql  \n",
            "  inflating: /content/spider/database/department_store/department_store.sqlite  \n",
            "  inflating: /content/spider/database/department_store/schema.sql  \n",
            "  inflating: /content/spider/database/device/device.sqlite  \n",
            "  inflating: /content/spider/database/device/schema.sql  \n",
            "  inflating: /content/spider/database/document_management/document_management.sqlite  \n",
            "  inflating: /content/spider/database/document_management/schema.sql  \n",
            "  inflating: /content/spider/database/dog_kennels/dog_kennels.sqlite  \n",
            "  inflating: /content/spider/database/dog_kennels/schema.sql  \n",
            "  inflating: /content/spider/database/dorm_1/dorm_1.sqlite  \n",
            "  inflating: /content/spider/database/dorm_1/schema.sql  \n",
            "  inflating: /content/spider/database/driving_school/driving_school.sqlite  \n",
            "  inflating: /content/spider/database/driving_school/schema.sql  \n",
            "  inflating: /content/spider/database/e_government/e_government.sqlite  \n",
            "  inflating: /content/spider/database/e_government/schema.sql  \n",
            "  inflating: /content/spider/database/e_learning/e_learning.sqlite  \n",
            "  inflating: /content/spider/database/e_learning/schema.sql  \n",
            "  inflating: /content/spider/database/election/election.sqlite  \n",
            "  inflating: /content/spider/database/election/schema.sql  \n",
            "  inflating: /content/spider/database/election_representative/election_representative.sqlite  \n",
            "  inflating: /content/spider/database/election_representative/schema.sql  \n",
            "  inflating: /content/spider/database/employee_hire_evaluation/employee_hire_evaluation.sqlite  \n",
            "  inflating: /content/spider/database/employee_hire_evaluation/schema.sql  \n",
            "  inflating: /content/spider/database/entertainment_awards/entertainment_awards.sqlite  \n",
            "  inflating: /content/spider/database/entertainment_awards/schema.sql  \n",
            "  inflating: /content/spider/database/entrepreneur/entrepreneur.sqlite  \n",
            "  inflating: /content/spider/database/entrepreneur/schema.sql  \n",
            "  inflating: /content/spider/database/epinions_1/epinions_1.sqlite  \n",
            "  inflating: /content/spider/database/farm/farm.sqlite  \n",
            "  inflating: /content/spider/database/farm/schema.sql  \n",
            "  inflating: /content/spider/database/film_rank/film_rank.sqlite  \n",
            "  inflating: /content/spider/database/film_rank/schema.sql  \n",
            "  inflating: /content/spider/database/flight_1/flight_1.sqlite  \n",
            "  inflating: /content/spider/database/flight_1/schema.sql  \n",
            "  inflating: /content/spider/database/flight_2/annotation.json  \n",
            "  inflating: /content/spider/database/flight_2/data_csv/README.AIRLINES.txt  \n",
            "  inflating: /content/spider/database/flight_2/data_csv/airlines.csv  \n",
            "  inflating: /content/spider/database/flight_2/data_csv/airports100.csv  \n",
            "  inflating: /content/spider/database/flight_2/data_csv/flights.csv  \n",
            "  inflating: /content/spider/database/flight_2/flight_2.json  \n",
            "  inflating: /content/spider/database/flight_2/flight_2.sql  \n",
            "  inflating: /content/spider/database/flight_2/flight_2.sqlite  \n",
            "  inflating: /content/spider/database/flight_2/link.txt  \n",
            "  inflating: /content/spider/database/flight_2/q.txt  \n",
            "  inflating: /content/spider/database/flight_4/flight_4.sqlite  \n",
            "  inflating: /content/spider/database/flight_4/link.txt  \n",
            "  inflating: /content/spider/database/flight_4/sql.txt  \n",
            "  inflating: /content/spider/database/flight_company/flight_company.sqlite  \n",
            "  inflating: /content/spider/database/flight_company/schema.sql  \n",
            "  inflating: /content/spider/database/formula_1/annotation.json  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/circuits.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/constructorResults.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/constructorStandings.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/constructors.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/driverStandings.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/drivers.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/lapTimes.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/pitStops.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/qualifying.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/races.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/results.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/seasons.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/status.csv  \n",
            "  inflating: /content/spider/database/formula_1/formula_1.splite  \n",
            "  inflating: /content/spider/database/formula_1/formula_1.sql  \n",
            "  inflating: /content/spider/database/formula_1/formula_1.sqlite  \n",
            "  inflating: /content/spider/database/game_1/game_1.sqlite  \n",
            "  inflating: /content/spider/database/game_1/schema.sql  \n",
            "  inflating: /content/spider/database/game_injury/game_injury.sqlite  \n",
            "  inflating: /content/spider/database/game_injury/schema.sql  \n",
            "  inflating: /content/spider/database/gas_company/gas_company.sqlite  \n",
            "  inflating: /content/spider/database/gas_company/schema.sql  \n",
            "  inflating: /content/spider/database/geo/geo.sqlite  \n",
            "  inflating: /content/spider/database/geo/schema.sql  \n",
            "  inflating: /content/spider/database/gymnast/gymnast.sqlite  \n",
            "  inflating: /content/spider/database/gymnast/schema.sql  \n",
            "  inflating: /content/spider/database/hospital_1/hospital_1.sqlite  \n",
            "  inflating: /content/spider/database/hospital_1/schema.sql  \n",
            "  inflating: /content/spider/database/hr_1/hr_1.sqlite  \n",
            "  inflating: /content/spider/database/hr_1/schema.sql  \n",
            "  inflating: /content/spider/database/icfp_1/icfp_1.sqlite  \n",
            "  inflating: /content/spider/database/icfp_1/link.txt  \n",
            "  inflating: /content/spider/database/icfp_1/q.txt  \n",
            "  inflating: /content/spider/database/imdb/imdb.sqlite  \n",
            "  inflating: /content/spider/database/imdb/schema.sql  \n",
            "  inflating: /content/spider/database/inn_1/annotation.json  \n",
            "  inflating: /content/spider/database/inn_1/change_date.py  \n",
            "  inflating: /content/spider/database/inn_1/data_csv/README.INN.TXT  \n",
            "  inflating: /content/spider/database/inn_1/data_csv/Reservations.csv  \n",
            "  inflating: /content/spider/database/inn_1/data_csv/Reservations_t.csv  \n",
            "  inflating: /content/spider/database/inn_1/data_csv/Rooms.csv  \n",
            "  inflating: /content/spider/database/inn_1/inn_1.sql  \n",
            "  inflating: /content/spider/database/inn_1/inn_1.sqlite  \n",
            "  inflating: /content/spider/database/inn_1/link.txt  \n",
            "  inflating: /content/spider/database/inn_1/q.txt  \n",
            "  inflating: /content/spider/database/insurance_and_eClaims/insurance_and_eClaims.sqlite  \n",
            "  inflating: /content/spider/database/insurance_and_eClaims/schema.sql  \n",
            "  inflating: /content/spider/database/insurance_fnol/insurance_fnol.sqlite  \n",
            "  inflating: /content/spider/database/insurance_fnol/schema.sql  \n",
            "  inflating: /content/spider/database/insurance_policies/insurance_policies.sqlite  \n",
            "  inflating: /content/spider/database/insurance_policies/schema.sql  \n",
            "  inflating: /content/spider/database/journal_committee/journal_committee.sqlite  \n",
            "  inflating: /content/spider/database/journal_committee/schema.sql  \n",
            "  inflating: /content/spider/database/loan_1/loan_1.sqlite  \n",
            "  inflating: /content/spider/database/loan_1/schema.sql  \n",
            "  inflating: /content/spider/database/local_govt_and_lot/local_govt_and_lot.sqlite  \n",
            "  inflating: /content/spider/database/local_govt_and_lot/schema.sql  \n",
            "  inflating: /content/spider/database/local_govt_in_alabama/local_govt_in_alabama.sqlite  \n",
            "  inflating: /content/spider/database/local_govt_in_alabama/schema.sql  \n",
            "  inflating: /content/spider/database/local_govt_mdm/local_govt_mdm.sqlite  \n",
            "  inflating: /content/spider/database/local_govt_mdm/schema.sql  \n",
            "  inflating: /content/spider/database/machine_repair/machine_repair.sqlite  \n",
            "  inflating: /content/spider/database/machine_repair/schema.sql  \n",
            "  inflating: /content/spider/database/manufactory_1/manufactory_1.sqlite  \n",
            "  inflating: /content/spider/database/manufactory_1/schema.sql  \n",
            "  inflating: /content/spider/database/manufacturer/manufacturer.sqlite  \n",
            "  inflating: /content/spider/database/manufacturer/schema.sql  \n",
            "  inflating: /content/spider/database/match_season/match_season.sqlite  \n",
            "  inflating: /content/spider/database/match_season/schema.sql  \n",
            "  inflating: /content/spider/database/medicine_enzyme_interaction/medicine_enzyme_interaction.sqlite  \n",
            "  inflating: /content/spider/database/medicine_enzyme_interaction/schema.sql  \n",
            "  inflating: /content/spider/database/mountain_photos/mountain_photos.sqlite  \n",
            "  inflating: /content/spider/database/mountain_photos/schema.sql  \n",
            "  inflating: /content/spider/database/movie_1/movie_1.sqlite  \n",
            "  inflating: /content/spider/database/movie_1/schema.sql  \n",
            "  inflating: /content/spider/database/museum_visit/museum_visit.sqlite  \n",
            "  inflating: /content/spider/database/museum_visit/schema.sql  \n",
            "  inflating: /content/spider/database/music_1/music_1.sqlite  \n",
            "  inflating: /content/spider/database/music_1/schema.sql  \n",
            "  inflating: /content/spider/database/music_2/music_2.sqlite  \n",
            "  inflating: /content/spider/database/music_2/schema.sql  \n",
            "  inflating: /content/spider/database/music_4/music_4.sqlite  \n",
            "  inflating: /content/spider/database/music_4/schema.sql  \n",
            "  inflating: /content/spider/database/musical/musical.sqlite  \n",
            "  inflating: /content/spider/database/musical/schema.sql  \n",
            "  inflating: /content/spider/database/network_1/network_1.sqlite  \n",
            "  inflating: /content/spider/database/network_1/schema.sql  \n",
            "  inflating: /content/spider/database/network_2/network_2.sqlite  \n",
            "  inflating: /content/spider/database/network_2/schema.sql  \n",
            "  inflating: /content/spider/database/news_report/news_report.sqlite  \n",
            "  inflating: /content/spider/database/news_report/schema.sql  \n",
            "  inflating: /content/spider/database/orchestra/orchestra.sqlite  \n",
            "  inflating: /content/spider/database/orchestra/schema.sql  \n",
            "  inflating: /content/spider/database/party_host/party_host.sqlite  \n",
            "  inflating: /content/spider/database/party_host/schema.sql  \n",
            "  inflating: /content/spider/database/party_people/party_people.sqlite  \n",
            "  inflating: /content/spider/database/party_people/schema.sql  \n",
            "  inflating: /content/spider/database/performance_attendance/performance_attendance.sqlite  \n",
            "  inflating: /content/spider/database/performance_attendance/schema.sql  \n",
            "  inflating: /content/spider/database/perpetrator/perpetrator.sqlite  \n",
            "  inflating: /content/spider/database/perpetrator/schema.sql  \n",
            "  inflating: /content/spider/database/pets_1/pets_1.sqlite  \n",
            "  inflating: /content/spider/database/pets_1/schema.sql  \n",
            "  inflating: /content/spider/database/phone_1/phone_1.sqlite  \n",
            "  inflating: /content/spider/database/phone_1/schema.sql  \n",
            "  inflating: /content/spider/database/phone_market/phone_market.sqlite  \n",
            "  inflating: /content/spider/database/phone_market/schema.sql  \n",
            "  inflating: /content/spider/database/pilot_record/pilot_record.sqlite  \n",
            "  inflating: /content/spider/database/pilot_record/schema.sql  \n",
            "  inflating: /content/spider/database/poker_player/poker_player.sqlite  \n",
            "  inflating: /content/spider/database/poker_player/schema.sql  \n",
            "  inflating: /content/spider/database/product_catalog/product_catalog.sqlite  \n",
            "  inflating: /content/spider/database/product_catalog/schema.sql  \n",
            "  inflating: /content/spider/database/products_for_hire/products_for_hire.sqlite  \n",
            "  inflating: /content/spider/database/products_for_hire/schema.sql  \n",
            "  inflating: /content/spider/database/products_gen_characteristics/products_gen_characteristics.sqlite  \n",
            "  inflating: /content/spider/database/products_gen_characteristics/schema.sql  \n",
            "  inflating: /content/spider/database/program_share/program_share.sqlite  \n",
            "  inflating: /content/spider/database/program_share/schema.sql  \n",
            "  inflating: /content/spider/database/protein_institute/protein_institute.sqlite  \n",
            "  inflating: /content/spider/database/protein_institute/schema.sql  \n",
            "  inflating: /content/spider/database/race_track/race_track.sqlite  \n",
            "  inflating: /content/spider/database/race_track/schema.sql  \n",
            "  inflating: /content/spider/database/railway/railway.sqlite  \n",
            "  inflating: /content/spider/database/railway/schema.sql  \n",
            "  inflating: /content/spider/database/real_estate_properties/real_estate_properties.sqlite  \n",
            "  inflating: /content/spider/database/real_estate_properties/schema.sql  \n",
            "  inflating: /content/spider/database/restaurant_1/restaurant_1.sqlite  \n",
            "  inflating: /content/spider/database/restaurant_1/schema.sql  \n",
            "  inflating: /content/spider/database/restaurants/restaurants.sqlite  \n",
            "  inflating: /content/spider/database/restaurants/schema.sql  \n",
            "  inflating: /content/spider/database/riding_club/riding_club.sqlite  \n",
            "  inflating: /content/spider/database/riding_club/schema.sql  \n",
            "  inflating: /content/spider/database/roller_coaster/roller_coaster.sqlite  \n",
            "  inflating: /content/spider/database/roller_coaster/schema.sql  \n",
            "  inflating: /content/spider/database/sakila_1/sakila_1.sqlite  \n",
            "  inflating: /content/spider/database/sakila_1/schema.sql  \n",
            "  inflating: /content/spider/database/scholar/schema.sql  \n",
            "  inflating: /content/spider/database/scholar/scholar.sqlite  \n",
            "  inflating: /content/spider/database/school_bus/schema.sql  \n",
            "  inflating: /content/spider/database/school_bus/school_bus.sqlite  \n",
            "  inflating: /content/spider/database/school_finance/schema.sql  \n",
            "  inflating: /content/spider/database/school_finance/school_finance.sqlite  \n",
            "  inflating: /content/spider/database/school_player/schema.sql  \n",
            "  inflating: /content/spider/database/school_player/school_player.sqlite  \n",
            "  inflating: /content/spider/database/scientist_1/schema.sql  \n",
            "  inflating: /content/spider/database/scientist_1/scientist_1.sqlite  \n",
            "  inflating: /content/spider/database/ship_1/schema.sql  \n",
            "  inflating: /content/spider/database/ship_1/ship_1.sqlite  \n",
            "  inflating: /content/spider/database/ship_mission/schema.sql  \n",
            "  inflating: /content/spider/database/ship_mission/ship_mission.sqlite  \n",
            "  inflating: /content/spider/database/shop_membership/schema.sql  \n",
            "  inflating: /content/spider/database/shop_membership/shop_membership.sqlite  \n",
            "  inflating: /content/spider/database/singer/schema.sql  \n",
            "  inflating: /content/spider/database/singer/singer.sqlite  \n",
            "  inflating: /content/spider/database/small_bank_1/small_bank_1.sqlite  \n",
            "  inflating: /content/spider/database/soccer_1/schema.sql  \n",
            "  inflating: /content/spider/database/soccer_1/soccer_1.sqlite  \n",
            "  inflating: /content/spider/database/soccer_2/schema.sql  \n",
            "  inflating: /content/spider/database/soccer_2/soccer_2.sqlite  \n",
            "  inflating: /content/spider/database/solvency_ii/schema.sql  \n",
            "  inflating: /content/spider/database/solvency_ii/solvency_ii.sqlite  \n",
            "  inflating: /content/spider/database/sports_competition/schema.sql  \n",
            "  inflating: /content/spider/database/sports_competition/sports_competition.sqlite  \n",
            "  inflating: /content/spider/database/station_weather/schema.sql  \n",
            "  inflating: /content/spider/database/station_weather/station_weather.sqlite  \n",
            "  inflating: /content/spider/database/store_1/schema.sql  \n",
            "  inflating: /content/spider/database/store_1/store_1.sqlite  \n",
            "  inflating: /content/spider/database/store_product/schema.sql  \n",
            "  inflating: /content/spider/database/store_product/store_product.sqlite  \n",
            "  inflating: /content/spider/database/storm_record/schema.sql  \n",
            "  inflating: /content/spider/database/storm_record/storm_record.sqlite  \n",
            "  inflating: /content/spider/database/student_1/annotation.json  \n",
            "  inflating: /content/spider/database/student_1/data_csv/README.STUDENTS.TXT  \n",
            "  inflating: /content/spider/database/student_1/data_csv/list.csv  \n",
            "  inflating: /content/spider/database/student_1/data_csv/teachers.csv  \n",
            "  inflating: /content/spider/database/student_1/link.txt  \n",
            "  inflating: /content/spider/database/student_1/q.txt  \n",
            "  inflating: /content/spider/database/student_1/student_1.sql  \n",
            "  inflating: /content/spider/database/student_1/student_1.sqlite  \n",
            "  inflating: /content/spider/database/student_assessment/schema.sql  \n",
            "  inflating: /content/spider/database/student_assessment/student_assessment.sqlite  \n",
            "  inflating: /content/spider/database/student_transcripts_tracking/schema.sql  \n",
            "  inflating: /content/spider/database/student_transcripts_tracking/student_transcripts_tracking.sqlite  \n",
            "  inflating: /content/spider/database/swimming/schema.sql  \n",
            "  inflating: /content/spider/database/swimming/swimming.sqlite  \n",
            "  inflating: /content/spider/database/theme_gallery/schema.sql  \n",
            "  inflating: /content/spider/database/theme_gallery/theme_gallery.sqlite  \n",
            "  inflating: /content/spider/database/tracking_grants_for_research/schema.sql  \n",
            "  inflating: /content/spider/database/tracking_grants_for_research/tracking_grants_for_research.sqlite  \n",
            "  inflating: /content/spider/database/tracking_orders/schema.sql  \n",
            "  inflating: /content/spider/database/tracking_orders/tracking_orders.sqlite  \n",
            "  inflating: /content/spider/database/tracking_share_transactions/schema.sql  \n",
            "  inflating: /content/spider/database/tracking_share_transactions/tracking_share_transactions.sqlite  \n",
            "  inflating: /content/spider/database/tracking_software_problems/schema.sql  \n",
            "  inflating: /content/spider/database/tracking_software_problems/tracking_software_problems.sqlite  \n",
            "  inflating: /content/spider/database/train_station/schema.sql  \n",
            "  inflating: /content/spider/database/train_station/train_station.sqlite  \n",
            "  inflating: /content/spider/database/tvshow/schema.sql  \n",
            "  inflating: /content/spider/database/tvshow/tvshow.sqlite  \n",
            "  inflating: /content/spider/database/twitter_1/queries/oracle-dialects.xml  \n",
            "  inflating: /content/spider/database/twitter_1/queries/postgres-dialects.xml  \n",
            "  inflating: /content/spider/database/twitter_1/queries/sqlserver-dialects.xml  \n",
            "  inflating: /content/spider/database/twitter_1/twitter_1.sqlite  \n",
            "  inflating: /content/spider/database/university_basketball/schema.sql  \n",
            "  inflating: /content/spider/database/university_basketball/university_basketball.sqlite  \n",
            "  inflating: /content/spider/database/voter_1/voter_1.sqlite  \n",
            "  inflating: /content/spider/database/voter_2/schema.sql  \n",
            "  inflating: /content/spider/database/voter_2/voter_2.sqlite  \n",
            "  inflating: /content/spider/database/wedding/schema.sql  \n",
            "  inflating: /content/spider/database/wedding/wedding.sqlite  \n",
            "  inflating: /content/spider/database/wine_1/annotation.json  \n",
            "  inflating: /content/spider/database/wine_1/data_csv/README.WINE.txt  \n",
            "  inflating: /content/spider/database/wine_1/data_csv/appellations.csv  \n",
            "  inflating: /content/spider/database/wine_1/data_csv/grapes.csv  \n",
            "  inflating: /content/spider/database/wine_1/data_csv/wine.csv  \n",
            "  inflating: /content/spider/database/wine_1/link.txt  \n",
            "  inflating: /content/spider/database/wine_1/q.txt  \n",
            "  inflating: /content/spider/database/wine_1/wine_1.sql  \n",
            "  inflating: /content/spider/database/wine_1/wine_1.sqlite  \n",
            "  inflating: /content/spider/database/workshop_paper/schema.sql  \n",
            "  inflating: /content/spider/database/workshop_paper/workshop_paper.sqlite  \n",
            "  inflating: /content/spider/database/world_1/world_1.json  \n",
            "  inflating: /content/spider/database/world_1/world_1.sqlite  \n",
            "  inflating: /content/spider/database/wrestler/schema.sql  \n",
            "  inflating: /content/spider/database/wrestler/wrestler.sqlite  \n",
            "  inflating: /content/spider/database/wta_1/wta_1.sql  \n",
            "  inflating: /content/spider/database/wta_1/wta_1.sqlite  \n",
            "  inflating: /content/spider/database/yelp/schema.sql  \n",
            "  inflating: /content/spider/database/yelp/yelp.sqlite  \n",
            "  inflating: /content/spider/dev.json  \n",
            "  inflating: /content/spider/dev_gold.sql  \n",
            "  inflating: /content/spider/tables.json  \n",
            "  inflating: /content/spider/train_gold.sql  \n",
            "  inflating: /content/spider/train_others.json  \n",
            "  inflating: /content/spider/train_spider.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================\n",
        "# # SPIDER DATASET PREPROCESSING ‚Üí SCHEMA-AWARE FORMAT\n",
        "# # OUTPUT: spider_schema_aware.json (7000 examples)\n",
        "# # ============================================================\n",
        "\n",
        "# import json\n",
        "# import re\n",
        "# import os\n",
        "\n",
        "# # ============================================================\n",
        "# # 1Ô∏è‚É£ PATHS (DATASET ALREADY UNZIPPED)\n",
        "# # ============================================================\n",
        "# TRAIN_PATH = \"/content/spider/train_spider.json\"\n",
        "# TABLES_PATH = \"/content/spider/tables.json\"\n",
        "\n",
        "# assert os.path.exists(TRAIN_PATH), \"‚ùå train_spider.json not found\"\n",
        "# assert os.path.exists(TABLES_PATH), \"‚ùå tables.json not found\"\n",
        "\n",
        "# # ============================================================\n",
        "# # 2Ô∏è‚É£ LOAD RAW SPIDER FILES\n",
        "# # ============================================================\n",
        "# with open(TRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "#     train_data = json.load(f)\n",
        "\n",
        "# with open(TABLES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "#     tables_data = json.load(f)\n",
        "\n",
        "# print(\"‚úÖ Loaded questions:\", len(train_data))\n",
        "# print(\"‚úÖ Loaded schemas:\", len(tables_data))\n",
        "\n",
        "# # ============================================================\n",
        "# # 3Ô∏è‚É£ BUILD DB ‚Üí SCHEMA MAP\n",
        "# # ============================================================\n",
        "# db_schemas = {}\n",
        "\n",
        "# for db in tables_data:\n",
        "#     db_id = db[\"db_id\"]\n",
        "#     tables = db[\"table_names_original\"]\n",
        "#     columns = db[\"column_names_original\"]\n",
        "\n",
        "#     schema = {}\n",
        "#     for tid, col in columns:\n",
        "#         if tid == -1:\n",
        "#             continue\n",
        "#         table = tables[tid]\n",
        "#         schema.setdefault(table, []).append(col)\n",
        "\n",
        "#     db_schemas[db_id] = schema\n",
        "\n",
        "# print(\"‚úÖ Schema map built\")\n",
        "\n",
        "# # ============================================================\n",
        "# # 4Ô∏è‚É£ TOKENIZE QUESTION\n",
        "# # ============================================================\n",
        "# def tokenize_question(text):\n",
        "#     text = text.lower()\n",
        "#     text = re.sub(r\"[^a-z0-9_ ]\", \" \", text)\n",
        "#     return text.split()\n",
        "\n",
        "# # ============================================================\n",
        "# # 5Ô∏è‚É£ FIND USED TABLES & COLUMNS FROM SQL\n",
        "# # ============================================================\n",
        "# def find_schema_mentions(sql, schema):\n",
        "#     sql = sql.lower()\n",
        "#     used_tables = set()\n",
        "#     used_columns = set()\n",
        "\n",
        "#     for table, cols in schema.items():\n",
        "#         if re.search(rf\"\\b{re.escape(table.lower())}\\b\", sql):\n",
        "#             used_tables.add(table)\n",
        "\n",
        "#         for col in cols:\n",
        "#             if re.search(rf\"\\b{re.escape(col.lower())}\\b\", sql):\n",
        "#                 used_columns.add(f\"{table}.{col}\")\n",
        "\n",
        "#     return used_tables, used_columns\n",
        "\n",
        "# # ============================================================\n",
        "# # 6Ô∏è‚É£ BUILD INPUT TOKENS + TOKEN TYPES\n",
        "# # ============================================================\n",
        "# def build_input_with_schema(question_tokens, schema):\n",
        "#     tokens = []\n",
        "#     token_types = []\n",
        "\n",
        "#     # Question tokens ‚Üí type 0\n",
        "#     for tok in question_tokens:\n",
        "#         tokens.append(tok)\n",
        "#         token_types.append(0)\n",
        "\n",
        "#     # Schema tokens\n",
        "#     for table, cols in schema.items():\n",
        "#         tokens.append(table)\n",
        "#         token_types.append(1)\n",
        "#         for col in cols:\n",
        "#             tokens.append(col)\n",
        "#             token_types.append(2)\n",
        "\n",
        "#     return tokens, token_types\n",
        "\n",
        "# # ============================================================\n",
        "# # 7Ô∏è‚É£ BUILD SCHEMA LABELS\n",
        "# # ============================================================\n",
        "# def build_schema_labels(tokens, token_types, used_tables, used_columns):\n",
        "#     labels = []\n",
        "\n",
        "#     for tok, ttype in zip(tokens, token_types):\n",
        "#         if ttype == 1:  # table\n",
        "#             labels.append(1 if tok in used_tables else 0)\n",
        "#         elif ttype == 2:  # column\n",
        "#             labels.append(\n",
        "#                 1 if any(tok == c.split(\".\")[1] for c in used_columns) else 0\n",
        "#             )\n",
        "#         else:\n",
        "#             labels.append(0)\n",
        "\n",
        "#     return labels\n",
        "\n",
        "# # ============================================================\n",
        "# # 8Ô∏è‚É£ CONVERT ONE EXAMPLE\n",
        "# # ============================================================\n",
        "# def convert_example(ex):\n",
        "#     schema = db_schemas[ex[\"db_id\"]]\n",
        "\n",
        "#     q_tokens = tokenize_question(ex[\"question\"])\n",
        "#     used_tables, used_columns = find_schema_mentions(ex[\"query\"], schema)\n",
        "\n",
        "#     tokens, token_types = build_input_with_schema(q_tokens, schema)\n",
        "#     schema_labels = build_schema_labels(tokens, token_types, used_tables, used_columns)\n",
        "\n",
        "#     return {\n",
        "#         \"tokens\": tokens,\n",
        "#         \"token_types\": token_types,\n",
        "#         \"schema_labels\": schema_labels,\n",
        "#         \"schema\": schema,\n",
        "#         \"sql\": ex[\"query\"]\n",
        "#     }\n",
        "\n",
        "# # ============================================================\n",
        "# # 9Ô∏è‚É£ PROCESS DATASET (LIMIT TO 7000)\n",
        "# # ============================================================\n",
        "# MAX_EXAMPLES = 7000\n",
        "# processed = []\n",
        "\n",
        "# for i, ex in enumerate(train_data[:MAX_EXAMPLES]):\n",
        "#     processed.append(convert_example(ex))\n",
        "#     if i > 0 and i % 500 == 0:\n",
        "#         print(f\"‚è≥ Processed {i} examples\")\n",
        "\n",
        "# print(\"‚úÖ Total processed:\", len(processed))\n",
        "\n",
        "# # ============================================================\n",
        "# # üîü SAVE OUTPUT\n",
        "# # ============================================================\n",
        "# OUTPUT_PATH = \"/content/spider_schema_aware.json\"\n",
        "\n",
        "# with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(processed, f, indent=2)\n",
        "\n",
        "# print(\"‚úÖ Saved to:\", OUTPUT_PATH)\n",
        "\n",
        "# # ============================================================\n",
        "# # 1Ô∏è‚É£1Ô∏è‚É£ SANITY CHECK\n",
        "# # ============================================================\n",
        "# sample = processed[0]\n",
        "# print(\"\\n--- SAMPLE ---\")\n",
        "# print(\"Tokens:\", sample[\"tokens\"][:20])\n",
        "# print(\"Token types:\", sample[\"token_types\"][:20])\n",
        "# print(\"Schema labels:\", sample[\"schema_labels\"][:20])\n",
        "# print(\"SQL:\", sample[\"sql\"])\n",
        "import json, re, os\n",
        "\n",
        "# Paths\n",
        "TRAIN_PATH = \"/content/spider/train_spider.json\"\n",
        "TABLES_PATH = \"/content/spider/tables.json\"\n",
        "\n",
        "with open(TRAIN_PATH, \"r\") as f: train_data = json.load(f)\n",
        "with open(TABLES_PATH, \"r\") as f: tables_data = json.load(f)\n",
        "\n",
        "db_schemas = {}\n",
        "for db in tables_data:\n",
        "    schema = {}\n",
        "    for tid, col in db[\"column_names_original\"]:\n",
        "        if tid == -1: continue\n",
        "        table = db[\"table_names_original\"][tid].lower()\n",
        "        schema.setdefault(table, []).append(col.lower())\n",
        "    db_schemas[db[\"db_id\"]] = schema\n",
        "\n",
        "def build_placeholders(schema):\n",
        "    tmap = {t: f\"T{i}\" for i, t in enumerate(schema.keys())}\n",
        "    cmap = {}\n",
        "    cid = 0\n",
        "    for t, cols in schema.items():\n",
        "        for c in cols:\n",
        "            cmap[f\"{t}.{c}\"] = f\"C{cid}\"\n",
        "            cid += 1\n",
        "    return tmap, cmap\n",
        "\n",
        "def preprocess_sql(sql, tmap, cmap):\n",
        "    sql = sql.lower()\n",
        "    # Replace columns first (longer strings first to avoid partial matches)\n",
        "    for k, v in sorted(cmap.items(), key=lambda x: -len(x[0])):\n",
        "        sql = re.sub(rf\"\\b{re.escape(k)}\\b\", v, sql)\n",
        "    for k, v in sorted(tmap.items(), key=lambda x: -len(x[0])):\n",
        "        sql = re.sub(rf\"\\b{re.escape(k)}\\b\", v, sql)\n",
        "    sql = re.sub(r\"\\b\\d+\\b\", \"VALUE\", sql)\n",
        "    return sql.upper().replace(\",\", \" , \").replace(\"(\", \" ( \").replace(\")\", \" ) \").split()\n",
        "\n",
        "processed = []\n",
        "for ex in train_data[:7000]:\n",
        "    db_id = ex[\"db_id\"]\n",
        "    schema = db_schemas[db_id]\n",
        "    tmap, cmap = build_placeholders(schema)\n",
        "\n",
        "    # Question Tokens\n",
        "    q_toks = re.sub(r\"[^a-z0-9_ ]\", \" \", ex[\"question\"].lower()).split()\n",
        "\n",
        "    # Schema Tokens for Encoder\n",
        "    s_toks, s_types = [], []\n",
        "    for t, cols in schema.items():\n",
        "        s_toks.append(t); s_types.append(1)\n",
        "        for c in cols:\n",
        "            s_toks.append(c); s_types.append(2)\n",
        "\n",
        "    processed.append({\n",
        "        \"tokens\": q_toks + s_toks,\n",
        "        \"token_types\": [0]*len(q_toks) + s_types,\n",
        "        \"sql_tokens\": preprocess_sql(ex[\"query\"], tmap, cmap),\n",
        "        \"schema\": schema,\n",
        "        \"db_id\": db_id\n",
        "    })\n",
        "\n",
        "with open(\"spider_processed.json\", \"w\") as f: json.dump(processed, f)"
      ],
      "metadata": {
        "id": "lobdUuOb5MmP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import math\n",
        "# import json\n",
        "# import re\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# # ============================================================\n",
        "# # 1Ô∏è‚É£ DEVICE\n",
        "# # ============================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(\"Using device:\", device)\n",
        "\n",
        "# # ============================================================\n",
        "# # 2Ô∏è‚É£ LOAD PREPROCESSED DATA (7000 examples)\n",
        "# # ============================================================\n",
        "# DATA_PATH = \"spider_schema_aware.json\"\n",
        "# with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "#     data = json.load(f)\n",
        "\n",
        "# data = data[:7000]\n",
        "# print(\"Training examples:\", len(data))\n",
        "\n",
        "# # ============================================================\n",
        "# # 3Ô∏è‚É£ BUILD ENCODER VOCAB\n",
        "# # ============================================================\n",
        "# def build_encoder_vocab(data):\n",
        "#     vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "#     idx = 2\n",
        "#     for ex in data:\n",
        "#         for tok in ex[\"tokens\"]:\n",
        "#             if tok not in vocab:\n",
        "#                 vocab[tok] = idx\n",
        "#                 idx += 1\n",
        "#     return vocab\n",
        "\n",
        "# encoder_vocab = build_encoder_vocab(data)\n",
        "# enc_vocab_size = len(encoder_vocab)\n",
        "# print(\"Encoder vocab size:\", enc_vocab_size)\n",
        "\n",
        "# # ============================================================\n",
        "# # 4Ô∏è‚É£ DECODER SQL VOCAB (PLACEHOLDER-BASED)\n",
        "# # ============================================================\n",
        "# SQL_KEYWORDS = [\"SELECT\",\"FROM\",\"WHERE\",\"JOIN\",\"ON\",\"AS\",\"DISTINCT\"]\n",
        "# LOGICAL_OPS = [\"AND\",\"OR\",\"NOT\"]\n",
        "# COMPARISON_OPS = [\"=\",\"!=\",\"<>\",\">\",\"<\",\">=\",\"<=\"]\n",
        "# AGG_FUNCS = [\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\"]\n",
        "# GROUPING = [\"GROUP\",\"BY\",\"HAVING\"]\n",
        "# ORDERING = [\"ORDER\",\"ASC\",\"DESC\",\"LIMIT\"]\n",
        "# PUNCT = [\",\",\"(\",\")\"]\n",
        "# SPECIAL = [\"<PAD>\",\"<EOS>\",\"VALUE\"]\n",
        "\n",
        "# MAX_TABLES = 128\n",
        "# MAX_COLUMNS = 512\n",
        "# TABLE_TOKENS = [f\"T{i}\" for i in range(MAX_TABLES)]\n",
        "# COLUMN_TOKENS = [f\"C{i}\" for i in range(MAX_COLUMNS)]\n",
        "\n",
        "# DECODER_VOCAB = (\n",
        "#     SQL_KEYWORDS + LOGICAL_OPS + COMPARISON_OPS + AGG_FUNCS +\n",
        "#     GROUPING + ORDERING + PUNCT + SPECIAL +\n",
        "#     TABLE_TOKENS + COLUMN_TOKENS\n",
        "# )\n",
        "\n",
        "# token_to_id = {t:i for i,t in enumerate(DECODER_VOCAB)}\n",
        "# id_to_token = {i:t for t,i in token_to_id.items()}\n",
        "# VOCAB_SIZE = len(token_to_id)\n",
        "# print(\"Decoder vocab size:\", VOCAB_SIZE)\n",
        "\n",
        "# # ============================================================\n",
        "# # 5Ô∏è‚É£ SQL ‚Üí PLACEHOLDER CONVERSION\n",
        "# # ============================================================\n",
        "# def build_schema_maps(schema):\n",
        "#     tmap = {t: f\"T{i}\" for i,t in enumerate(schema.keys())}\n",
        "#     cmap = {}\n",
        "#     cid = 0\n",
        "#     for t, cols in schema.items():\n",
        "#         for c in cols:\n",
        "#             cmap[f\"{t}.{c}\"] = f\"C{cid}\"\n",
        "#             cid += 1\n",
        "#     return tmap, cmap\n",
        "\n",
        "# def sql_to_placeholder(sql, tmap, cmap):\n",
        "#     sql = sql.lower()\n",
        "#     for k,v in sorted(cmap.items(), key=lambda x:-len(x[0])):\n",
        "#         sql = re.sub(rf\"\\b{re.escape(k)}\\b\", v, sql)\n",
        "#     for k,v in tmap.items():\n",
        "#         sql = re.sub(rf\"\\b{re.escape(k)}\\b\", v, sql)\n",
        "#     sql = re.sub(r\"\\b\\d+\\b\", \"VALUE\", sql)\n",
        "#     return sql.upper()\n",
        "\n",
        "# # ============================================================\n",
        "# # 6Ô∏è‚É£ DATASET\n",
        "# # ============================================================\n",
        "# class SpiderDataset(Dataset):\n",
        "#     def __init__(self, data, enc_vocab):\n",
        "#         self.data = data\n",
        "#         self.enc_vocab = enc_vocab\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         ex = self.data[idx]\n",
        "\n",
        "#         x = [self.enc_vocab.get(t,1) for t in ex[\"tokens\"]]\n",
        "#         t = ex[\"token_types\"]\n",
        "#         s = ex[\"schema_labels\"]\n",
        "\n",
        "#         tmap, cmap = build_schema_maps(ex[\"schema\"])\n",
        "#         sql_ph = sql_to_placeholder(ex[\"sql\"], tmap, cmap)\n",
        "#         y = [token_to_id.get(tok, token_to_id[\"<PAD>\"]) for tok in sql_ph.split()]\n",
        "#         y.append(token_to_id[\"<EOS>\"])\n",
        "\n",
        "#         return (\n",
        "#             torch.tensor(x),\n",
        "#             torch.tensor(t),\n",
        "#             torch.tensor(s, dtype=torch.float),\n",
        "#             torch.tensor(y)\n",
        "#         )\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     def pad(seqs, pad=0):\n",
        "#         m = max(len(s) for s in seqs)\n",
        "#         return torch.stack([\n",
        "#             torch.cat([s, torch.full((m-len(s),), pad)]) for s in seqs\n",
        "#         ])\n",
        "\n",
        "#     x,t,s,y = zip(*batch)\n",
        "#     return pad(x), pad(t), pad(s), pad(y)\n",
        "\n",
        "# loader = DataLoader(\n",
        "#     SpiderDataset(data, encoder_vocab),\n",
        "#     batch_size=8,\n",
        "#     shuffle=True,\n",
        "#     collate_fn=collate_fn\n",
        "# )\n",
        "\n",
        "# # ============================================================\n",
        "# # 7Ô∏è‚É£ TRANSFORMER MODULES\n",
        "# # ============================================================\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#     def __init__(self, d, h):\n",
        "#         super().__init__()\n",
        "#         self.h = h\n",
        "#         self.dk = d // h\n",
        "#         self.qkv = nn.Linear(d, d*3)\n",
        "#         self.o = nn.Linear(d, d)\n",
        "\n",
        "#     def forward(self, q,k,v,mask=None):\n",
        "#         B,T,D = q.size()\n",
        "#         q,k,v = self.qkv(q).chunk(3,dim=-1)\n",
        "#         q = q.view(B,T,self.h,self.dk).transpose(1,2)\n",
        "#         k = k.view(B,-1,self.h,self.dk).transpose(1,2)\n",
        "#         v = v.view(B,-1,self.h,self.dk).transpose(1,2)\n",
        "#         scores = q @ k.transpose(-2,-1) / math.sqrt(self.dk)\n",
        "#         if mask is not None:\n",
        "#             scores = scores.masked_fill(mask==0, -1e9)\n",
        "#         out = (scores.softmax(-1) @ v)\n",
        "#         out = out.transpose(1,2).contiguous().view(B,T,D)\n",
        "#         return self.o(out)\n",
        "\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, vocab, d=256, h=8, ff=1024, layers=4):\n",
        "#         super().__init__()\n",
        "#         self.emb = nn.Embedding(vocab, d, padding_idx=0)\n",
        "#         self.type_emb = nn.Embedding(3, d)\n",
        "#         self.layers = nn.ModuleList([\n",
        "#             nn.ModuleList([\n",
        "#                 MultiHeadAttention(d,h),\n",
        "#                 nn.Sequential(nn.Linear(d,ff), nn.ReLU(), nn.Linear(ff,d)),\n",
        "#                 nn.LayerNorm(d),\n",
        "#                 nn.LayerNorm(d)\n",
        "#             ]) for _ in range(layers)\n",
        "#         ])\n",
        "#         self.schema_head = nn.Linear(d,1)\n",
        "\n",
        "#     def forward(self, x, t, mask):\n",
        "#         x = self.emb(x) + self.type_emb(t)\n",
        "#         for attn, ff, n1, n2 in self.layers:\n",
        "#             x = n1(x + attn(x,x,x,mask))\n",
        "#             x = n2(x + ff(x))\n",
        "#         return x, self.schema_head(x).squeeze(-1)\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, vocab, d=256, h=8, ff=1024, layers=4):\n",
        "#         super().__init__()\n",
        "#         self.emb = nn.Embedding(vocab, d)\n",
        "#         self.pos = nn.Embedding(512, d)\n",
        "#         self.layers = nn.ModuleList([\n",
        "#             nn.ModuleList([\n",
        "#                 MultiHeadAttention(d,h),\n",
        "#                 MultiHeadAttention(d,h),\n",
        "#                 nn.Sequential(nn.Linear(d,ff), nn.ReLU(), nn.Linear(ff,d)),\n",
        "#                 nn.LayerNorm(d),\n",
        "#                 nn.LayerNorm(d),\n",
        "#                 nn.LayerNorm(d)\n",
        "#             ]) for _ in range(layers)\n",
        "#         ])\n",
        "#         self.out = nn.Linear(d, vocab)\n",
        "\n",
        "#     def forward(self, y, enc, mask):\n",
        "#         B,T = y.size()\n",
        "#         x = self.emb(y) + self.pos(torch.arange(T, device=y.device))\n",
        "#         for sa,ca,ff,n1,n2,n3 in self.layers:\n",
        "#             x = n1(x + sa(x,x,x,mask))\n",
        "#             x = n2(x + ca(x,enc,enc,None))\n",
        "#             x = n3(x + ff(x))\n",
        "#         return self.out(x)\n",
        "\n",
        "# # ============================================================\n",
        "# # 8Ô∏è‚É£ TRAINING\n",
        "# # ============================================================\n",
        "# encoder = Encoder(enc_vocab_size).to(device)\n",
        "# decoder = Decoder(VOCAB_SIZE).to(device)\n",
        "\n",
        "# opt = optim.Adam(\n",
        "#     list(encoder.parameters()) + list(decoder.parameters()),\n",
        "#     lr=3e-4\n",
        "# )\n",
        "\n",
        "# sql_loss_fn = nn.CrossEntropyLoss(ignore_index=token_to_id[\"<PAD>\"])\n",
        "# schema_loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# EPOCHS = 8\n",
        "\n",
        "# for ep in range(EPOCHS):\n",
        "#     total = 0\n",
        "#     for x,t,s,y in loader:\n",
        "#         x,t,s,y = x.to(device), t.to(device), s.to(device), y.to(device)\n",
        "\n",
        "#         enc_mask = (x!=0).unsqueeze(1).unsqueeze(2)\n",
        "#         enc_out, sch = encoder(x,t,enc_mask)\n",
        "\n",
        "#         dec_in, dec_tgt = y[:,:-1], y[:,1:]\n",
        "#         T = dec_in.size(1)\n",
        "#         causal = torch.tril(torch.ones(T,T,device=device)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "#         logits = decoder(dec_in, enc_out, causal)\n",
        "\n",
        "#         loss = sql_loss_fn(logits.reshape(-1, VOCAB_SIZE), dec_tgt.reshape(-1))\n",
        "#         loss += 0.3 * schema_loss_fn(sch*(t!=0), s*(t!=0))\n",
        "\n",
        "#         opt.zero_grad()\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "\n",
        "#         total += loss.item()\n",
        "\n",
        "#     print(f\"Epoch {ep+1} | Loss: {total/len(loader):.4f}\")\n",
        "\n",
        "# # ============================================================\n",
        "# # 9Ô∏è‚É£ SAVE MODEL\n",
        "# # ============================================================\n",
        "# torch.save({\n",
        "#     \"encoder\": encoder.state_dict(),\n",
        "#     \"decoder\": decoder.state_dict(),\n",
        "#     \"encoder_vocab\": encoder_vocab,\n",
        "#     \"decoder_vocab\": token_to_id\n",
        "# }, \"nl2sql_schema_aware.pt\")\n",
        "\n",
        "# print(\"‚úÖ Training complete & model saved\")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- Vocab ---\n",
        "with open(\"spider_processed.json\", \"r\") as f: data = json.load(f)\n",
        "\n",
        "enc_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "for ex in data:\n",
        "    for t in ex[\"tokens\"]:\n",
        "        if t not in enc_vocab: enc_vocab[t] = len(enc_vocab)\n",
        "\n",
        "SQL_TOKENS = [\"SELECT\",\"FROM\",\"WHERE\",\"JOIN\",\"ON\",\"AND\",\"GROUP\",\"BY\",\"ORDER\",\"LIMIT\",\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\",\"=\",\"<\",\">\",\"!=\",\"VALUE\",\",\",\"(\",\")\"]\n",
        "dec_vocab = [\"<PAD>\", \"<BOS>\", \"<EOS>\"] + SQL_TOKENS + [f\"T{i}\" for i in range(10)] + [f\"C{i}\" for i in range(50)]\n",
        "tok2id = {t: i for i, t in enumerate(dec_vocab)}\n",
        "id2tok = {i: t for t, i in tok2id.items()}\n",
        "\n",
        "class SpiderDataset(Dataset):\n",
        "    def __init__(self, data): self.data = data\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        x = [enc_vocab.get(t, 1) for t in ex[\"tokens\"]]\n",
        "        y = [tok2id[\"<BOS>\"]] + [tok2id.get(t, 0) for t in ex[\"sql_tokens\"]] + [tok2id[\"<EOS>\"]]\n",
        "        return torch.tensor(x), torch.tensor(ex[\"token_types\"]), torch.tensor(y)\n",
        "\n",
        "def collate(batch):\n",
        "    x, t, y = zip(*batch)\n",
        "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
        "    t = nn.utils.rnn.pad_sequence(t, batch_first=True)\n",
        "    y = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
        "    return x, t, y\n",
        "\n",
        "# --- Transformer Architecture ---\n",
        "class TransformerSLM(nn.Module):\n",
        "    def __init__(self, enc_size, dec_size, d=256):\n",
        "        super().__init__()\n",
        "        self.enc_emb = nn.Embedding(enc_size, d)\n",
        "        self.type_emb = nn.Embedding(3, d)\n",
        "        self.dec_emb = nn.Embedding(dec_size, d)\n",
        "        self.tf = nn.Transformer(d_model=d, batch_first=True, nhead=8, num_encoder_layers=3, num_decoder_layers=3)\n",
        "        self.fc = nn.Linear(d, dec_size)\n",
        "\n",
        "    def forward(self, x, t, y):\n",
        "        # Create masks\n",
        "        y_mask = self.tf.generate_square_subsequent_mask(y.size(1)).to(y.device)\n",
        "        x_emb = self.enc_emb(x) + self.type_emb(t)\n",
        "        y_emb = self.dec_emb(y)\n",
        "        out = self.tf(x_emb, y_emb, tgt_mask=y_mask)\n",
        "        return self.fc(out)\n",
        "\n",
        "# --- Training Loop ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerSLM(len(enc_vocab), len(dec_vocab)).to(device)\n",
        "loader = DataLoader(SpiderDataset(data), batch_size=16, shuffle=True, collate_fn=collate)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(8):\n",
        "    total_loss = 0\n",
        "    for x, t, y in loader:\n",
        "        x, t, y = x.to(device), t.to(device), y.to(device)\n",
        "        y_in, y_out = y[:, :-1], y[:, 1:]\n",
        "        logits = model(x, t, y_in)\n",
        "        loss = criterion(logits.reshape(-1, len(dec_vocab)), y_out.reshape(-1))\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "torch.save({\"model\": model.state_dict(), \"enc\": enc_vocab, \"dec\": tok2id}, \"slm_v1.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u9tC5hZ5M0z",
        "outputId": "bcc5c86d-70ca-4303-cd9e-be62ae6b17a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 1.0168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================\n",
        "# # NL2SQL INFERENCE (FINAL ‚Äì FIXED, NON-LOOPING)\n",
        "# # ============================================================\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import math\n",
        "# import re\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(\"Using device:\", device)\n",
        "\n",
        "# # ---------------- LOAD MODEL ----------------\n",
        "# ckpt = torch.load(\"nl2sql_schema_aware.pt\", map_location=device)\n",
        "\n",
        "# encoder_vocab = ckpt[\"encoder_vocab\"]\n",
        "# decoder_vocab = ckpt[\"decoder_vocab\"]\n",
        "# id_to_token = {v:k for k,v in decoder_vocab.items()}\n",
        "\n",
        "# VOCAB_SIZE = len(decoder_vocab)\n",
        "\n",
        "# # ---------------- MODEL DEFINITIONS ----------------\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#     def __init__(self, d, h):\n",
        "#         super().__init__()\n",
        "#         self.h = h\n",
        "#         self.dk = d // h\n",
        "#         self.qkv = nn.Linear(d, d*3)\n",
        "#         self.o = nn.Linear(d, d)\n",
        "\n",
        "#     def forward(self, q,k,v,mask=None):\n",
        "#         B,T,D = q.size()\n",
        "#         q,k,v = self.qkv(q).chunk(3,dim=-1)\n",
        "#         q = q.view(B,T,self.h,self.dk).transpose(1,2)\n",
        "#         k = k.view(B,-1,self.h,self.dk).transpose(1,2)\n",
        "#         v = v.view(B,-1,self.h,self.dk).transpose(1,2)\n",
        "#         scores = q @ k.transpose(-2,-1) / math.sqrt(self.dk)\n",
        "#         if mask is not None:\n",
        "#             scores = scores.masked_fill(mask==0, -1e9)\n",
        "#         out = (scores.softmax(-1) @ v)\n",
        "#         out = out.transpose(1,2).contiguous().view(B,T,D)\n",
        "#         return self.o(out)\n",
        "\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, vocab, d=256, h=8, ff=1024, layers=4):\n",
        "#         super().__init__()\n",
        "#         self.emb = nn.Embedding(vocab, d, padding_idx=0)\n",
        "#         self.type_emb = nn.Embedding(3, d)\n",
        "#         self.layers = nn.ModuleList([\n",
        "#             nn.ModuleList([\n",
        "#                 MultiHeadAttention(d,h),\n",
        "#                 nn.Sequential(nn.Linear(d,ff), nn.ReLU(), nn.Linear(ff,d)),\n",
        "#                 nn.LayerNorm(d),\n",
        "#                 nn.LayerNorm(d)\n",
        "#             ]) for _ in range(layers)\n",
        "#         ])\n",
        "\n",
        "#     def forward(self, x, t, mask):\n",
        "#         x = self.emb(x) + self.type_emb(t)\n",
        "#         for attn, ff, n1, n2 in self.layers:\n",
        "#             x = n1(x + attn(x,x,x,mask))\n",
        "#             x = n2(x + ff(x))\n",
        "#         return x\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, vocab, d=256, h=8, ff=1024, layers=4):\n",
        "#         super().__init__()\n",
        "#         self.emb = nn.Embedding(vocab, d)\n",
        "#         self.pos = nn.Embedding(512, d)\n",
        "#         self.layers = nn.ModuleList([\n",
        "#             nn.ModuleList([\n",
        "#                 MultiHeadAttention(d,h),\n",
        "#                 MultiHeadAttention(d,h),\n",
        "#                 nn.Sequential(nn.Linear(d,ff), nn.ReLU(), nn.Linear(ff,d)),\n",
        "#                 nn.LayerNorm(d),\n",
        "#                 nn.LayerNorm(d),\n",
        "#                 nn.LayerNorm(d)\n",
        "#             ]) for _ in range(layers)\n",
        "#         ])\n",
        "#         self.out = nn.Linear(d, vocab)\n",
        "\n",
        "#     def forward(self, y, enc, mask):\n",
        "#         B,T = y.size()\n",
        "#         x = self.emb(y) + self.pos(torch.arange(T, device=y.device))\n",
        "#         for sa,ca,ff,n1,n2,n3 in self.layers:\n",
        "#             x = n1(x + sa(x,x,x,mask))\n",
        "#             x = n2(x + ca(x,enc,enc,None))\n",
        "#             x = n3(x + ff(x))\n",
        "#         return self.out(x)\n",
        "\n",
        "# encoder = Encoder(len(encoder_vocab)).to(device)\n",
        "# decoder = Decoder(len(decoder_vocab)).to(device)\n",
        "\n",
        "# encoder.load_state_dict(ckpt[\"encoder\"], strict=False)\n",
        "# decoder.load_state_dict(ckpt[\"decoder\"])\n",
        "\n",
        "# encoder.eval()\n",
        "# decoder.eval()\n",
        "# print(\"‚úÖ Model loaded successfully\")\n",
        "\n",
        "# # ---------------- HELPERS ----------------\n",
        "# def tokenize_question(text):\n",
        "#     text = text.lower()\n",
        "#     text = re.sub(r\"[^a-z0-9_ ]\", \" \", text)\n",
        "#     return text.split()\n",
        "\n",
        "# def build_encoder_input(question, schema):\n",
        "#     tokens, types = [], []\n",
        "#     for tok in tokenize_question(question):\n",
        "#         tokens.append(tok)\n",
        "#         types.append(0)\n",
        "#     for table, cols in schema.items():\n",
        "#         tokens.append(table); types.append(1)\n",
        "#         for col in cols:\n",
        "#             tokens.append(col); types.append(2)\n",
        "\n",
        "#     ids = [encoder_vocab.get(t, encoder_vocab[\"<UNK>\"]) for t in tokens]\n",
        "#     return (\n",
        "#         torch.tensor(ids).unsqueeze(0).to(device),\n",
        "#         torch.tensor(types).unsqueeze(0).to(device)\n",
        "#     )\n",
        "\n",
        "# # ---------------- SAFE GREEDY DECODING ----------------\n",
        "# def generate_sql(question, schema, max_len=40):\n",
        "#     x,t = build_encoder_input(question, schema)\n",
        "#     mask = (x!=0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         enc = encoder(x,t,mask)\n",
        "\n",
        "#     # Force SELECT only once\n",
        "#     cur = torch.tensor([[decoder_vocab[\"SELECT\"]]], device=device)\n",
        "#     generated = [\"SELECT\"]\n",
        "\n",
        "#     used = set([\"SELECT\"])\n",
        "\n",
        "#     for _ in range(max_len):\n",
        "#         T = cur.size(1)\n",
        "#         causal = torch.tril(torch.ones(T,T,device=device)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             logits = decoder(cur, enc, causal)[0,-1]\n",
        "\n",
        "#         # repetition penalty\n",
        "#         for tok in used:\n",
        "#             logits[decoder_vocab[tok]] -= 2.0\n",
        "\n",
        "#         next_id = logits.argmax().item()\n",
        "#         token = id_to_token[next_id]\n",
        "\n",
        "#         if token == \"<EOS>\":\n",
        "#             break\n",
        "\n",
        "#         generated.append(token)\n",
        "#         used.add(token)\n",
        "#         cur = torch.cat([cur, torch.tensor([[next_id]], device=device)], dim=1)\n",
        "\n",
        "#     return \" \".join(generated)\n",
        "\n",
        "# # ---------------- TEST ----------------\n",
        "# schema = {\n",
        "#      \"department\": [\n",
        "#         \"Department_ID\",\n",
        "#         \"Name\",\n",
        "#         \"Creation\",\n",
        "#         \"Ranking\",\n",
        "#         \"Budget_in_Billions\",\n",
        "#         \"Num_Employees\"\n",
        "#       ],\n",
        "#       \"head\": [\n",
        "#         \"head_ID\",\n",
        "#         \"name\",\n",
        "#         \"born_state\",\n",
        "#         \"age\"\n",
        "#       ],\n",
        "#       \"management\": [\n",
        "#         \"department_ID\",\n",
        "#         \"head_ID\",\n",
        "#         \"temporary_acting\"\n",
        "#       ]\n",
        "# }\n",
        "\n",
        "# question = \"how many heads of the departments are older than 56\"\n",
        "\n",
        "# print(\"\\nüü¢ Generated SQL:\")\n",
        "# print(generate_sql(question, schema))\n",
        "import torch\n",
        "\n",
        "def generate_sql(question, schema, model_path=\"slm_v1.pt\"):\n",
        "    ckpt = torch.load(model_path)\n",
        "    enc_v, dec_v = ckpt[\"enc\"], ckpt[\"dec\"]\n",
        "    id2t = {i: t for t, i in dec_v.items()}\n",
        "\n",
        "    model = TransformerSLM(len(enc_v), len(dec_v)).to(device)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare input\n",
        "    q_toks = question.lower().split()\n",
        "    s_toks, s_types = [], []\n",
        "    for t, cols in schema.items():\n",
        "        s_toks.append(t); s_types.append(1)\n",
        "        for c in cols: s_toks.append(c); s_types.append(2)\n",
        "\n",
        "    x = torch.tensor([[enc_v.get(t, 1) for t in q_toks + s_toks]]).to(device)\n",
        "    t = torch.tensor([[0]*len(q_toks) + s_types]).to(device)\n",
        "\n",
        "    # Greedy Search with Repetition Penalty\n",
        "    y = torch.tensor([[dec_v[\"<BOS>\"]]]).to(device)\n",
        "    for _ in range(30):\n",
        "        with torch.no_grad():\n",
        "            logits = model(x, t, y)[:, -1, :]\n",
        "\n",
        "            # Repetition Penalty: reduce probability of already seen tokens\n",
        "            for token_id in set(y[0].tolist()):\n",
        "                logits[0, token_id] *= 0.8\n",
        "\n",
        "            next_id = logits.argmax(dim=-1).item()\n",
        "            if next_id == dec_v[\"<EOS>\"]: break\n",
        "            y = torch.cat([y, torch.tensor([[next_id]]).to(device)], dim=1)\n",
        "\n",
        "    return \" \".join([id2t[i] for i in y[0].tolist() if i not in [dec_v[\"<BOS>\"], 0]])\n",
        "\n",
        "# Test\n",
        "schema_test = {\"head\": [\"name\", \"age\"], \"department\": [\"id\", \"name\"]}\n",
        "print(\"\\nGenerated SQL:\", generate_sql(\"how many heads are there\", schema_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmxeGuWj5M7x",
        "outputId": "98c85b31-8a97-4f38-db6c-ae8ce0e2e018"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "‚úÖ Model loaded successfully\n",
            "\n",
            "üü¢ Generated SQL:\n",
            "SELECT T1 = T1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}