{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sympy --upgrade\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import random\n",
        "import string\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "SIHoGolTG3pe",
        "outputId": "fa0c5fa7-ab12-4f79-d174-0aa547658abb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (1.14.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy) (1.3.0)\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_token(min_len=4, max_len=10):\n",
        "    return ''.join(random.choices(string.ascii_lowercase,\n",
        "                                  k=random.randint(min_len, max_len)))\n",
        "\n",
        "def generate_schema():\n",
        "\n",
        "    schema = {}\n",
        "\n",
        "    num_tables = random.randint(1,5)\n",
        "\n",
        "    for _ in range(num_tables):\n",
        "\n",
        "        table = random_token()\n",
        "\n",
        "        cols = {random_token() for _ in range(random.randint(3,7))}\n",
        "        cols.add(\"id\")  # anchor column\n",
        "\n",
        "        schema[table] = list(cols)\n",
        "\n",
        "    return schema\n"
      ],
      "metadata": {
        "id": "AYNwd3e1G3ro"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def serialize_schema(schema):\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    for t, cols in schema.items():\n",
        "\n",
        "        random.shuffle(cols)\n",
        "\n",
        "        parts.append(\n",
        "            f\"{t} : \" + \" , \".join(cols)\n",
        "        )\n",
        "\n",
        "    return \" <SCHEMA> \" + \" | \".join(parts) + \" </SCHEMA> \"\n"
      ],
      "metadata": {
        "id": "DbUswd0OG3t8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AGGS = [\"SUM\",\"AVG\",\"COUNT\",\"MAX\",\"MIN\"]\n",
        "OPS = [\">\",\"<\",\">=\",\"<=\",\"!=\"]\n",
        "JOIN_TYPES = [\"JOIN\",\"LEFT JOIN\",\"RIGHT JOIN\"]\n",
        "SORT = [\"ASC\",\"DESC\"]\n",
        "\n",
        "def generate_example():\n",
        "\n",
        "    schema = generate_schema()\n",
        "\n",
        "    tables = list(schema.keys())\n",
        "    main = random.choice(tables)\n",
        "    cols = schema[main]\n",
        "\n",
        "    intent = random.choice([\n",
        "        \"SELECT\",\"WHERE\",\"HAVING\",\n",
        "        \"ORDER\",\"LIMIT\",\"JOIN\",\"NESTED\"\n",
        "    ])\n",
        "\n",
        "    ################ SELECT ################\n",
        "\n",
        "    if intent == \"SELECT\":\n",
        "\n",
        "        chosen = random.sample(cols, random.randint(1,min(3,len(cols))))\n",
        "\n",
        "        q = f\"get {', '.join(chosen)} from {main}\"\n",
        "        sql = f\"SELECT {', '.join(chosen)} FROM {main}\"\n",
        "\n",
        "    ################ WHERE ################\n",
        "\n",
        "    elif intent == \"WHERE\":\n",
        "\n",
        "        c1, c2 = random.sample(cols,2)\n",
        "        op = random.choice(OPS)\n",
        "        val = random.randint(1,1000)\n",
        "\n",
        "        q = f\"find {c1} from {main} where {c2} {op} {val}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "        SELECT {c1}\n",
        "        FROM {main}\n",
        "        WHERE {c2} {op} {val}\n",
        "        \"\"\"\n",
        "\n",
        "    ################ HAVING ################\n",
        "\n",
        "    elif intent == \"HAVING\":\n",
        "\n",
        "        group = random.choice(cols)\n",
        "        agg_col = random.choice(cols)\n",
        "\n",
        "        agg = random.choice(AGGS)\n",
        "        op = random.choice(OPS)\n",
        "        val = random.randint(1,500)\n",
        "\n",
        "        q = f\"group {main} by {group} having {agg.lower()} {agg_col} {op} {val}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "        SELECT {group}, {agg}({agg_col})\n",
        "        FROM {main}\n",
        "        GROUP BY {group}\n",
        "        HAVING {agg}({agg_col}) {op} {val}\n",
        "        \"\"\"\n",
        "\n",
        "    ################ ORDER ################\n",
        "\n",
        "    elif intent == \"ORDER\":\n",
        "\n",
        "        col = random.choice(cols)\n",
        "        direction = random.choice(SORT)\n",
        "\n",
        "        q = f\"order {main} by {col} {direction.lower()}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "        SELECT {col}\n",
        "        FROM {main}\n",
        "        ORDER BY {col} {direction}\n",
        "        \"\"\"\n",
        "\n",
        "    ################ LIMIT ################\n",
        "\n",
        "    elif intent == \"LIMIT\":\n",
        "\n",
        "        col = random.choice(cols)\n",
        "        limit = random.randint(1,20)\n",
        "\n",
        "        q = f\"top {limit} rows of {col} from {main}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "        SELECT {col}\n",
        "        FROM {main}\n",
        "        LIMIT {limit}\n",
        "        \"\"\"\n",
        "\n",
        "    ################ JOIN ################\n",
        "\n",
        "    elif intent == \"JOIN\" and len(tables) > 1:\n",
        "\n",
        "        t2 = random.choice([t for t in tables if t!=main])\n",
        "\n",
        "        c1 = random.choice(schema[main])\n",
        "        c2 = random.choice(schema[t2])\n",
        "\n",
        "        join = random.choice(JOIN_TYPES)\n",
        "\n",
        "        q = f\"join {main} with {t2}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "        SELECT {main}.{c1}, {t2}.{c2}\n",
        "        FROM {main}\n",
        "        {join} {t2}\n",
        "        ON {main}.{c1} = {t2}.{c2}\n",
        "        \"\"\"\n",
        "\n",
        "    ################ NESTED ################\n",
        "\n",
        "    else:\n",
        "\n",
        "        col = random.choice(cols)\n",
        "        agg = random.choice(AGGS)\n",
        "\n",
        "        q = f\"find {col} from {main} greater than average\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "        SELECT {col}\n",
        "        FROM {main}\n",
        "        WHERE {col} >\n",
        "        (SELECT {agg}({col}) FROM {main})\n",
        "        \"\"\"\n",
        "\n",
        "    full_question = q + serialize_schema(schema)\n",
        "\n",
        "    return {\n",
        "        \"question\": full_question.lower(),\n",
        "        \"sql\": \" \".join(sql.split())\n",
        "    }\n"
      ],
      "metadata": {
        "id": "v3eIvv8cG3we"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = [generate_example() for _ in range(60000)]\n",
        "\n",
        "with open(\"nl2sql.json\",\"w\") as f:\n",
        "    json.dump(DATA,f)\n",
        "\n",
        "print(\"Dataset Ready üöÄ\")\n"
      ],
      "metadata": {
        "id": "vcbaAioLG3yx",
        "outputId": "12e073bf-b056-452f-c054-cbe0566c67ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Ready üöÄ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sql_tokenize(sql):\n",
        "\n",
        "    return re.findall(\n",
        "        r\"[A-Za-z_]+\\.[A-Za-z_]+\"\n",
        "        r\"|>=|<=|!=|=|>|<\"\n",
        "        r\"|\\bselect\\b|\\bfrom\\b|\\bwhere\\b|\\bjoin\\b|\\bon\\b\"\n",
        "        r\"|\\bgroup\\b|\\bby\\b|\\bhaving\\b|\\border\\b|\\blimit\\b\"\n",
        "        r\"|\\bavg\\b|\\bsum\\b|\\bcount\\b|\\bmax\\b|\\bmin\\b\"\n",
        "        r\"|\\(|\\)|,\"\n",
        "        r\"|[A-Za-z_]+\"\n",
        "        r\"|\\d+\",\n",
        "        sql.lower()\n",
        "    )\n",
        "\n",
        "\n",
        "ENC_VOCAB={\"<PAD>\":0,\"<UNK>\":1}\n",
        "DEC_VOCAB={\"<PAD>\":0,\"<UNK>\":1,\"<BOS>\":2,\"<EOS>\":3}\n",
        "\n",
        "def add(vocab,t):\n",
        "    if t not in vocab:\n",
        "        vocab[t]=len(vocab)\n",
        "\n",
        "for ex in DATA:\n",
        "\n",
        "    for t in ex[\"question\"].split():\n",
        "        add(ENC_VOCAB,t)\n",
        "\n",
        "    for t in sql_tokenize(ex[\"sql\"]):\n",
        "        add(DEC_VOCAB,t)\n",
        "\n",
        "print(len(ENC_VOCAB), len(DEC_VOCAB))\n"
      ],
      "metadata": {
        "id": "Qxu3RqGTG31Q",
        "outputId": "824e4760-347b-48e8-daf1-30859e98c229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1066647 144923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_LEN = 96\n",
        "TGT_LEN = 48\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "\n",
        "    def __init__(self,data):\n",
        "        self.data=data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "\n",
        "        ex=self.data[i]\n",
        "\n",
        "        src=[ENC_VOCAB.get(t,1) for t in ex[\"question\"].split()][:SRC_LEN]\n",
        "        src+=[0]*(SRC_LEN-len(src))\n",
        "\n",
        "        tgt=[DEC_VOCAB[\"<BOS>\"]] + \\\n",
        "            [DEC_VOCAB.get(t,1) for t in sql_tokenize(ex[\"sql\"])] + \\\n",
        "            [DEC_VOCAB[\"<EOS>\"]]\n",
        "\n",
        "        tgt=tgt[:TGT_LEN]\n",
        "        tgt+=[0]*(TGT_LEN-len(tgt))\n",
        "\n",
        "        return torch.tensor(src),torch.tensor(tgt)\n"
      ],
      "metadata": {
        "id": "8KvWulYyG33t"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "# MODEL CONFIG  (VERY IMPORTANT ‚Äî DO NOT INCREASE)\n",
        "############################################\n",
        "\n",
        "D_MODEL = 256        # Sweet spot (fast + stable)\n",
        "N_HEADS = 4\n",
        "NUM_LAYERS = 2\n",
        "FF_DIM = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "\n",
        "############################################\n",
        "# POSITIONAL ENCODING\n",
        "############################################\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) *\n",
        "            (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "############################################\n",
        "# ENCODER\n",
        "############################################\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size,\n",
        "            D_MODEL,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        self.pos = PositionalEncoding(D_MODEL)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=D_MODEL,\n",
        "            nhead=N_HEADS,\n",
        "            dim_feedforward=FF_DIM,\n",
        "            dropout=DROPOUT,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=NUM_LAYERS\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        padding_mask = (x == 0)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos(x)\n",
        "\n",
        "        return self.encoder(\n",
        "            x,\n",
        "            src_key_padding_mask=padding_mask\n",
        "        )\n",
        "\n",
        "\n",
        "############################################\n",
        "# DECODER\n",
        "############################################\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size,\n",
        "            D_MODEL\n",
        "        )\n",
        "\n",
        "        self.pos = PositionalEncoding(D_MODEL)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=D_MODEL,\n",
        "            nhead=N_HEADS,\n",
        "            dim_feedforward=FF_DIM,\n",
        "            dropout=DROPOUT,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_layers=NUM_LAYERS\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(D_MODEL, vocab_size)\n",
        "\n",
        "        # ‚≠ê weight tying (VERY IMPORTANT for NL‚ÜíSQL)\n",
        "        self.fc.weight = self.embedding.weight\n",
        "\n",
        "\n",
        "    def forward(self, y, memory, src_padding_mask):\n",
        "\n",
        "        L = y.size(1)\n",
        "\n",
        "        # causal mask prevents cheating\n",
        "        causal_mask = torch.triu(\n",
        "            torch.ones(L, L, device=y.device),\n",
        "            diagonal=1\n",
        "        ).bool()\n",
        "\n",
        "        y = self.embedding(y)\n",
        "        y = self.pos(y)\n",
        "\n",
        "        output = self.decoder(\n",
        "            y,\n",
        "            memory,\n",
        "            tgt_mask=causal_mask,\n",
        "            memory_key_padding_mask=src_padding_mask\n",
        "        )\n",
        "\n",
        "        return self.fc(output)\n"
      ],
      "metadata": {
        "id": "DOtmtIYoG36C"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = train_test_split(DATA, test_size=0.1)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    NL2SQLDataset(train_data),\n",
        "    batch_size=16,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    NL2SQLDataset(val_data),\n",
        "    batch_size=16\n",
        ")\n"
      ],
      "metadata": {
        "id": "KMtr0laAG38L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = Encoder(len(ENC_VOCAB)).to(device)\n",
        "dec = Decoder(len(DEC_VOCAB)).to(device)\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    list(enc.parameters()) + list(dec.parameters()),\n",
        "    lr=1e-4\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,T_max=12\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0,label_smoothing=0.05)\n",
        "\n",
        "scaler = torch.amp.GradScaler(\"cuda\")\n"
      ],
      "metadata": {
        "id": "OrlYRsN_G3-d",
        "outputId": "e3114a6e-3dbf-4249-fb97-e39b16bfa0a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"nl2sql_checkpoint.pt\"\n",
        "start_epoch = 0\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH,map_location=device)\n",
        "\n",
        "    enc.load_state_dict(checkpoint[\"encoder\"])\n",
        "    dec.load_state_dict(checkpoint[\"decoder\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    start_epoch = checkpoint[\"epoch\"]+1\n",
        "\n",
        "EPOCHS = 12\n",
        "\n",
        "for epoch in range(start_epoch,EPOCHS):\n",
        "\n",
        "    enc.train()\n",
        "    dec.train()\n",
        "\n",
        "    total=0\n",
        "\n",
        "    for x,y in train_loader:\n",
        "\n",
        "        x,y=x.to(device),y.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "\n",
        "            mem=enc(x)\n",
        "            out=dec(y[:,:-1],mem,(x==0))\n",
        "\n",
        "            loss=loss_fn(\n",
        "                out.reshape(-1,len(DEC_VOCAB)),\n",
        "                y[:,1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total+=loss.item()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Train Loss:\",total/len(train_loader))\n",
        "\n",
        "    torch.save({\n",
        "        \"epoch\":epoch,\n",
        "        \"encoder\":enc.state_dict(),\n",
        "        \"decoder\":dec.state_dict(),\n",
        "        \"optimizer\":optimizer.state_dict(),\n",
        "        \"enc_vocab\":ENC_VOCAB,\n",
        "        \"dec_vocab\":DEC_VOCAB\n",
        "    },CHECKPOINT_PATH)\n",
        "\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "NmtDjT7eG4Av",
        "outputId": "6a7e507f-d57b-45c5-db1f-2f6ad9cc5223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Train Loss: 18.812972576282643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MOhiNI3BG4DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mKnTrUCMG4FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9P7HtUEfG4Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6bru_LeCG4LD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}