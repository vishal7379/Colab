{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip -d /content/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hgZk8SPyoDPG",
        "outputId": "882aa1e5-1ce5-4723-d2d1-0add8634560d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "  inflating: /content/spider/README.txt  \n",
            "  inflating: /content/spider/database/academic/academic.sqlite  \n",
            "  inflating: /content/spider/database/academic/schema.sql  \n",
            "  inflating: /content/spider/database/activity_1/activity_1.sqlite  \n",
            "  inflating: /content/spider/database/activity_1/schema.sql  \n",
            "  inflating: /content/spider/database/aircraft/aircraft.sqlite  \n",
            "  inflating: /content/spider/database/aircraft/schema.sql  \n",
            "  inflating: /content/spider/database/allergy_1/allergy_1.sqlite  \n",
            "  inflating: /content/spider/database/allergy_1/schema.sql  \n",
            "  inflating: /content/spider/database/apartment_rentals/apartment_rentals.sqlite  \n",
            "  inflating: /content/spider/database/apartment_rentals/schema.sql  \n",
            "  inflating: /content/spider/database/architecture/architecture.sqlite  \n",
            "  inflating: /content/spider/database/architecture/schema.sql  \n",
            "  inflating: /content/spider/database/assets_maintenance/assets_maintenance.sqlite  \n",
            "  inflating: /content/spider/database/assets_maintenance/schema.sql  \n",
            "  inflating: /content/spider/database/baseball_1/baseball_1.sqlite  \n",
            "  inflating: /content/spider/database/baseball_1/schema.sql  \n",
            "  inflating: /content/spider/database/battle_death/battle_death.sqlite  \n",
            "  inflating: /content/spider/database/battle_death/schema.sql  \n",
            "  inflating: /content/spider/database/behavior_monitoring/behavior_monitoring.sqlite  \n",
            "  inflating: /content/spider/database/behavior_monitoring/schema.sql  \n",
            "  inflating: /content/spider/database/bike_1/bike_1.sqlite  \n",
            "  inflating: /content/spider/database/bike_1/schema.sql  \n",
            "  inflating: /content/spider/database/body_builder/body_builder.sqlite  \n",
            "  inflating: /content/spider/database/body_builder/schema.sql  \n",
            "  inflating: /content/spider/database/book_2/book_2.sqlite  \n",
            "  inflating: /content/spider/database/book_2/schema.sql  \n",
            "  inflating: /content/spider/database/browser_web/browser_web.sqlite  \n",
            "  inflating: /content/spider/database/browser_web/schema.sql  \n",
            "  inflating: /content/spider/database/candidate_poll/candidate_poll.sqlite  \n",
            "  inflating: /content/spider/database/candidate_poll/schema.sql  \n",
            "  inflating: /content/spider/database/car_1/annotation.json  \n",
            "  inflating: /content/spider/database/car_1/car_1.json  \n",
            "  inflating: /content/spider/database/car_1/car_1.sql  \n",
            "  inflating: /content/spider/database/car_1/car_1.sqlite  \n",
            "  inflating: /content/spider/database/car_1/data_csv/README.CARS.TXT  \n",
            "  inflating: /content/spider/database/car_1/data_csv/car-makers.csv  \n",
            "  inflating: /content/spider/database/car_1/data_csv/car-names.csv  \n",
            "  inflating: /content/spider/database/car_1/data_csv/cars-data.csv  \n",
            "  inflating: /content/spider/database/car_1/data_csv/cars.desc  \n",
            "  inflating: /content/spider/database/car_1/data_csv/continents.csv  \n",
            "  inflating: /content/spider/database/car_1/data_csv/countries.csv  \n",
            "  inflating: /content/spider/database/car_1/data_csv/model-list.csv  \n",
            "  inflating: /content/spider/database/car_1/link.txt  \n",
            "  inflating: /content/spider/database/car_1/q.txt  \n",
            "  inflating: /content/spider/database/chinook_1/annotation.json  \n",
            "  inflating: /content/spider/database/chinook_1/chinook_1.sqlite  \n",
            "  inflating: /content/spider/database/cinema/cinema.sqlite  \n",
            "  inflating: /content/spider/database/cinema/schema.sql  \n",
            "  inflating: /content/spider/database/city_record/city_record.sqlite  \n",
            "  inflating: /content/spider/database/city_record/schema.sql  \n",
            "  inflating: /content/spider/database/climbing/climbing.sqlite  \n",
            "  inflating: /content/spider/database/climbing/schema.sql  \n",
            "  inflating: /content/spider/database/club_1/club_1.sqlite  \n",
            "  inflating: /content/spider/database/club_1/schema.sql  \n",
            "  inflating: /content/spider/database/coffee_shop/coffee_shop.sqlite  \n",
            "  inflating: /content/spider/database/coffee_shop/schema.sql  \n",
            "  inflating: /content/spider/database/college_1/TinyCollege.sql  \n",
            "  inflating: /content/spider/database/college_1/college_1.sqlite  \n",
            "  inflating: /content/spider/database/college_1/link.txt  \n",
            "  inflating: /content/spider/database/college_2/TextBookExampleSchema.sql  \n",
            "  inflating: /content/spider/database/college_2/college_2.sqlite  \n",
            "  inflating: /content/spider/database/college_2/link.txt  \n",
            "  inflating: /content/spider/database/college_3/college_3.sqlite  \n",
            "  inflating: /content/spider/database/college_3/schema.sql  \n",
            "  inflating: /content/spider/database/company_1/company_1.sqlite  \n",
            "  inflating: /content/spider/database/company_1/link.txt  \n",
            "  inflating: /content/spider/database/company_employee/company_employee.sqlite  \n",
            "  inflating: /content/spider/database/company_employee/schema.sql  \n",
            "  inflating: /content/spider/database/company_office/company_office.sqlite  \n",
            "  inflating: /content/spider/database/company_office/schema.sql  \n",
            "  inflating: /content/spider/database/concert_singer/concert_singer.sqlite  \n",
            "  inflating: /content/spider/database/concert_singer/schema.sql  \n",
            "  inflating: /content/spider/database/county_public_safety/county_public_safety.sqlite  \n",
            "  inflating: /content/spider/database/county_public_safety/schema.sql  \n",
            "  inflating: /content/spider/database/course_teach/course_teach.sqlite  \n",
            "  inflating: /content/spider/database/course_teach/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Doc_Control_Systems/cre_Doc_Control_Systems.sqlite  \n",
            "  inflating: /content/spider/database/cre_Doc_Control_Systems/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Doc_Template_Mgt/cre_Doc_Template_Mgt.sqlite  \n",
            "  inflating: /content/spider/database/cre_Doc_Template_Mgt/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Doc_Tracking_DB/cre_Doc_Tracking_DB.sqlite  \n",
            "  inflating: /content/spider/database/cre_Doc_Tracking_DB/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Docs_and_Epenses/cre_Docs_and_Epenses.sqlite  \n",
            "  inflating: /content/spider/database/cre_Docs_and_Epenses/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Drama_Workshop_Groups/cre_Drama_Workshop_Groups.sqlite  \n",
            "  inflating: /content/spider/database/cre_Drama_Workshop_Groups/schema.sql  \n",
            "  inflating: /content/spider/database/cre_Theme_park/cre_Theme_park.sqlite  \n",
            "  inflating: /content/spider/database/cre_Theme_park/schema.sql  \n",
            "  inflating: /content/spider/database/csu_1/csu_1.sqlite  \n",
            "  inflating: /content/spider/database/csu_1/schema.sql  \n",
            "  inflating: /content/spider/database/culture_company/culture_company.sqlite  \n",
            "  inflating: /content/spider/database/culture_company/schema.sql  \n",
            "  inflating: /content/spider/database/customer_complaints/customer_complaints.sqlite  \n",
            "  inflating: /content/spider/database/customer_complaints/schema.sql  \n",
            "  inflating: /content/spider/database/customer_deliveries/customer_deliveries.sqlite  \n",
            "  inflating: /content/spider/database/customer_deliveries/schema.sql  \n",
            "  inflating: /content/spider/database/customers_and_addresses/customers_and_addresses.sqlite  \n",
            "  inflating: /content/spider/database/customers_and_addresses/schema.sql  \n",
            "  inflating: /content/spider/database/customers_and_invoices/customers_and_invoices.sqlite  \n",
            "  inflating: /content/spider/database/customers_and_invoices/schema.sql  \n",
            "  inflating: /content/spider/database/customers_and_products_contacts/customers_and_products_contacts.sqlite  \n",
            "  inflating: /content/spider/database/customers_and_products_contacts/schema.sql  \n",
            "  inflating: /content/spider/database/customers_campaigns_ecommerce/customers_campaigns_ecommerce.sqlite  \n",
            "  inflating: /content/spider/database/customers_campaigns_ecommerce/schema.sql  \n",
            "  inflating: /content/spider/database/customers_card_transactions/customers_card_transactions.sqlite  \n",
            "  inflating: /content/spider/database/customers_card_transactions/schema.sql  \n",
            "  inflating: /content/spider/database/debate/debate.sqlite  \n",
            "  inflating: /content/spider/database/debate/schema.sql  \n",
            "  inflating: /content/spider/database/decoration_competition/decoration_competition.sqlite  \n",
            "  inflating: /content/spider/database/decoration_competition/schema.sql  \n",
            "  inflating: /content/spider/database/department_management/department_management.sqlite  \n",
            "  inflating: /content/spider/database/department_management/schema.sql  \n",
            "  inflating: /content/spider/database/department_store/department_store.sqlite  \n",
            "  inflating: /content/spider/database/department_store/schema.sql  \n",
            "  inflating: /content/spider/database/device/device.sqlite  \n",
            "  inflating: /content/spider/database/device/schema.sql  \n",
            "  inflating: /content/spider/database/document_management/document_management.sqlite  \n",
            "  inflating: /content/spider/database/document_management/schema.sql  \n",
            "  inflating: /content/spider/database/dog_kennels/dog_kennels.sqlite  \n",
            "  inflating: /content/spider/database/dog_kennels/schema.sql  \n",
            "  inflating: /content/spider/database/dorm_1/dorm_1.sqlite  \n",
            "  inflating: /content/spider/database/dorm_1/schema.sql  \n",
            "  inflating: /content/spider/database/driving_school/driving_school.sqlite  \n",
            "  inflating: /content/spider/database/driving_school/schema.sql  \n",
            "  inflating: /content/spider/database/e_government/e_government.sqlite  \n",
            "  inflating: /content/spider/database/e_government/schema.sql  \n",
            "  inflating: /content/spider/database/e_learning/e_learning.sqlite  \n",
            "  inflating: /content/spider/database/e_learning/schema.sql  \n",
            "  inflating: /content/spider/database/election/election.sqlite  \n",
            "  inflating: /content/spider/database/election/schema.sql  \n",
            "  inflating: /content/spider/database/election_representative/election_representative.sqlite  \n",
            "  inflating: /content/spider/database/election_representative/schema.sql  \n",
            "  inflating: /content/spider/database/employee_hire_evaluation/employee_hire_evaluation.sqlite  \n",
            "  inflating: /content/spider/database/employee_hire_evaluation/schema.sql  \n",
            "  inflating: /content/spider/database/entertainment_awards/entertainment_awards.sqlite  \n",
            "  inflating: /content/spider/database/entertainment_awards/schema.sql  \n",
            "  inflating: /content/spider/database/entrepreneur/entrepreneur.sqlite  \n",
            "  inflating: /content/spider/database/entrepreneur/schema.sql  \n",
            "  inflating: /content/spider/database/epinions_1/epinions_1.sqlite  \n",
            "  inflating: /content/spider/database/farm/farm.sqlite  \n",
            "  inflating: /content/spider/database/farm/schema.sql  \n",
            "  inflating: /content/spider/database/film_rank/film_rank.sqlite  \n",
            "  inflating: /content/spider/database/film_rank/schema.sql  \n",
            "  inflating: /content/spider/database/flight_1/flight_1.sqlite  \n",
            "  inflating: /content/spider/database/flight_1/schema.sql  \n",
            "  inflating: /content/spider/database/flight_2/annotation.json  \n",
            "  inflating: /content/spider/database/flight_2/data_csv/README.AIRLINES.txt  \n",
            "  inflating: /content/spider/database/flight_2/data_csv/airlines.csv  \n",
            "  inflating: /content/spider/database/flight_2/data_csv/airports100.csv  \n",
            "  inflating: /content/spider/database/flight_2/data_csv/flights.csv  \n",
            "  inflating: /content/spider/database/flight_2/flight_2.json  \n",
            "  inflating: /content/spider/database/flight_2/flight_2.sql  \n",
            "  inflating: /content/spider/database/flight_2/flight_2.sqlite  \n",
            "  inflating: /content/spider/database/flight_2/link.txt  \n",
            "  inflating: /content/spider/database/flight_2/q.txt  \n",
            "  inflating: /content/spider/database/flight_4/flight_4.sqlite  \n",
            "  inflating: /content/spider/database/flight_4/link.txt  \n",
            "  inflating: /content/spider/database/flight_4/sql.txt  \n",
            "  inflating: /content/spider/database/flight_company/flight_company.sqlite  \n",
            "  inflating: /content/spider/database/flight_company/schema.sql  \n",
            "  inflating: /content/spider/database/formula_1/annotation.json  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/circuits.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/constructorResults.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/constructorStandings.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/constructors.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/driverStandings.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/drivers.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/lapTimes.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/pitStops.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/qualifying.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/races.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/results.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/seasons.csv  \n",
            "  inflating: /content/spider/database/formula_1/data_csv/status.csv  \n",
            "  inflating: /content/spider/database/formula_1/formula_1.splite  \n",
            "  inflating: /content/spider/database/formula_1/formula_1.sql  \n",
            "  inflating: /content/spider/database/formula_1/formula_1.sqlite  \n",
            "  inflating: /content/spider/database/game_1/game_1.sqlite  \n",
            "  inflating: /content/spider/database/game_1/schema.sql  \n",
            "  inflating: /content/spider/database/game_injury/game_injury.sqlite  \n",
            "  inflating: /content/spider/database/game_injury/schema.sql  \n",
            "  inflating: /content/spider/database/gas_company/gas_company.sqlite  \n",
            "  inflating: /content/spider/database/gas_company/schema.sql  \n",
            "  inflating: /content/spider/database/geo/geo.sqlite  \n",
            "  inflating: /content/spider/database/geo/schema.sql  \n",
            "  inflating: /content/spider/database/gymnast/gymnast.sqlite  \n",
            "  inflating: /content/spider/database/gymnast/schema.sql  \n",
            "  inflating: /content/spider/database/hospital_1/hospital_1.sqlite  \n",
            "  inflating: /content/spider/database/hospital_1/schema.sql  \n",
            "  inflating: /content/spider/database/hr_1/hr_1.sqlite  \n",
            "  inflating: /content/spider/database/hr_1/schema.sql  \n",
            "  inflating: /content/spider/database/icfp_1/icfp_1.sqlite  \n",
            "  inflating: /content/spider/database/icfp_1/link.txt  \n",
            "  inflating: /content/spider/database/icfp_1/q.txt  \n",
            "  inflating: /content/spider/database/imdb/imdb.sqlite  \n",
            "  inflating: /content/spider/database/imdb/schema.sql  \n",
            "  inflating: /content/spider/database/inn_1/annotation.json  \n",
            "  inflating: /content/spider/database/inn_1/change_date.py  \n",
            "  inflating: /content/spider/database/inn_1/data_csv/README.INN.TXT  \n",
            "  inflating: /content/spider/database/inn_1/data_csv/Reservations.csv  \n",
            "  inflating: /content/spider/database/inn_1/data_csv/Reservations_t.csv  \n",
            "  inflating: /content/spider/database/inn_1/data_csv/Rooms.csv  \n",
            "  inflating: /content/spider/database/inn_1/inn_1.sql  \n",
            "  inflating: /content/spider/database/inn_1/inn_1.sqlite  \n",
            "  inflating: /content/spider/database/inn_1/link.txt  \n",
            "  inflating: /content/spider/database/inn_1/q.txt  \n",
            "  inflating: /content/spider/database/insurance_and_eClaims/insurance_and_eClaims.sqlite  \n",
            "  inflating: /content/spider/database/insurance_and_eClaims/schema.sql  \n",
            "  inflating: /content/spider/database/insurance_fnol/insurance_fnol.sqlite  \n",
            "  inflating: /content/spider/database/insurance_fnol/schema.sql  \n",
            "  inflating: /content/spider/database/insurance_policies/insurance_policies.sqlite  \n",
            "  inflating: /content/spider/database/insurance_policies/schema.sql  \n",
            "  inflating: /content/spider/database/journal_committee/journal_committee.sqlite  \n",
            "  inflating: /content/spider/database/journal_committee/schema.sql  \n",
            "  inflating: /content/spider/database/loan_1/loan_1.sqlite  \n",
            "  inflating: /content/spider/database/loan_1/schema.sql  \n",
            "  inflating: /content/spider/database/local_govt_and_lot/local_govt_and_lot.sqlite  \n",
            "  inflating: /content/spider/database/local_govt_and_lot/schema.sql  \n",
            "  inflating: /content/spider/database/local_govt_in_alabama/local_govt_in_alabama.sqlite  \n",
            "  inflating: /content/spider/database/local_govt_in_alabama/schema.sql  \n",
            "  inflating: /content/spider/database/local_govt_mdm/local_govt_mdm.sqlite  \n",
            "  inflating: /content/spider/database/local_govt_mdm/schema.sql  \n",
            "  inflating: /content/spider/database/machine_repair/machine_repair.sqlite  \n",
            "  inflating: /content/spider/database/machine_repair/schema.sql  \n",
            "  inflating: /content/spider/database/manufactory_1/manufactory_1.sqlite  \n",
            "  inflating: /content/spider/database/manufactory_1/schema.sql  \n",
            "  inflating: /content/spider/database/manufacturer/manufacturer.sqlite  \n",
            "  inflating: /content/spider/database/manufacturer/schema.sql  \n",
            "  inflating: /content/spider/database/match_season/match_season.sqlite  \n",
            "  inflating: /content/spider/database/match_season/schema.sql  \n",
            "  inflating: /content/spider/database/medicine_enzyme_interaction/medicine_enzyme_interaction.sqlite  \n",
            "  inflating: /content/spider/database/medicine_enzyme_interaction/schema.sql  \n",
            "  inflating: /content/spider/database/mountain_photos/mountain_photos.sqlite  \n",
            "  inflating: /content/spider/database/mountain_photos/schema.sql  \n",
            "  inflating: /content/spider/database/movie_1/movie_1.sqlite  \n",
            "  inflating: /content/spider/database/movie_1/schema.sql  \n",
            "  inflating: /content/spider/database/museum_visit/museum_visit.sqlite  \n",
            "  inflating: /content/spider/database/museum_visit/schema.sql  \n",
            "  inflating: /content/spider/database/music_1/music_1.sqlite  \n",
            "  inflating: /content/spider/database/music_1/schema.sql  \n",
            "  inflating: /content/spider/database/music_2/music_2.sqlite  \n",
            "  inflating: /content/spider/database/music_2/schema.sql  \n",
            "  inflating: /content/spider/database/music_4/music_4.sqlite  \n",
            "  inflating: /content/spider/database/music_4/schema.sql  \n",
            "  inflating: /content/spider/database/musical/musical.sqlite  \n",
            "  inflating: /content/spider/database/musical/schema.sql  \n",
            "  inflating: /content/spider/database/network_1/network_1.sqlite  \n",
            "  inflating: /content/spider/database/network_1/schema.sql  \n",
            "  inflating: /content/spider/database/network_2/network_2.sqlite  \n",
            "  inflating: /content/spider/database/network_2/schema.sql  \n",
            "  inflating: /content/spider/database/news_report/news_report.sqlite  \n",
            "  inflating: /content/spider/database/news_report/schema.sql  \n",
            "  inflating: /content/spider/database/orchestra/orchestra.sqlite  \n",
            "  inflating: /content/spider/database/orchestra/schema.sql  \n",
            "  inflating: /content/spider/database/party_host/party_host.sqlite  \n",
            "  inflating: /content/spider/database/party_host/schema.sql  \n",
            "  inflating: /content/spider/database/party_people/party_people.sqlite  \n",
            "  inflating: /content/spider/database/party_people/schema.sql  \n",
            "  inflating: /content/spider/database/performance_attendance/performance_attendance.sqlite  \n",
            "  inflating: /content/spider/database/performance_attendance/schema.sql  \n",
            "  inflating: /content/spider/database/perpetrator/perpetrator.sqlite  \n",
            "  inflating: /content/spider/database/perpetrator/schema.sql  \n",
            "  inflating: /content/spider/database/pets_1/pets_1.sqlite  \n",
            "  inflating: /content/spider/database/pets_1/schema.sql  \n",
            "  inflating: /content/spider/database/phone_1/phone_1.sqlite  \n",
            "  inflating: /content/spider/database/phone_1/schema.sql  \n",
            "  inflating: /content/spider/database/phone_market/phone_market.sqlite  \n",
            "  inflating: /content/spider/database/phone_market/schema.sql  \n",
            "  inflating: /content/spider/database/pilot_record/pilot_record.sqlite  \n",
            "  inflating: /content/spider/database/pilot_record/schema.sql  \n",
            "  inflating: /content/spider/database/poker_player/poker_player.sqlite  \n",
            "  inflating: /content/spider/database/poker_player/schema.sql  \n",
            "  inflating: /content/spider/database/product_catalog/product_catalog.sqlite  \n",
            "  inflating: /content/spider/database/product_catalog/schema.sql  \n",
            "  inflating: /content/spider/database/products_for_hire/products_for_hire.sqlite  \n",
            "  inflating: /content/spider/database/products_for_hire/schema.sql  \n",
            "  inflating: /content/spider/database/products_gen_characteristics/products_gen_characteristics.sqlite  \n",
            "  inflating: /content/spider/database/products_gen_characteristics/schema.sql  \n",
            "  inflating: /content/spider/database/program_share/program_share.sqlite  \n",
            "  inflating: /content/spider/database/program_share/schema.sql  \n",
            "  inflating: /content/spider/database/protein_institute/protein_institute.sqlite  \n",
            "  inflating: /content/spider/database/protein_institute/schema.sql  \n",
            "  inflating: /content/spider/database/race_track/race_track.sqlite  \n",
            "  inflating: /content/spider/database/race_track/schema.sql  \n",
            "  inflating: /content/spider/database/railway/railway.sqlite  \n",
            "  inflating: /content/spider/database/railway/schema.sql  \n",
            "  inflating: /content/spider/database/real_estate_properties/real_estate_properties.sqlite  \n",
            "  inflating: /content/spider/database/real_estate_properties/schema.sql  \n",
            "  inflating: /content/spider/database/restaurant_1/restaurant_1.sqlite  \n",
            "  inflating: /content/spider/database/restaurant_1/schema.sql  \n",
            "  inflating: /content/spider/database/restaurants/restaurants.sqlite  \n",
            "  inflating: /content/spider/database/restaurants/schema.sql  \n",
            "  inflating: /content/spider/database/riding_club/riding_club.sqlite  \n",
            "  inflating: /content/spider/database/riding_club/schema.sql  \n",
            "  inflating: /content/spider/database/roller_coaster/roller_coaster.sqlite  \n",
            "  inflating: /content/spider/database/roller_coaster/schema.sql  \n",
            "  inflating: /content/spider/database/sakila_1/sakila_1.sqlite  \n",
            "  inflating: /content/spider/database/sakila_1/schema.sql  \n",
            "  inflating: /content/spider/database/scholar/schema.sql  \n",
            "  inflating: /content/spider/database/scholar/scholar.sqlite  \n",
            "  inflating: /content/spider/database/school_bus/schema.sql  \n",
            "  inflating: /content/spider/database/school_bus/school_bus.sqlite  \n",
            "  inflating: /content/spider/database/school_finance/schema.sql  \n",
            "  inflating: /content/spider/database/school_finance/school_finance.sqlite  \n",
            "  inflating: /content/spider/database/school_player/schema.sql  \n",
            "  inflating: /content/spider/database/school_player/school_player.sqlite  \n",
            "  inflating: /content/spider/database/scientist_1/schema.sql  \n",
            "  inflating: /content/spider/database/scientist_1/scientist_1.sqlite  \n",
            "  inflating: /content/spider/database/ship_1/schema.sql  \n",
            "  inflating: /content/spider/database/ship_1/ship_1.sqlite  \n",
            "  inflating: /content/spider/database/ship_mission/schema.sql  \n",
            "  inflating: /content/spider/database/ship_mission/ship_mission.sqlite  \n",
            "  inflating: /content/spider/database/shop_membership/schema.sql  \n",
            "  inflating: /content/spider/database/shop_membership/shop_membership.sqlite  \n",
            "  inflating: /content/spider/database/singer/schema.sql  \n",
            "  inflating: /content/spider/database/singer/singer.sqlite  \n",
            "  inflating: /content/spider/database/small_bank_1/small_bank_1.sqlite  \n",
            "  inflating: /content/spider/database/soccer_1/schema.sql  \n",
            "  inflating: /content/spider/database/soccer_1/soccer_1.sqlite  \n",
            "  inflating: /content/spider/database/soccer_2/schema.sql  \n",
            "  inflating: /content/spider/database/soccer_2/soccer_2.sqlite  \n",
            "  inflating: /content/spider/database/solvency_ii/schema.sql  \n",
            "  inflating: /content/spider/database/solvency_ii/solvency_ii.sqlite  \n",
            "  inflating: /content/spider/database/sports_competition/schema.sql  \n",
            "  inflating: /content/spider/database/sports_competition/sports_competition.sqlite  \n",
            "  inflating: /content/spider/database/station_weather/schema.sql  \n",
            "  inflating: /content/spider/database/station_weather/station_weather.sqlite  \n",
            "  inflating: /content/spider/database/store_1/schema.sql  \n",
            "  inflating: /content/spider/database/store_1/store_1.sqlite  \n",
            "  inflating: /content/spider/database/store_product/schema.sql  \n",
            "  inflating: /content/spider/database/store_product/store_product.sqlite  \n",
            "  inflating: /content/spider/database/storm_record/schema.sql  \n",
            "  inflating: /content/spider/database/storm_record/storm_record.sqlite  \n",
            "  inflating: /content/spider/database/student_1/annotation.json  \n",
            "  inflating: /content/spider/database/student_1/data_csv/README.STUDENTS.TXT  \n",
            "  inflating: /content/spider/database/student_1/data_csv/list.csv  \n",
            "  inflating: /content/spider/database/student_1/data_csv/teachers.csv  \n",
            "  inflating: /content/spider/database/student_1/link.txt  \n",
            "  inflating: /content/spider/database/student_1/q.txt  \n",
            "  inflating: /content/spider/database/student_1/student_1.sql  \n",
            "  inflating: /content/spider/database/student_1/student_1.sqlite  \n",
            "  inflating: /content/spider/database/student_assessment/schema.sql  \n",
            "  inflating: /content/spider/database/student_assessment/student_assessment.sqlite  \n",
            "  inflating: /content/spider/database/student_transcripts_tracking/schema.sql  \n",
            "  inflating: /content/spider/database/student_transcripts_tracking/student_transcripts_tracking.sqlite  \n",
            "  inflating: /content/spider/database/swimming/schema.sql  \n",
            "  inflating: /content/spider/database/swimming/swimming.sqlite  \n",
            "  inflating: /content/spider/database/theme_gallery/schema.sql  \n",
            "  inflating: /content/spider/database/theme_gallery/theme_gallery.sqlite  \n",
            "  inflating: /content/spider/database/tracking_grants_for_research/schema.sql  \n",
            "  inflating: /content/spider/database/tracking_grants_for_research/tracking_grants_for_research.sqlite  \n",
            "  inflating: /content/spider/database/tracking_orders/schema.sql  \n",
            "  inflating: /content/spider/database/tracking_orders/tracking_orders.sqlite  \n",
            "  inflating: /content/spider/database/tracking_share_transactions/schema.sql  \n",
            "  inflating: /content/spider/database/tracking_share_transactions/tracking_share_transactions.sqlite  \n",
            "  inflating: /content/spider/database/tracking_software_problems/schema.sql  \n",
            "  inflating: /content/spider/database/tracking_software_problems/tracking_software_problems.sqlite  \n",
            "  inflating: /content/spider/database/train_station/schema.sql  \n",
            "  inflating: /content/spider/database/train_station/train_station.sqlite  \n",
            "  inflating: /content/spider/database/tvshow/schema.sql  \n",
            "  inflating: /content/spider/database/tvshow/tvshow.sqlite  \n",
            "  inflating: /content/spider/database/twitter_1/queries/oracle-dialects.xml  \n",
            "  inflating: /content/spider/database/twitter_1/queries/postgres-dialects.xml  \n",
            "  inflating: /content/spider/database/twitter_1/queries/sqlserver-dialects.xml  \n",
            "  inflating: /content/spider/database/twitter_1/twitter_1.sqlite  \n",
            "  inflating: /content/spider/database/university_basketball/schema.sql  \n",
            "  inflating: /content/spider/database/university_basketball/university_basketball.sqlite  \n",
            "  inflating: /content/spider/database/voter_1/voter_1.sqlite  \n",
            "  inflating: /content/spider/database/voter_2/schema.sql  \n",
            "  inflating: /content/spider/database/voter_2/voter_2.sqlite  \n",
            "  inflating: /content/spider/database/wedding/schema.sql  \n",
            "  inflating: /content/spider/database/wedding/wedding.sqlite  \n",
            "  inflating: /content/spider/database/wine_1/annotation.json  \n",
            "  inflating: /content/spider/database/wine_1/data_csv/README.WINE.txt  \n",
            "  inflating: /content/spider/database/wine_1/data_csv/appellations.csv  \n",
            "  inflating: /content/spider/database/wine_1/data_csv/grapes.csv  \n",
            "  inflating: /content/spider/database/wine_1/data_csv/wine.csv  \n",
            "  inflating: /content/spider/database/wine_1/link.txt  \n",
            "  inflating: /content/spider/database/wine_1/q.txt  \n",
            "  inflating: /content/spider/database/wine_1/wine_1.sql  \n",
            "  inflating: /content/spider/database/wine_1/wine_1.sqlite  \n",
            "  inflating: /content/spider/database/workshop_paper/schema.sql  \n",
            "  inflating: /content/spider/database/workshop_paper/workshop_paper.sqlite  \n",
            "  inflating: /content/spider/database/world_1/world_1.json  \n",
            "  inflating: /content/spider/database/world_1/world_1.sqlite  \n",
            "  inflating: /content/spider/database/wrestler/schema.sql  \n",
            "  inflating: /content/spider/database/wrestler/wrestler.sqlite  \n",
            "  inflating: /content/spider/database/wta_1/wta_1.sql  \n",
            "  inflating: /content/spider/database/wta_1/wta_1.sqlite  \n",
            "  inflating: /content/spider/database/yelp/schema.sql  \n",
            "  inflating: /content/spider/database/yelp/yelp.sqlite  \n",
            "  inflating: /content/spider/dev.json  \n",
            "  inflating: /content/spider/dev_gold.sql  \n",
            "  inflating: /content/spider/tables.json  \n",
            "  inflating: /content/spider/train_gold.sql  \n",
            "  inflating: /content/spider/train_others.json  \n",
            "  inflating: /content/spider/train_spider.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SPIDER DATASET PREPROCESSING ‚Üí SCHEMA-AWARE FORMAT\n",
        "# OUTPUT: spider_schema_aware.json (7000 examples)\n",
        "# ============================================================\n",
        "\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£ LOAD RAW SPIDER FILES\n",
        "# ============================================================\n",
        "TRAIN_PATH = \"/content/spider/train_spider.json\"\n",
        "TABLES_PATH = \"/content/spider/tables.json\"\n",
        "\n",
        "assert os.path.exists(TRAIN_PATH), \"‚ùå train_spider.json not found\"\n",
        "assert os.path.exists(TABLES_PATH), \"‚ùå tables.json not found\"\n",
        "\n",
        "with open(TRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open(TABLES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    tables_data = json.load(f)\n",
        "\n",
        "print(\"‚úÖ Loaded questions:\", len(train_data))\n",
        "print(\"‚úÖ Loaded schemas:\", len(tables_data))\n",
        "\n",
        "# ============================================================\n",
        "# 2Ô∏è‚É£ BUILD DB ‚Üí SCHEMA MAP\n",
        "# ============================================================\n",
        "db_schemas = {}\n",
        "\n",
        "for db in tables_data:\n",
        "    db_id = db[\"db_id\"]\n",
        "    tables = db[\"table_names_original\"]\n",
        "    columns = db[\"column_names_original\"]\n",
        "\n",
        "    schema = {}\n",
        "    for tid, col in columns:\n",
        "        if tid == -1:\n",
        "            continue\n",
        "        table = tables[tid]\n",
        "        schema.setdefault(table, []).append(col)\n",
        "\n",
        "    db_schemas[db_id] = schema\n",
        "\n",
        "print(\"‚úÖ Schema map built\")\n",
        "\n",
        "# ============================================================\n",
        "# 3Ô∏è‚É£ TOKENIZE QUESTION\n",
        "# ============================================================\n",
        "def tokenize_question(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9_ ]\", \" \", text)\n",
        "    return text.split()\n",
        "\n",
        "# ============================================================\n",
        "# 4Ô∏è‚É£ SAFE SCHEMA MATCHING FROM SQL\n",
        "# ============================================================\n",
        "def find_schema_mentions(sql, schema):\n",
        "    sql = sql.lower()\n",
        "    used_tables = set()\n",
        "    used_columns = set()\n",
        "\n",
        "    for table, cols in schema.items():\n",
        "        if re.search(rf\"\\b{re.escape(table.lower())}\\b\", sql):\n",
        "            used_tables.add(table)\n",
        "\n",
        "        for col in cols:\n",
        "            if re.search(rf\"\\b{re.escape(col.lower())}\\b\", sql):\n",
        "                used_columns.add(f\"{table}.{col}\")\n",
        "\n",
        "    return used_tables, used_columns\n",
        "\n",
        "# ============================================================\n",
        "# 5Ô∏è‚É£ BUILD INPUT TOKENS + TOKEN TYPES\n",
        "# ============================================================\n",
        "def build_input_with_schema(question_tokens, schema):\n",
        "    tokens = []\n",
        "    token_types = []\n",
        "\n",
        "    # Question tokens ‚Üí type 0\n",
        "    for tok in question_tokens:\n",
        "        tokens.append(tok)\n",
        "        token_types.append(0)\n",
        "\n",
        "    # Tables ‚Üí type 1, Columns ‚Üí type 2\n",
        "    for table, cols in schema.items():\n",
        "        tokens.append(table)\n",
        "        token_types.append(1)\n",
        "        for col in cols:\n",
        "            tokens.append(col)\n",
        "            token_types.append(2)\n",
        "\n",
        "    return tokens, token_types\n",
        "\n",
        "# ============================================================\n",
        "# 6Ô∏è‚É£ BUILD SCHEMA LABELS\n",
        "# ============================================================\n",
        "def build_schema_labels(tokens, token_types, used_tables, used_columns):\n",
        "    labels = []\n",
        "\n",
        "    for tok, ttype in zip(tokens, token_types):\n",
        "        if ttype == 1:  # table\n",
        "            labels.append(1 if tok in used_tables else 0)\n",
        "        elif ttype == 2:  # column\n",
        "            labels.append(\n",
        "                1 if any(tok == c.split(\".\")[1] for c in used_columns) else 0\n",
        "            )\n",
        "        else:\n",
        "            labels.append(0)\n",
        "\n",
        "    return labels\n",
        "\n",
        "# ============================================================\n",
        "# 7Ô∏è‚É£ CONVERT ONE EXAMPLE\n",
        "# ============================================================\n",
        "def convert_example(ex):\n",
        "    schema = db_schemas[ex[\"db_id\"]]\n",
        "\n",
        "    q_tokens = tokenize_question(ex[\"question\"])\n",
        "    used_tables, used_columns = find_schema_mentions(ex[\"query\"], schema)\n",
        "\n",
        "    tokens, token_types = build_input_with_schema(q_tokens, schema)\n",
        "    schema_labels = build_schema_labels(\n",
        "        tokens, token_types, used_tables, used_columns\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"tokens\": tokens,\n",
        "        \"token_types\": token_types,\n",
        "        \"schema_labels\": schema_labels,\n",
        "        \"schema\": schema,\n",
        "        \"sql\": ex[\"query\"]\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# 8Ô∏è‚É£ PROCESS DATASET (7000 EXAMPLES)\n",
        "# ============================================================\n",
        "MAX_EXAMPLES = 7000\n",
        "processed = []\n",
        "\n",
        "for i, ex in enumerate(train_data[:MAX_EXAMPLES]):\n",
        "    processed.append(convert_example(ex))\n",
        "    if i > 0 and i % 500 == 0:\n",
        "        print(f\"‚è≥ Processed {i} examples\")\n",
        "\n",
        "print(\"‚úÖ Total processed:\", len(processed))\n",
        "\n",
        "# ============================================================\n",
        "# 9Ô∏è‚É£ SAVE OUTPUT\n",
        "# ============================================================\n",
        "OUTPUT_PATH = \"/content/spider_schema_aware.json\"\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Saved to:\", OUTPUT_PATH)\n",
        "\n",
        "# ============================================================\n",
        "# üîü SANITY CHECK\n",
        "# ============================================================\n",
        "sample = processed[0]\n",
        "print(\"\\n--- SAMPLE ---\")\n",
        "print(\"Tokens:\", sample[\"tokens\"][:20])\n",
        "print(\"Token types:\", sample[\"token_types\"][:20])\n",
        "print(\"Schema labels:\", sample[\"schema_labels\"][:20])\n",
        "print(\"SQL:\", sample[\"sql\"])\n"
      ],
      "metadata": {
        "id": "osyI4QPwoDSf",
        "outputId": "a93b0d1b-a9b8-448b-e64c-007cf9b1b5dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded questions: 7000\n",
            "‚úÖ Loaded schemas: 166\n",
            "‚úÖ Schema map built\n",
            "‚è≥ Processed 500 examples\n",
            "‚è≥ Processed 1000 examples\n",
            "‚è≥ Processed 1500 examples\n",
            "‚è≥ Processed 2000 examples\n",
            "‚è≥ Processed 2500 examples\n",
            "‚è≥ Processed 3000 examples\n",
            "‚è≥ Processed 3500 examples\n",
            "‚è≥ Processed 4000 examples\n",
            "‚è≥ Processed 4500 examples\n",
            "‚è≥ Processed 5000 examples\n",
            "‚è≥ Processed 5500 examples\n",
            "‚è≥ Processed 6000 examples\n",
            "‚è≥ Processed 6500 examples\n",
            "‚úÖ Total processed: 7000\n",
            "‚úÖ Saved to: /content/spider_schema_aware.json\n",
            "\n",
            "--- SAMPLE ---\n",
            "Tokens: ['how', 'many', 'heads', 'of', 'the', 'departments', 'are', 'older', 'than', '56', 'department', 'Department_ID', 'Name', 'Creation', 'Ranking', 'Budget_in_Billions', 'Num_Employees', 'head', 'head_ID', 'name']\n",
            "Token types: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2]\n",
            "Schema labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "SQL: SELECT count(*) FROM head WHERE age  >  56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# NL2SQL ‚Äì SCHEMA-AWARE ENCODER DECODER (7000 EXAMPLES)\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£ DEVICE\n",
        "# ============================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# 2Ô∏è‚É£ LOAD PREPROCESSED DATA\n",
        "# ============================================================\n",
        "DATA_PATH = \"/content/spider_schema_aware.json\"\n",
        "assert os.path.exists(DATA_PATH), \"‚ùå spider_schema_aware.json not found\"\n",
        "\n",
        "with open(DATA_PATH) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"Total examples in JSON:\", len(data))\n",
        "\n",
        "# ============================================================\n",
        "# 3Ô∏è‚É£ BUILD ENCODER VOCAB\n",
        "# ============================================================\n",
        "def build_encoder_vocab(data):\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    idx = 2\n",
        "    for ex in data:\n",
        "        for t in ex[\"tokens\"]:\n",
        "            if t not in vocab:\n",
        "                vocab[t] = idx\n",
        "                idx += 1\n",
        "    return vocab\n",
        "\n",
        "encoder_vocab = build_encoder_vocab(data)\n",
        "enc_vocab_size = len(encoder_vocab)\n",
        "print(\"Encoder vocab size:\", enc_vocab_size)\n",
        "\n",
        "# ============================================================\n",
        "# 4Ô∏è‚É£ DECODER SQL VOCAB (PICARD-STYLE)\n",
        "# ============================================================\n",
        "SQL_KEYWORDS = [\"SELECT\",\"FROM\",\"WHERE\",\"JOIN\",\"ON\",\"AS\",\"DISTINCT\"]\n",
        "LOGICAL_OPS = [\"AND\",\"OR\",\"NOT\"]\n",
        "COMPARISON_OPS = [\"=\",\"!=\",\"<>\",\">\",\"<\",\">=\",\"<=\"]\n",
        "AGG_FUNCS = [\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\"]\n",
        "GROUPING = [\"GROUP\",\"BY\",\"HAVING\"]\n",
        "ORDERING = [\"ORDER\",\"ASC\",\"DESC\",\"LIMIT\"]\n",
        "PUNCT = [\",\",\"(\",\")\"]\n",
        "SPECIAL = [\"<PAD>\",\"<EOS>\",\"VALUE\"]\n",
        "\n",
        "MAX_TABLES = 128\n",
        "MAX_COLUMNS = 1024\n",
        "TABLE_TOKENS = [f\"T{i}\" for i in range(MAX_TABLES)]\n",
        "COLUMN_TOKENS = [f\"C{i}\" for i in range(MAX_COLUMNS)]\n",
        "\n",
        "DECODER_VOCAB = (\n",
        "    SQL_KEYWORDS + LOGICAL_OPS + COMPARISON_OPS + AGG_FUNCS +\n",
        "    GROUPING + ORDERING + PUNCT + SPECIAL +\n",
        "    TABLE_TOKENS + COLUMN_TOKENS\n",
        ")\n",
        "\n",
        "token_to_id = {t:i for i,t in enumerate(DECODER_VOCAB)}\n",
        "VOCAB_SIZE = len(token_to_id)\n",
        "print(\"Decoder vocab size:\", VOCAB_SIZE)\n",
        "\n",
        "# ============================================================\n",
        "# 5Ô∏è‚É£ SCHEMA ‚Üí PLACEHOLDER MAPPING\n",
        "# ============================================================\n",
        "def build_schema_maps(schema):\n",
        "    table_map = {t: f\"T{i}\" for i, t in enumerate(schema.keys())}\n",
        "    col_map = {}\n",
        "    cid = 0\n",
        "    for t, cols in schema.items():\n",
        "        for c in cols:\n",
        "            col_map[f\"{t}.{c}\"] = f\"C{cid}\"\n",
        "            cid += 1\n",
        "    return table_map, col_map\n",
        "\n",
        "def sql_to_placeholder(sql, tmap, cmap):\n",
        "    sql = sql.lower()\n",
        "    for k, v in sorted(cmap.items(), key=lambda x: -len(x[0])):\n",
        "        sql = re.sub(rf\"\\b{re.escape(k)}\\b\", v, sql)\n",
        "    for k, v in tmap.items():\n",
        "        sql = re.sub(rf\"\\b{re.escape(k)}\\b\", v, sql)\n",
        "    sql = re.sub(r\"\\b\\d+\\b\", \"VALUE\", sql)\n",
        "    return sql.upper()\n",
        "\n",
        "# ============================================================\n",
        "# 6Ô∏è‚É£ DATASET\n",
        "# ============================================================\n",
        "class SpiderDataset(Dataset):\n",
        "    def __init__(self, data, vocab, limit=7000):\n",
        "        self.data = data[:limit]\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        tmap, cmap = build_schema_maps(ex[\"schema\"])\n",
        "        sql = sql_to_placeholder(ex[\"sql\"], tmap, cmap)\n",
        "\n",
        "        x = [self.vocab.get(t, 1) for t in ex[\"tokens\"]]\n",
        "        y = [token_to_id.get(t, token_to_id[\"<PAD>\"]) for t in sql.split()]\n",
        "        y.append(token_to_id[\"<EOS>\"])\n",
        "\n",
        "        return (\n",
        "            torch.tensor(x),\n",
        "            torch.tensor(ex[\"token_types\"]),\n",
        "            torch.tensor(ex[\"schema_labels\"], dtype=torch.float),\n",
        "            torch.tensor(y)\n",
        "        )\n",
        "\n",
        "def collate_fn(batch):\n",
        "    def pad(seqs, val=0):\n",
        "        m = max(len(s) for s in seqs)\n",
        "        return torch.stack([\n",
        "            torch.cat([s, torch.full((m - len(s),), val)]) for s in seqs\n",
        "        ])\n",
        "    x, t, s, y = zip(*batch)\n",
        "    return pad(x), pad(t), pad(s), pad(y)\n",
        "\n",
        "dataset = SpiderDataset(data, encoder_vocab, limit=7000)\n",
        "loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "print(\"Training examples:\", len(dataset))\n",
        "\n",
        "# ============================================================\n",
        "# 7Ô∏è‚É£ TRANSFORMER MODULES\n",
        "# ============================================================\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d, h):\n",
        "        super().__init__()\n",
        "        self.h = h\n",
        "        self.dk = d // h\n",
        "        self.qkv = nn.Linear(d, d * 3)\n",
        "        self.o = nn.Linear(d, d)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        B, T, D = q.size()\n",
        "        q, k, v = self.qkv(q).chunk(3, dim=-1)\n",
        "        q = q.view(B, T, self.h, self.dk).transpose(1,2)\n",
        "        k = k.view(B, -1, self.h, self.dk).transpose(1,2)\n",
        "        v = v.view(B, -1, self.h, self.dk).transpose(1,2)\n",
        "        scores = q @ k.transpose(-2,-1) / math.sqrt(self.dk)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        out = (scores.softmax(-1) @ v).transpose(1,2).contiguous().view(B,T,D)\n",
        "        return self.o(out)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab, d, h, ff, layers):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, d, padding_idx=0)\n",
        "        self.type_emb = nn.Embedding(3, d)\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                MultiHeadAttention(d,h),\n",
        "                nn.Sequential(nn.Linear(d,ff), nn.ReLU(), nn.Linear(ff,d)),\n",
        "                nn.LayerNorm(d),\n",
        "                nn.LayerNorm(d)\n",
        "            ]) for _ in range(layers)\n",
        "        ])\n",
        "        self.schema_head = nn.Linear(d,1)\n",
        "\n",
        "    def forward(self, x, t, mask):\n",
        "        x = self.emb(x) + self.type_emb(t)\n",
        "        for attn, ff, n1, n2 in self.layers:\n",
        "            x = n1(x + attn(x,x,x,mask))\n",
        "            x = n2(x + ff(x))\n",
        "        return x, self.schema_head(x).squeeze(-1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab, d, h, ff, layers):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, d)\n",
        "        self.pos = nn.Embedding(512, d)\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                MultiHeadAttention(d,h),\n",
        "                MultiHeadAttention(d,h),\n",
        "                nn.Sequential(nn.Linear(d,ff), nn.ReLU(), nn.Linear(ff,d)),\n",
        "                nn.LayerNorm(d),\n",
        "                nn.LayerNorm(d),\n",
        "                nn.LayerNorm(d)\n",
        "            ]) for _ in range(layers)\n",
        "        ])\n",
        "        self.out = nn.Linear(d, vocab)\n",
        "\n",
        "    def forward(self, y, enc, mask):\n",
        "        B, T = y.size()\n",
        "        x = self.emb(y) + self.pos(torch.arange(T, device=y.device))\n",
        "        for sa, ca, ff, n1, n2, n3 in self.layers:\n",
        "            x = n1(x + sa(x,x,x,mask))\n",
        "            x = n2(x + ca(x,enc,enc,None))\n",
        "            x = n3(x + ff(x))\n",
        "        return self.out(x)\n",
        "\n",
        "# ============================================================\n",
        "# 8Ô∏è‚É£ TRAINING\n",
        "# ============================================================\n",
        "encoder = Encoder(enc_vocab_size, 256, 8, 1024, 4).to(device)\n",
        "decoder = Decoder(VOCAB_SIZE, 256, 8, 1024, 4).to(device)\n",
        "\n",
        "opt = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=3e-4)\n",
        "sql_loss = nn.CrossEntropyLoss(ignore_index=token_to_id[\"<PAD>\"])\n",
        "schema_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "EPOCHS = 25\n",
        "\n",
        "for ep in range(EPOCHS):\n",
        "    total = 0\n",
        "    for x, t, s, y in loader:\n",
        "        x, t, s, y = x.to(device), t.to(device), s.to(device), y.to(device)\n",
        "        mask = (x != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        enc, sch = encoder(x, t, mask)\n",
        "        dec_in, dec_tgt = y[:, :-1], y[:, 1:]\n",
        "\n",
        "        T = dec_in.size(1)\n",
        "        causal = torch.tril(torch.ones(T, T, device=device)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        logits = decoder(dec_in, enc, causal)\n",
        "\n",
        "        loss = sql_loss(logits.reshape(-1, VOCAB_SIZE), dec_tgt.reshape(-1))\n",
        "        loss += 0.7 * schema_loss(sch*(t!=0), s*(t!=0))\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total += loss.item()\n",
        "\n",
        "    print(f\"Epoch {ep+1} | Loss: {total/len(loader):.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 9Ô∏è‚É£ SAVE MODEL\n",
        "# ============================================================\n",
        "torch.save({\n",
        "    \"encoder\": encoder.state_dict(),\n",
        "    \"decoder\": decoder.state_dict(),\n",
        "    \"encoder_vocab\": encoder_vocab,\n",
        "    \"decoder_vocab\": token_to_id\n",
        "}, \"/content/nl2sql_schema_aware.pt\")\n",
        "\n",
        "print(\"‚úÖ Model saved successfully\")\n"
      ],
      "metadata": {
        "id": "8eZSfZQNoDoG",
        "outputId": "b7b87e7c-75d3-41d5-a3f6-43270498132b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Total examples in JSON: 7000\n",
            "Encoder vocab size: 5312\n",
            "Decoder vocab size: 1187\n",
            "Training examples: 7000\n",
            "Epoch 1 | Loss: 0.5832\n",
            "Epoch 2 | Loss: 0.3839\n",
            "Epoch 3 | Loss: 0.3574\n",
            "Epoch 4 | Loss: 0.3396\n",
            "Epoch 5 | Loss: 0.3294\n",
            "Epoch 6 | Loss: 0.3204\n",
            "Epoch 7 | Loss: 0.3177\n",
            "Epoch 8 | Loss: 0.3144\n",
            "Epoch 9 | Loss: 0.3091\n",
            "Epoch 10 | Loss: 0.3114\n",
            "Epoch 11 | Loss: 0.3126\n",
            "Epoch 12 | Loss: 0.3081\n",
            "Epoch 13 | Loss: 0.3104\n",
            "Epoch 14 | Loss: 0.3066\n",
            "Epoch 15 | Loss: 0.3084\n",
            "Epoch 16 | Loss: 0.3053\n",
            "Epoch 17 | Loss: 0.3391\n",
            "Epoch 18 | Loss: 0.3115\n",
            "Epoch 19 | Loss: 0.3052\n",
            "Epoch 20 | Loss: 0.3063\n",
            "Epoch 21 | Loss: 0.3067\n",
            "Epoch 22 | Loss: 0.3072\n",
            "Epoch 23 | Loss: 0.3064\n",
            "Epoch 24 | Loss: 0.3053\n",
            "Epoch 25 | Loss: 0.3060\n",
            "‚úÖ Model saved successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# NL2SQL INFERENCE ‚Äì FINAL SINGLE CELL (WORKING & STABLE)\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import re\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DEVICE\n",
        "# ------------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# LOAD CHECKPOINT\n",
        "# ------------------------------------------------------------\n",
        "ckpt = torch.load(\"/content/nl2sql_schema_aware.pt\", map_location=device)\n",
        "\n",
        "encoder_vocab = ckpt[\"encoder_vocab\"]\n",
        "decoder_vocab = ckpt[\"decoder_vocab\"]\n",
        "id_to_token = {v: k for k, v in decoder_vocab.items()}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# SQL CONSTANTS (MUST MATCH TRAINING)\n",
        "# ------------------------------------------------------------\n",
        "AGG_FUNCS = [\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\"]\n",
        "COMPARISON_OPS = [\"=\",\"!=\",\"<>\",\">\",\"<\",\">=\",\"<=\"]\n",
        "\n",
        "TABLE_TOKENS = [t for t in decoder_vocab if t.startswith(\"T\")]\n",
        "COLUMN_TOKENS = [t for t in decoder_vocab if t.startswith(\"C\")]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# MODEL DEFINITIONS (MATCH TRAINING)\n",
        "# ------------------------------------------------------------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d, h):\n",
        "        super().__init__()\n",
        "        self.h = h\n",
        "        self.dk = d // h\n",
        "        self.qkv = nn.Linear(d, d * 3)\n",
        "        self.o = nn.Linear(d, d)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        B, T, D = q.size()\n",
        "        q, k, v = self.qkv(q).chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, self.h, self.dk).transpose(1, 2)\n",
        "        k = k.view(B, -1, self.h, self.dk).transpose(1, 2)\n",
        "        v = v.view(B, -1, self.h, self.dk).transpose(1, 2)\n",
        "\n",
        "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.dk)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        out = (scores.softmax(-1) @ v)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        return self.o(out)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab, d=256, h=8, ff=1024, layers=4):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, d, padding_idx=0)\n",
        "        self.type_emb = nn.Embedding(3, d)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                MultiHeadAttention(d, h),\n",
        "                nn.Sequential(nn.Linear(d, ff), nn.ReLU(), nn.Linear(ff, d)),\n",
        "                nn.LayerNorm(d),\n",
        "                nn.LayerNorm(d)\n",
        "            ]) for _ in range(layers)\n",
        "        ])\n",
        "\n",
        "        # schema_head existed during training but NOT needed at inference\n",
        "\n",
        "    def forward(self, x, t, mask):\n",
        "        x = self.emb(x) + self.type_emb(t)\n",
        "        for attn, ff, n1, n2 in self.layers:\n",
        "            x = n1(x + attn(x, x, x, mask))\n",
        "            x = n2(x + ff(x))\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab, d=256, h=8, ff=1024, layers=4):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, d)\n",
        "        self.pos = nn.Embedding(512, d)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                MultiHeadAttention(d, h),\n",
        "                MultiHeadAttention(d, h),\n",
        "                nn.Sequential(nn.Linear(d, ff), nn.ReLU(), nn.Linear(ff, d)),\n",
        "                nn.LayerNorm(d),\n",
        "                nn.LayerNorm(d),\n",
        "                nn.LayerNorm(d)\n",
        "            ]) for _ in range(layers)\n",
        "        ])\n",
        "\n",
        "        self.out = nn.Linear(d, vocab)\n",
        "\n",
        "    def forward(self, y, enc, mask):\n",
        "        B, T = y.size()\n",
        "        x = self.emb(y) + self.pos(torch.arange(T, device=y.device))\n",
        "\n",
        "        for sa, ca, ff, n1, n2, n3 in self.layers:\n",
        "            x = n1(x + sa(x, x, x, mask))\n",
        "            x = n2(x + ca(x, enc, enc, None))\n",
        "            x = n3(x + ff(x))\n",
        "\n",
        "        return self.out(x)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# LOAD MODEL WEIGHTS (KEY FIX HERE)\n",
        "# ------------------------------------------------------------\n",
        "encoder = Encoder(len(encoder_vocab)).to(device)\n",
        "decoder = Decoder(len(decoder_vocab)).to(device)\n",
        "\n",
        "encoder.load_state_dict(ckpt[\"encoder\"], strict=False)  # ‚úÖ FIX\n",
        "decoder.load_state_dict(ckpt[\"decoder\"])\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# HELPERS\n",
        "# ------------------------------------------------------------\n",
        "def tokenize_question(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9_ ]\", \" \", text)\n",
        "    return text.split()\n",
        "\n",
        "def build_encoder_input(question, schema):\n",
        "    tokens, types = [], []\n",
        "\n",
        "    for t in tokenize_question(question):\n",
        "        tokens.append(t)\n",
        "        types.append(0)\n",
        "\n",
        "    for table, cols in schema.items():\n",
        "        tokens.append(table)\n",
        "        types.append(1)\n",
        "        for col in cols:\n",
        "            tokens.append(col)\n",
        "            types.append(2)\n",
        "\n",
        "    ids = [encoder_vocab.get(t, encoder_vocab[\"<UNK>\"]) for t in tokens]\n",
        "\n",
        "    return (\n",
        "        torch.tensor(ids).unsqueeze(0).to(device),\n",
        "        torch.tensor(types).unsqueeze(0).to(device)\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# SIMPLE PICARD GRAMMAR\n",
        "# ------------------------------------------------------------\n",
        "def picard_filter(prefix):\n",
        "    if len(prefix) == 0:\n",
        "        return [\"SELECT\"]\n",
        "\n",
        "    last = prefix[-1]\n",
        "\n",
        "    if last == \"SELECT\":\n",
        "        return AGG_FUNCS + COLUMN_TOKENS\n",
        "    if last in AGG_FUNCS or last in COLUMN_TOKENS:\n",
        "        return [\"FROM\", \",\"]\n",
        "    if last == \",\":\n",
        "        return COLUMN_TOKENS\n",
        "    if last == \"FROM\":\n",
        "        return TABLE_TOKENS\n",
        "    if last in TABLE_TOKENS:\n",
        "        return [\"WHERE\", \"<EOS>\"]\n",
        "    if last == \"WHERE\":\n",
        "        return COLUMN_TOKENS\n",
        "    if last in COLUMN_TOKENS:\n",
        "        return COMPARISON_OPS\n",
        "    if last in COMPARISON_OPS:\n",
        "        return [\"VALUE\"]\n",
        "    if last == \"VALUE\":\n",
        "        return [\"AND\", \"<EOS>\"]\n",
        "    if last == \"AND\":\n",
        "        return COLUMN_TOKENS\n",
        "\n",
        "    return [\"<EOS>\"]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# SQL GENERATION\n",
        "# ------------------------------------------------------------\n",
        "def generate_sql(question, schema, max_len=40):\n",
        "    x, t = build_encoder_input(question, schema)\n",
        "    mask = (x != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc = encoder(x, t, mask)\n",
        "\n",
        "    cur = torch.tensor([[decoder_vocab[\"SELECT\"]]], device=device)\n",
        "    generated = [\"SELECT\"]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        T = cur.size(1)\n",
        "        causal = torch.tril(torch.ones(T, T, device=device)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = decoder(cur, enc, causal)[0, -1]\n",
        "\n",
        "        allowed = picard_filter(generated)\n",
        "        allowed_ids = [decoder_vocab[t] for t in allowed if t in decoder_vocab]\n",
        "\n",
        "        masked = torch.full_like(logits, -1e9)\n",
        "        masked[allowed_ids] = logits[allowed_ids]\n",
        "\n",
        "        next_id = masked.argmax().item()\n",
        "        token = id_to_token[next_id]\n",
        "\n",
        "        if token == \"<EOS>\":\n",
        "            break\n",
        "\n",
        "        generated.append(token)\n",
        "        cur = torch.cat([cur, torch.tensor([[next_id]], device=device)], dim=1)\n",
        "\n",
        "    return \" \".join(generated)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TEST\n",
        "# ------------------------------------------------------------\n",
        "schema = {\n",
        "    \"employees\": [\"id\", \"name\", \"salary\", \"department_id\"],\n",
        "    \"departments\": [\"id\", \"department_name\"]\n",
        "}\n",
        "\n",
        "question = \"show salary of employees\"\n",
        "\n",
        "print(\"\\nüü¢ Generated SQL:\")\n",
        "print(generate_sql(question, schema))\n"
      ],
      "metadata": {
        "id": "E6tiySdvoELb",
        "outputId": "0270521a-4fad-405b-e0bb-05acaca4c330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "‚úÖ Model loaded successfully\n",
            "\n",
            "üü¢ Generated SQL:\n",
            "SELECT COUNT FROM T0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}