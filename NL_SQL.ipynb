{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTK3rmJsb2LCC5aEPZGQ1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/NL_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "# ---------------- SCHEMAS ----------------\n",
        "SCHEMAS = [\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"employees\": [\"id\", \"name\", \"salary\", \"dept_id\"],\n",
        "            \"departments\": [\"id\", \"name\"]\n",
        "        },\n",
        "        \"numeric\": {\n",
        "            \"employees\": [\"id\", \"salary\", \"dept_id\"]\n",
        "        },\n",
        "        \"text\": {\n",
        "            \"employees\": [\"name\"]\n",
        "        },\n",
        "        \"join\": (\"employees\", \"departments\", \"dept_id\", \"id\")\n",
        "    },\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"students\": [\"id\", \"name\", \"marks\", \"class_id\"],\n",
        "            \"classes\": [\"id\", \"name\"]\n",
        "        },\n",
        "        \"numeric\": {\n",
        "            \"students\": [\"id\", \"marks\", \"class_id\"]\n",
        "        },\n",
        "        \"text\": {\n",
        "            \"students\": [\"name\"]\n",
        "        },\n",
        "        \"join\": (\"students\", \"classes\", \"class_id\", \"id\")\n",
        "    }\n",
        "]\n",
        "\n",
        "AGGS = [\"COUNT\", \"SUM\", \"AVG\", \"MAX\", \"MIN\"]\n",
        "\n",
        "# ---------------- GENERATOR ----------------\n",
        "def generate_example():\n",
        "    db = random.choice(SCHEMAS)\n",
        "    schema = db[\"tables\"]\n",
        "\n",
        "    main_table = list(schema.keys())[0]\n",
        "    cols = schema[main_table]\n",
        "\n",
        "    intent = random.choices(\n",
        "        [\"SELECT\", \"AGG\", \"WHERE\", \"GROUP\", \"JOIN\"],\n",
        "        weights=[0.30, 0.25, 0.20, 0.15, 0.10]\n",
        "    )[0]\n",
        "\n",
        "    # ---------- SELECT ----------\n",
        "    if intent == \"SELECT\":\n",
        "        col = random.choice(cols)\n",
        "        question = random.choice([\n",
        "            f\"show {col} of {main_table}\",\n",
        "            f\"get {col} from {main_table}\",\n",
        "            f\"list {col} in {main_table}\",\n",
        "            f\"display {col} for {main_table}\"\n",
        "        ])\n",
        "        sql = f\"SELECT {col} FROM {main_table}\"\n",
        "\n",
        "    # ---------- AGGREGATION ----------\n",
        "    elif intent == \"AGG\":\n",
        "        agg = random.choice(AGGS)\n",
        "\n",
        "        if agg == \"COUNT\":\n",
        "            col = random.choice(cols)\n",
        "        else:\n",
        "            col = random.choice(db[\"numeric\"][main_table])\n",
        "\n",
        "        question = random.choice([\n",
        "            f\"show {agg.lower()} of {col} of {main_table}\",\n",
        "            f\"what is the {agg.lower()} {col} in {main_table}\",\n",
        "            f\"give me the {agg.lower()} {col} from {main_table}\"\n",
        "        ])\n",
        "        sql = f\"SELECT {agg}({col}) FROM {main_table}\"\n",
        "\n",
        "    # ---------- WHERE ----------\n",
        "    elif intent == \"WHERE\":\n",
        "        col = random.choice(db[\"numeric\"][main_table])\n",
        "        val = random.choice([10, 20, 50, 100])\n",
        "        question = random.choice([\n",
        "            f\"show {col} of {main_table} where {col} > {val}\",\n",
        "            f\"list {main_table} with {col} greater than {val}\",\n",
        "            f\"get {col} from {main_table} having {col} above {val}\"\n",
        "        ])\n",
        "        sql = f\"SELECT {col} FROM {main_table} WHERE {col} > {val}\"\n",
        "\n",
        "    # ---------- GROUP BY ----------\n",
        "    elif intent == \"GROUP\":\n",
        "        agg = random.choice([\"COUNT\", \"AVG\"])\n",
        "        group_col = random.choice(db[\"text\"][main_table])\n",
        "        num_col = random.choice(db[\"numeric\"][main_table])\n",
        "\n",
        "        question = random.choice([\n",
        "            f\"show {agg.lower()} of {num_col} per {group_col}\",\n",
        "            f\"get {agg.lower()} {num_col} grouped by {group_col}\",\n",
        "            f\"list {group_col} wise {agg.lower()} {num_col}\"\n",
        "        ])\n",
        "        sql = (\n",
        "            f\"SELECT {group_col}, {agg}({num_col}) \"\n",
        "            f\"FROM {main_table} GROUP BY {group_col}\"\n",
        "        )\n",
        "\n",
        "    # ---------- JOIN ----------\n",
        "    else:\n",
        "        t1, t2, c1, c2 = db[\"join\"]\n",
        "        question = random.choice([\n",
        "            f\"show {t1} name and {t2} name\",\n",
        "            f\"get {t1} names with their {t2}\",\n",
        "            f\"list {t1} and corresponding {t2}\"\n",
        "        ])\n",
        "        sql = (\n",
        "            f\"SELECT {t1}.name, {t2}.name \"\n",
        "            f\"FROM {t1} JOIN {t2} ON {t1}.{c1} = {t2}.{c2}\"\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"schema\": schema,\n",
        "        \"sql\": sql\n",
        "    }\n",
        "\n",
        "# ---------------- DATASET CREATION ----------------\n",
        "def generate_dataset(n=50_000, outfile=\"nl2sql_varied.json\"):\n",
        "    data = []\n",
        "    for i in range(n):\n",
        "        data.append(generate_example())\n",
        "        if (i + 1) % 10_000 == 0:\n",
        "            print(f\"Generated {i+1} examples\")\n",
        "\n",
        "    with open(outfile, \"w\") as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    print(\"âœ… Dataset size:\", len(data))\n",
        "\n",
        "\n",
        "# ðŸ”¥ CHANGE n TO 300_000 IF YOU WANT\n",
        "generate_dataset(n=50_000)\n"
      ],
      "metadata": {
        "id": "ZkCMmK9pe6xl",
        "outputId": "e5e244a0-d81f-4719-e9a0-a3b53a0c4109",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 10000 examples\n",
            "Generated 20000 examples\n",
            "Generated 30000 examples\n",
            "Generated 40000 examples\n",
            "Generated 50000 examples\n",
            "âœ… Dataset size: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# with open(\"nl2sql_varied.json\", \"r\") as f:\n",
        "#     dataset = json.load(f)\n",
        "\n",
        "# print(\"Loaded dataset size:\", len(dataset))\n",
        "\n",
        "# from collections import Counter\n",
        "\n",
        "# ENC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<SEP>\":2}\n",
        "# DEC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<BOS>\":2, \"<EOS>\":3}\n",
        "\n",
        "\n",
        "# def add(vocab, tok):\n",
        "#     if tok not in vocab:\n",
        "#         vocab[tok] = len(vocab)\n",
        "\n",
        "# for ex in dataset:\n",
        "#     # NL tokens\n",
        "#     for t in ex[\"question\"].lower().split():\n",
        "#         add(ENC_VOCAB, t)\n",
        "\n",
        "#     # Schema tokens\n",
        "#     for t, cols in ex[\"schema\"].items():\n",
        "#         add(ENC_VOCAB, t)\n",
        "#         for c in cols:\n",
        "#             add(ENC_VOCAB, f\"{t}.{c}\")\n",
        "\n",
        "#     # SQL tokens\n",
        "#     for ex in dataset:\n",
        "#       for t in ex[\"sql\"].lower().split():\n",
        "#           if t not in DEC_VOCAB:\n",
        "#               DEC_VOCAB[t] = len(DEC_VOCAB)\n",
        "\n",
        "\n",
        "# print(\"Encoder vocab:\", len(ENC_VOCAB))\n",
        "# print(\"Decoder vocab:\", len(DEC_VOCAB))\n",
        "import json\n",
        "\n",
        "with open(\"nl2sql_varied.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(\"Loaded dataset size:\", len(dataset))\n",
        "\n",
        "ENC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<SEP>\":2}\n",
        "DEC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<BOS>\":2, \"<EOS>\":3}\n",
        "\n",
        "def add(vocab, tok):\n",
        "    if tok not in vocab:\n",
        "        vocab[tok] = len(vocab)\n",
        "\n",
        "for ex in dataset:\n",
        "    # -------- Encoder vocab --------\n",
        "    for t in ex[\"question\"].lower().split():\n",
        "        add(ENC_VOCAB, t)\n",
        "\n",
        "    for table, cols in ex[\"schema\"].items():\n",
        "        add(ENC_VOCAB, table)\n",
        "        for col in cols:\n",
        "            add(ENC_VOCAB, f\"{table}.{col}\")\n",
        "\n",
        "    # -------- Decoder vocab --------\n",
        "    for t in ex[\"sql\"].lower().split():\n",
        "        add(DEC_VOCAB, t)\n",
        "\n",
        "print(\"Encoder vocab:\", len(ENC_VOCAB))\n",
        "print(\"Decoder vocab:\", len(DEC_VOCAB))\n"
      ],
      "metadata": {
        "id": "cI1fIGF0e606",
        "outputId": "145f0859-c61e-49f3-d76a-6ddca75831ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset size: 50000\n",
            "Encoder vocab: 62\n",
            "Decoder vocab: 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "    def __init__(self, data, enc_vocab, dec_vocab,\n",
        "                 max_src_len=80, max_tgt_len=60):\n",
        "        self.data = data\n",
        "        self.enc_vocab = enc_vocab\n",
        "        self.dec_vocab = dec_vocab\n",
        "        self.max_src_len = max_src_len\n",
        "        self.max_tgt_len = max_tgt_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def encode_src(self, tokens):\n",
        "        ids = [self.enc_vocab.get(t, self.enc_vocab[\"<UNK>\"]) for t in tokens]\n",
        "        ids = ids[:self.max_src_len]\n",
        "        pad_len = self.max_src_len - len(ids)\n",
        "        return ids + [self.enc_vocab[\"<PAD>\"]] * pad_len\n",
        "\n",
        "    def encode_tgt(self, tokens):\n",
        "        ids = [self.dec_vocab[\"<BOS>\"]] + \\\n",
        "              [self.dec_vocab.get(t, self.dec_vocab[\"<UNK>\"]) for t in tokens] + \\\n",
        "              [self.dec_vocab[\"<EOS>\"]]\n",
        "\n",
        "        ids = ids[:self.max_tgt_len]\n",
        "        pad_len = self.max_tgt_len - len(ids)\n",
        "        return ids + [self.dec_vocab[\"<PAD>\"]] * pad_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "\n",
        "        # -------- Encoder input (NL + schema) --------\n",
        "        question_tokens = ex[\"question\"].lower().split()\n",
        "\n",
        "        schema_tokens = [\n",
        "            f\"{table}.{col}\"\n",
        "            for table, cols in ex[\"schema\"].items()\n",
        "            for col in cols\n",
        "        ]\n",
        "\n",
        "        src_tokens = question_tokens + [\"<SEP>\"] + schema_tokens\n",
        "        src_ids = self.encode_src(src_tokens)\n",
        "\n",
        "        # -------- Decoder target (SQL) --------\n",
        "        sql_tokens = ex[\"sql\"].lower().split()\n",
        "        tgt_ids = self.encode_tgt(sql_tokens)\n",
        "\n",
        "        return (\n",
        "            torch.tensor(src_ids, dtype=torch.long),\n",
        "            torch.tensor(tgt_ids, dtype=torch.long)\n",
        "        )\n"
      ],
      "metadata": {
        "id": "wI-4VB83e63W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, dim_ff=1024, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_ff,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(layer, num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T]\n",
        "        pad_mask = (x == 0)        # True where PAD\n",
        "        emb = self.emb(x)\n",
        "        return self.encoder(emb, src_key_padding_mask=pad_mask)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, dim_ff=1024, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_ff,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(layer, num_layers)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, y, memory, memory_pad_mask):\n",
        "        # y: [B, T]\n",
        "        tgt_len = y.size(1)\n",
        "\n",
        "        # causal mask (prevent seeing future tokens)\n",
        "        causal_mask = torch.triu(\n",
        "            torch.ones(tgt_len, tgt_len, device=y.device),\n",
        "            diagonal=1\n",
        "        ).bool()\n",
        "\n",
        "        emb = self.emb(y)\n",
        "        out = self.decoder(\n",
        "            emb,\n",
        "            memory,\n",
        "            tgt_mask=causal_mask,\n",
        "            memory_key_padding_mask=memory_pad_mask\n",
        "        )\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "jYZJ6FBce662"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize models (PASS vocab sizes)\n",
        "enc = Encoder(vocab_size=len(ENC_VOCAB)).to(device)\n",
        "dec = Decoder(vocab_size=len(DEC_VOCAB)).to(device)\n",
        "\n",
        "# DataLoader (NO collate_fn)\n",
        "loader = DataLoader(\n",
        "    NL2SQLDataset(\n",
        "        data=dataset,\n",
        "        enc_vocab=ENC_VOCAB,\n",
        "        dec_vocab=DEC_VOCAB,\n",
        "        max_src_len=80,\n",
        "        max_tgt_len=60\n",
        "    ),\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "opt = optim.Adam(\n",
        "    list(enc.parameters()) + list(dec.parameters()),\n",
        "    lr=1e-4\n",
        ")\n",
        "\n",
        "# Loss (ignore PAD in decoder vocab)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=DEC_VOCAB[\"<PAD>\"])\n",
        "\n",
        "print(\"ðŸš€ Training...\")\n",
        "for epoch in range(5):\n",
        "    enc.train()\n",
        "    dec.train()\n",
        "    total = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Encoder\n",
        "        mem = enc(x)\n",
        "        src_pad_mask = (x == 0)\n",
        "\n",
        "        # Decoder (teacher forcing)\n",
        "        out = dec(\n",
        "            y=y[:, :-1],\n",
        "            memory=mem,\n",
        "            memory_pad_mask=src_pad_mask\n",
        "        )\n",
        "\n",
        "        # Loss\n",
        "        loss = loss_fn(\n",
        "            out.reshape(-1, len(DEC_VOCAB)),\n",
        "            y[:, 1:].reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss {total / len(loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "lOT2bgNzfO22",
        "outputId": "11ae9dae-7ef9-4611-d13e-459310b981b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training...\n",
            "Epoch 1 | Loss 0.1452\n",
            "Epoch 2 | Loss 0.0024\n",
            "Epoch 3 | Loss 0.0010\n",
            "Epoch 4 | Loss 0.0005\n",
            "Epoch 5 | Loss 0.0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_dec_vocab = {v: k for k, v in DEC_VOCAB.items()}\n",
        "\n",
        "def infer_sql(question, schema, max_len=50):\n",
        "    enc.eval()\n",
        "    dec.eval()\n",
        "\n",
        "    # Build encoder tokens\n",
        "    tokens = question.lower().split() + [\"<SEP>\"] + [\n",
        "        f\"{t}.{c}\" for t, cols in schema.items() for c in cols\n",
        "    ]\n",
        "\n",
        "    x = torch.tensor([\n",
        "        ENC_VOCAB.get(t, ENC_VOCAB[\"<UNK>\"]) for t in tokens\n",
        "    ]).unsqueeze(0).to(device)\n",
        "\n",
        "    # Encoder\n",
        "    with torch.no_grad():\n",
        "        memory = enc(x)\n",
        "        memory_pad_mask = (x == ENC_VOCAB[\"<PAD>\"])\n",
        "\n",
        "        # Decoder autoregressive loop\n",
        "        y = torch.tensor([[DEC_VOCAB[\"<BOS>\"]]], device=device)\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            logits = dec(\n",
        "                y=y,\n",
        "                memory=memory,\n",
        "                memory_pad_mask=memory_pad_mask\n",
        "            )\n",
        "\n",
        "            next_token = logits[:, -1].argmax(dim=-1)\n",
        "            y = torch.cat([y, next_token.unsqueeze(1)], dim=1)\n",
        "\n",
        "            if next_token.item() == DEC_VOCAB[\"<EOS>\"]:\n",
        "                break\n",
        "\n",
        "    # Decode tokens to SQL\n",
        "    sql_tokens = [\n",
        "        inv_dec_vocab[t.item()]\n",
        "        for t in y[0][1:]\n",
        "        if t.item() not in (\n",
        "            DEC_VOCAB[\"<EOS>\"],\n",
        "            DEC_VOCAB[\"<PAD>\"]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return \" \".join(sql_tokens)\n"
      ],
      "metadata": {
        "id": "3b0CvC5vfUbx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = {\n",
        "    \"employees\": [\"id\",\"name\",\"salary\",\"dept_id\"],\n",
        "    \"departments\": [\"id\",\"name\"]\n",
        "}\n",
        "\n",
        "print(infer_sql(\"give names of employees\", schema))\n",
        "print(infer_sql(\"show name of employees where salary is greater than 50\", schema))\n"
      ],
      "metadata": {
        "id": "hV2HyXhvfVJI",
        "outputId": "8883d034-c08e-4a4c-833a-9767c3ea49bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "select name from employees\n",
            "select salary from employees where salary > 50\n"
          ]
        }
      ]
    }
  ]
}