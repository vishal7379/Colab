{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMu9CYyPfQK8iaHQIW9Sl9F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/NL_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "# ---------------- SCHEMAS ----------------\n",
        "SCHEMAS = [\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"employees\": [\"id\", \"name\", \"salary\", \"dept_id\"],\n",
        "            \"departments\": [\"id\", \"name\"]\n",
        "        },\n",
        "        \"numeric\": {\n",
        "            \"employees\": [\"id\", \"salary\", \"dept_id\"]\n",
        "        },\n",
        "        \"text\": {\n",
        "            \"employees\": [\"name\"]\n",
        "        },\n",
        "        \"join\": (\"employees\", \"departments\", \"dept_id\", \"id\")\n",
        "    },\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"students\": [\"id\", \"name\", \"marks\", \"class_id\"],\n",
        "            \"classes\": [\"id\", \"name\"]\n",
        "        },\n",
        "        \"numeric\": {\n",
        "            \"students\": [\"id\", \"marks\", \"class_id\"]\n",
        "        },\n",
        "        \"text\": {\n",
        "            \"students\": [\"name\"]\n",
        "        },\n",
        "        \"join\": (\"students\", \"classes\", \"class_id\", \"id\")\n",
        "    }\n",
        "]\n",
        "\n",
        "AGGS = [\"COUNT\", \"SUM\", \"AVG\", \"MAX\", \"MIN\"]\n",
        "\n",
        "# ---------------- GENERATOR ----------------\n",
        "def generate_example():\n",
        "    db = random.choice(SCHEMAS)\n",
        "    schema = db[\"tables\"]\n",
        "\n",
        "    main_table = list(schema.keys())[0]\n",
        "    cols = schema[main_table]\n",
        "\n",
        "    intent = random.choices(\n",
        "        [\"SELECT\", \"AGG\", \"WHERE\", \"GROUP\", \"JOIN\"],\n",
        "        weights=[0.30, 0.25, 0.20, 0.15, 0.10]\n",
        "    )[0]\n",
        "\n",
        "    # ---------- SELECT ----------\n",
        "    if intent == \"SELECT\":\n",
        "        col = random.choice(cols)\n",
        "        question = random.choice([\n",
        "            f\"show {col} of {main_table}\",\n",
        "            f\"get {col} from {main_table}\",\n",
        "            f\"list {col} in {main_table}\",\n",
        "            f\"display {col} for {main_table}\"\n",
        "        ])\n",
        "        sql = f\"SELECT {col} FROM {main_table}\"\n",
        "\n",
        "    # ---------- AGGREGATION ----------\n",
        "    elif intent == \"AGG\":\n",
        "        agg = random.choice(AGGS)\n",
        "\n",
        "        if agg == \"COUNT\":\n",
        "            col = random.choice(cols)\n",
        "        else:\n",
        "            col = random.choice(db[\"numeric\"][main_table])\n",
        "\n",
        "        question = random.choice([\n",
        "            f\"show {agg.lower()} of {col} of {main_table}\",\n",
        "            f\"what is the {agg.lower()} {col} in {main_table}\",\n",
        "            f\"give me the {agg.lower()} {col} from {main_table}\"\n",
        "        ])\n",
        "        sql = f\"SELECT {agg}({col}) FROM {main_table}\"\n",
        "\n",
        "    # ---------- WHERE ----------\n",
        "    elif intent == \"WHERE\":\n",
        "        col = random.choice(db[\"numeric\"][main_table])\n",
        "        val = random.choice([10, 20, 50, 100])\n",
        "        question = random.choice([\n",
        "            f\"show {col} of {main_table} where {col} > {val}\",\n",
        "            f\"list {main_table} with {col} greater than {val}\",\n",
        "            f\"get {col} from {main_table} having {col} above {val}\"\n",
        "        ])\n",
        "        sql = f\"SELECT {col} FROM {main_table} WHERE {col} > {val}\"\n",
        "\n",
        "    # ---------- GROUP BY ----------\n",
        "    elif intent == \"GROUP\":\n",
        "        agg = random.choice([\"COUNT\", \"AVG\"])\n",
        "        group_col = random.choice(db[\"text\"][main_table])\n",
        "        num_col = random.choice(db[\"numeric\"][main_table])\n",
        "\n",
        "        question = random.choice([\n",
        "            f\"show {agg.lower()} of {num_col} per {group_col}\",\n",
        "            f\"get {agg.lower()} {num_col} grouped by {group_col}\",\n",
        "            f\"list {group_col} wise {agg.lower()} {num_col}\"\n",
        "        ])\n",
        "        sql = (\n",
        "            f\"SELECT {group_col}, {agg}({num_col}) \"\n",
        "            f\"FROM {main_table} GROUP BY {group_col}\"\n",
        "        )\n",
        "\n",
        "    # ---------- JOIN ----------\n",
        "    else:\n",
        "        t1, t2, c1, c2 = db[\"join\"]\n",
        "        question = random.choice([\n",
        "            f\"show {t1} name and {t2} name\",\n",
        "            f\"get {t1} names with their {t2}\",\n",
        "            f\"list {t1} and corresponding {t2}\"\n",
        "        ])\n",
        "        sql = (\n",
        "            f\"SELECT {t1}.name, {t2}.name \"\n",
        "            f\"FROM {t1} JOIN {t2} ON {t1}.{c1} = {t2}.{c2}\"\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"schema\": schema,\n",
        "        \"sql\": sql\n",
        "    }\n",
        "\n",
        "# ---------------- DATASET CREATION ----------------\n",
        "def generate_dataset(n=80_000, outfile=\"nl2sql_varied.json\"):\n",
        "    data = []\n",
        "    for i in range(n):\n",
        "        data.append(generate_example())\n",
        "        if (i + 1) % 20_000 == 0:\n",
        "            print(f\"Generated {i+1} examples\")\n",
        "\n",
        "    with open(outfile, \"w\") as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    print(\"âœ… Dataset size:\", len(data))\n",
        "\n",
        "\n",
        "# ðŸ”¥ CHANGE n TO 300_000 IF YOU WANT\n",
        "generate_dataset(n=80_000)\n"
      ],
      "metadata": {
        "id": "ZkCMmK9pe6xl",
        "outputId": "bebb5e86-d928-40a7-f442-2c6c999e2961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 20000 examples\n",
            "Generated 40000 examples\n",
            "Generated 60000 examples\n",
            "Generated 80000 examples\n",
            "âœ… Dataset size: 80000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# with open(\"nl2sql_varied.json\", \"r\") as f:\n",
        "#     dataset = json.load(f)\n",
        "\n",
        "# print(\"Loaded dataset size:\", len(dataset))\n",
        "\n",
        "# from collections import Counter\n",
        "\n",
        "# ENC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<SEP>\":2}\n",
        "# DEC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<BOS>\":2, \"<EOS>\":3}\n",
        "\n",
        "\n",
        "# def add(vocab, tok):\n",
        "#     if tok not in vocab:\n",
        "#         vocab[tok] = len(vocab)\n",
        "\n",
        "# for ex in dataset:\n",
        "#     # NL tokens\n",
        "#     for t in ex[\"question\"].lower().split():\n",
        "#         add(ENC_VOCAB, t)\n",
        "\n",
        "#     # Schema tokens\n",
        "#     for t, cols in ex[\"schema\"].items():\n",
        "#         add(ENC_VOCAB, t)\n",
        "#         for c in cols:\n",
        "#             add(ENC_VOCAB, f\"{t}.{c}\")\n",
        "\n",
        "#     # SQL tokens\n",
        "#     for ex in dataset:\n",
        "#       for t in ex[\"sql\"].lower().split():\n",
        "#           if t not in DEC_VOCAB:\n",
        "#               DEC_VOCAB[t] = len(DEC_VOCAB)\n",
        "\n",
        "\n",
        "# print(\"Encoder vocab:\", len(ENC_VOCAB))\n",
        "# print(\"Decoder vocab:\", len(DEC_VOCAB))\n",
        "import json\n",
        "\n",
        "with open(\"nl2sql_varied.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(\"Loaded dataset size:\", len(dataset))\n",
        "\n",
        "ENC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<SEP>\":2}\n",
        "DEC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<BOS>\":2, \"<EOS>\":3}\n",
        "\n",
        "def add(vocab, tok):\n",
        "    if tok not in vocab:\n",
        "        vocab[tok] = len(vocab)\n",
        "\n",
        "for ex in dataset:\n",
        "    # -------- Encoder vocab --------\n",
        "    for t in ex[\"question\"].lower().split():\n",
        "        add(ENC_VOCAB, t)\n",
        "\n",
        "    for table, cols in ex[\"schema\"].items():\n",
        "        add(ENC_VOCAB, table)\n",
        "        for col in cols:\n",
        "            add(ENC_VOCAB, f\"{table}.{col}\")\n",
        "\n",
        "    # -------- Decoder vocab --------\n",
        "    for t in ex[\"sql\"].lower().split():\n",
        "        add(DEC_VOCAB, t)\n",
        "\n",
        "print(\"Encoder vocab:\", len(ENC_VOCAB))\n",
        "print(\"Decoder vocab:\", len(DEC_VOCAB))\n"
      ],
      "metadata": {
        "id": "cI1fIGF0e606",
        "outputId": "81319a79-03b4-4adf-e3bb-14a7aab53f68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset size: 80000\n",
            "Encoder vocab: 62\n",
            "Decoder vocab: 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "    def __init__(self, data, enc_vocab, dec_vocab,\n",
        "                 max_src_len=80, max_tgt_len=60):\n",
        "        self.data = data\n",
        "        self.enc_vocab = enc_vocab\n",
        "        self.dec_vocab = dec_vocab\n",
        "        self.max_src_len = max_src_len\n",
        "        self.max_tgt_len = max_tgt_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def encode_src(self, tokens):\n",
        "        ids = [self.enc_vocab.get(t, self.enc_vocab[\"<UNK>\"]) for t in tokens]\n",
        "        ids = ids[:self.max_src_len]\n",
        "        pad_len = self.max_src_len - len(ids)\n",
        "        return ids + [self.enc_vocab[\"<PAD>\"]] * pad_len\n",
        "\n",
        "    def encode_tgt(self, tokens):\n",
        "        ids = [self.dec_vocab[\"<BOS>\"]] + \\\n",
        "              [self.dec_vocab.get(t, self.dec_vocab[\"<UNK>\"]) for t in tokens] + \\\n",
        "              [self.dec_vocab[\"<EOS>\"]]\n",
        "\n",
        "        ids = ids[:self.max_tgt_len]\n",
        "        pad_len = self.max_tgt_len - len(ids)\n",
        "        return ids + [self.dec_vocab[\"<PAD>\"]] * pad_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "\n",
        "        # -------- Encoder input (NL + schema) --------\n",
        "        question_tokens = ex[\"question\"].lower().split()\n",
        "\n",
        "        schema_tokens = [\n",
        "            f\"{table}.{col}\"\n",
        "            for table, cols in ex[\"schema\"].items()\n",
        "            for col in cols\n",
        "        ]\n",
        "\n",
        "        src_tokens = question_tokens + [\"<SEP>\"] + schema_tokens\n",
        "        src_ids = self.encode_src(src_tokens)\n",
        "\n",
        "        # -------- Decoder target (SQL) --------\n",
        "        sql_tokens = ex[\"sql\"].lower().split()\n",
        "        tgt_ids = self.encode_tgt(sql_tokens)\n",
        "\n",
        "        return (\n",
        "            torch.tensor(src_ids, dtype=torch.long),\n",
        "            torch.tensor(tgt_ids, dtype=torch.long)\n",
        "        )\n"
      ],
      "metadata": {
        "id": "wI-4VB83e63W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, dim_ff=1024, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_ff,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(layer, num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T]\n",
        "        pad_mask = (x == 0)        # True where PAD\n",
        "        emb = self.emb(x)\n",
        "        return self.encoder(emb, src_key_padding_mask=pad_mask)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        d_model=256,\n",
        "        nhead=8,\n",
        "        dim_ff=1024,\n",
        "        num_layers=4,\n",
        "        dropout=0.2\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding with dropout\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_ff,\n",
        "            dropout=dropout,          # ðŸ”¥ IMPORTANT\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.TransformerDecoder(layer, num_layers)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, y, memory, memory_pad_mask):\n",
        "        tgt_len = y.size(1)\n",
        "\n",
        "        # Causal mask\n",
        "        causal_mask = torch.triu(\n",
        "            torch.ones(tgt_len, tgt_len, device=y.device),\n",
        "            diagonal=1\n",
        "        ).bool()\n",
        "\n",
        "        emb = self.emb(y)\n",
        "        emb = self.emb_dropout(emb)   # ðŸ”¥ Dropout applied\n",
        "\n",
        "        out = self.decoder(\n",
        "            emb,\n",
        "            memory,\n",
        "            tgt_mask=causal_mask,\n",
        "            memory_key_padding_mask=memory_pad_mask\n",
        "        )\n",
        "\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "jYZJ6FBce662"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize models (PASS vocab sizes)\n",
        "enc = Encoder(vocab_size=len(ENC_VOCAB)).to(device)\n",
        "dec = Decoder(vocab_size=len(DEC_VOCAB)).to(device)\n",
        "\n",
        "# DataLoader (NO collate_fn)\n",
        "loader = DataLoader(\n",
        "    NL2SQLDataset(\n",
        "        data=dataset,\n",
        "        enc_vocab=ENC_VOCAB,\n",
        "        dec_vocab=DEC_VOCAB,\n",
        "        max_src_len=80,\n",
        "        max_tgt_len=60\n",
        "    ),\n",
        "    batch_size=128,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "opt = optim.Adam(\n",
        "    list(enc.parameters()) + list(dec.parameters()),\n",
        "    lr=1e-4\n",
        ")\n",
        "\n",
        "# Loss (ignore PAD in decoder vocab)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=DEC_VOCAB[\"<PAD>\"])\n",
        "\n",
        "print(\"ðŸš€ Training...\")\n",
        "for epoch in range(3):\n",
        "    enc.train()\n",
        "    dec.train()\n",
        "    total = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Encoder\n",
        "        mem = enc(x)\n",
        "        src_pad_mask = (x == 0)\n",
        "\n",
        "        # Decoder (teacher forcing)\n",
        "        out = dec(\n",
        "            y=y[:, :-1],\n",
        "            memory=mem,\n",
        "            memory_pad_mask=src_pad_mask\n",
        "        )\n",
        "\n",
        "        # Loss\n",
        "        loss = loss_fn(\n",
        "            out.reshape(-1, len(DEC_VOCAB)),\n",
        "            y[:, 1:].reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss {total / len(loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "lOT2bgNzfO22",
        "outputId": "30af2205-f9ed-45bb-eb76-a9e3ff183dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training...\n",
            "Epoch 1 | Loss 0.2043\n",
            "Epoch 2 | Loss 0.0037\n",
            "Epoch 3 | Loss 0.0016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "SAVE_PATH = \"nl2sql_transformer.pt\"\n",
        "\n",
        "torch.save({\n",
        "    \"encoder_state_dict\": enc.state_dict(),\n",
        "    \"decoder_state_dict\": dec.state_dict(),\n",
        "    \"ENC_VOCAB\": ENC_VOCAB,\n",
        "    \"DEC_VOCAB\": DEC_VOCAB,\n",
        "    \"model_config\": {\n",
        "        \"d_model\": 256,\n",
        "        \"nhead\": 8,\n",
        "        \"dim_ff\": 1024,\n",
        "        \"num_layers\": 4,\n",
        "        \"dropout\": 0.2\n",
        "    }\n",
        "}, SAVE_PATH)\n",
        "\n",
        "print(f\"âœ… Model successfully saved to {SAVE_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "u9lDQFagCH-V",
        "outputId": "f573a314-1886-49e1-f311-8e1ea1666a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'enc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3087773501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m torch.save({\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m\"encoder_state_dict\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"decoder_state_dict\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"ENC_VOCAB\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mENC_VOCAB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'enc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SQL_KEYWORDS = {\"select\", \"from\", \"where\", \"join\", \"group\", \"by\"}\n",
        "\n",
        "def is_valid_next_token(prev_tokens, next_token):\n",
        "    prev_tokens = [t.lower() for t in prev_tokens]\n",
        "    nt = next_token.lower()\n",
        "\n",
        "    # Must start with SELECT\n",
        "    if len(prev_tokens) == 0:\n",
        "        return nt == \"select\"\n",
        "\n",
        "    # FROM cannot come before SELECT\n",
        "    if nt == \"from\" and \"select\" not in prev_tokens:\n",
        "        return False\n",
        "\n",
        "    # WHERE / GROUP cannot come before FROM\n",
        "    if nt in {\"where\", \"group\"} and \"from\" not in prev_tokens:\n",
        "        return False\n",
        "\n",
        "    # JOIN cannot come before FROM\n",
        "    if nt == \"join\" and \"from\" not in prev_tokens:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "inv_dec_vocab = {v: k for k, v in DEC_VOCAB.items()}\n",
        "\n",
        "def infer_sql(question, schema, max_len=50):\n",
        "    enc.eval()\n",
        "    dec.eval()\n",
        "\n",
        "    tokens = question.lower().split() + [\"<SEP>\"] + [\n",
        "        f\"{t}.{c}\" for t, cols in schema.items() for c in cols\n",
        "    ]\n",
        "\n",
        "    x = torch.tensor([\n",
        "        ENC_VOCAB.get(t, ENC_VOCAB[\"<UNK>\"]) for t in tokens\n",
        "    ]).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        memory = enc(x)\n",
        "        memory_pad_mask = (x == ENC_VOCAB[\"<PAD>\"])\n",
        "\n",
        "        y = torch.tensor([[DEC_VOCAB[\"<BOS>\"]]], device=device)\n",
        "        generated_tokens = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            logits = dec(\n",
        "                y=y,\n",
        "                memory=memory,\n",
        "                memory_pad_mask=memory_pad_mask\n",
        "            )\n",
        "\n",
        "            probs = logits[:, -1].softmax(dim=-1)\n",
        "            sorted_ids = torch.argsort(probs, descending=True)\n",
        "\n",
        "            next_token_id = None\n",
        "\n",
        "            # ðŸ”¥ PICARD-style filtering\n",
        "            for tok_id in sorted_ids[0]:\n",
        "                tok = inv_dec_vocab[tok_id.item()]\n",
        "\n",
        "                if tok in {\"<PAD>\", \"<BOS>\"}:\n",
        "                    continue\n",
        "\n",
        "                if is_valid_next_token(generated_tokens, tok):\n",
        "                    next_token_id = tok_id\n",
        "                    break\n",
        "\n",
        "            # Fallback (should rarely happen)\n",
        "            if next_token_id is None:\n",
        "                next_token_id = sorted_ids[0][0]\n",
        "\n",
        "            y = torch.cat([y, next_token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "            generated_tokens.append(inv_dec_vocab[next_token_id.item()])\n",
        "\n",
        "            if next_token_id.item() == DEC_VOCAB[\"<EOS>\"]:\n",
        "                break\n",
        "\n",
        "    return \" \".join(\n",
        "        t for t in generated_tokens\n",
        "        if t not in {\"<EOS>\", \"<PAD>\"}\n",
        "    )\n"
      ],
      "metadata": {
        "id": "3b0CvC5vfUbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = {\n",
        "    \"employees\": [\"id\",\"name\",\"salary\",\"dept_id\"],\n",
        "    \"departments\": [\"id\",\"name\"]\n",
        "}\n",
        "\n",
        "print(infer_sql(\"give names of employees\", schema))\n",
        "print(infer_sql(\"get count salary grouped by name\", schema))\n",
        "print(infer_sql(\"get marks from students having marks above 20\", schema))\n",
        "print(infer_sql(\"show students name and classes name\", schema))"
      ],
      "metadata": {
        "id": "hV2HyXhvfVJI",
        "outputId": "692b9888-6203-48c3-c11d-9088b30f9f85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "select name from employees\n",
            "select name, count(salary) from employees group by name\n",
            "select marks from employees where marks > 20\n",
            "select students.name, classes.name from employees join classes on students.class_id = departments.id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# NL â†’ SQL Transformer (OVERFITTING FIXED â€“ SINGLE CELL)\n",
        "# ============================================================\n",
        "\n",
        "import random, json, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============================================================\n",
        "# DATASET GENERATION\n",
        "# ============================================================\n",
        "\n",
        "SCHEMAS = [\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"employees\": [\"id\", \"name\", \"salary\", \"dept_id\"],\n",
        "            \"departments\": [\"id\", \"name\"]\n",
        "        },\n",
        "        \"numeric\": {\"employees\": [\"id\", \"salary\", \"dept_id\"]},\n",
        "        \"text\": {\"employees\": [\"name\"]},\n",
        "        \"join\": (\"employees\", \"departments\", \"dept_id\", \"id\")\n",
        "    },\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"students\": [\"id\", \"name\", \"marks\", \"class_id\"],\n",
        "            \"classes\": [\"id\", \"name\"]\n",
        "        },\n",
        "        \"numeric\": {\"students\": [\"id\", \"marks\", \"class_id\"]},\n",
        "        \"text\": {\"students\": [\"name\"]},\n",
        "        \"join\": (\"students\", \"classes\", \"class_id\", \"id\")\n",
        "    }\n",
        "]\n",
        "\n",
        "AGGS = [\"COUNT\", \"SUM\", \"AVG\", \"MAX\", \"MIN\"]\n",
        "\n",
        "def generate_example():\n",
        "    db = random.choice(SCHEMAS)\n",
        "    schema = db[\"tables\"]\n",
        "    main = list(schema.keys())[0]\n",
        "    cols = schema[main]\n",
        "\n",
        "    intent = random.choices(\n",
        "        [\"SELECT\",\"AGG\",\"WHERE\",\"GROUP\",\"JOIN\"],\n",
        "        weights=[0.30,0.25,0.20,0.15,0.10]\n",
        "    )[0]\n",
        "\n",
        "    if intent == \"SELECT\":\n",
        "        col = random.choice(cols)\n",
        "        q = f\"show {col} of {main}\"\n",
        "        sql = f\"SELECT {col} FROM {main}\"\n",
        "\n",
        "    elif intent == \"AGG\":\n",
        "        agg = random.choice(AGGS)\n",
        "        col = random.choice(db[\"numeric\"][main]) if agg!=\"COUNT\" else random.choice(cols)\n",
        "        q = f\"get {agg.lower()} {col} from {main}\"\n",
        "        sql = f\"SELECT {agg}({col}) FROM {main}\"\n",
        "\n",
        "    elif intent == \"WHERE\":\n",
        "        col = random.choice(db[\"numeric\"][main])\n",
        "        val = random.choice([10,20,50,100])\n",
        "        q = f\"get {col} from {main} where {col} > {val}\"\n",
        "        sql = f\"SELECT {col} FROM {main} WHERE {col} > {val}\"\n",
        "\n",
        "    elif intent == \"GROUP\":\n",
        "        agg = random.choice([\"COUNT\",\"AVG\"])\n",
        "        g = random.choice(db[\"text\"][main])\n",
        "        n = random.choice(db[\"numeric\"][main])\n",
        "        q = f\"get {agg.lower()} {n} grouped by {g}\"\n",
        "        sql = f\"SELECT {g}, {agg}({n}) FROM {main} GROUP BY {g}\"\n",
        "\n",
        "    else:\n",
        "        t1,t2,c1,c2 = db[\"join\"]\n",
        "        q = f\"show {t1} and {t2} names\"\n",
        "        sql = f\"SELECT {t1}.name, {t2}.name FROM {t1} JOIN {t2} ON {t1}.{c1} = {t2}.{c2}\"\n",
        "\n",
        "    return {\"question\": q, \"schema\": schema, \"sql\": sql}\n",
        "\n",
        "dataset = [generate_example() for _ in range(80000)]\n",
        "\n",
        "# ============================================================\n",
        "# VOCAB BUILDING\n",
        "# ============================================================\n",
        "\n",
        "ENC_VOCAB = {\"<PAD>\":0,\"<UNK>\":1,\"<SEP>\":2}\n",
        "DEC_VOCAB = {\"<PAD>\":0,\"<UNK>\":1,\"<BOS>\":2,\"<EOS>\":3}\n",
        "\n",
        "def add(v,t):\n",
        "    if t not in v: v[t]=len(v)\n",
        "\n",
        "for ex in dataset:\n",
        "    for t in ex[\"question\"].lower().split(): add(ENC_VOCAB,t)\n",
        "    for tb,cs in ex[\"schema\"].items():\n",
        "        add(ENC_VOCAB,tb)\n",
        "        for c in cs: add(ENC_VOCAB,f\"{tb}.{c}\")\n",
        "    for t in ex[\"sql\"].lower().split(): add(DEC_VOCAB,t)\n",
        "\n",
        "# ============================================================\n",
        "# DATASET CLASS (SCHEMA SHUFFLING)\n",
        "# ============================================================\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "    def __init__(self,data):\n",
        "        self.data=data\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        ex=self.data[i]\n",
        "        q = ex[\"question\"].lower().split()\n",
        "\n",
        "        schema_tokens=[\n",
        "            f\"{t}.{c}\" for t,cs in ex[\"schema\"].items() for c in cs\n",
        "        ]\n",
        "        random.shuffle(schema_tokens)  # ðŸ”¥ CRITICAL\n",
        "\n",
        "        src = q + [\"<SEP>\"] + schema_tokens\n",
        "        src_ids=[ENC_VOCAB.get(t,1) for t in src][:80]\n",
        "        src_ids += [0]*(80-len(src_ids))\n",
        "\n",
        "        tgt=[DEC_VOCAB[\"<BOS>\"]] + \\\n",
        "            [DEC_VOCAB.get(t,1) for t in ex[\"sql\"].lower().split()] + \\\n",
        "            [DEC_VOCAB[\"<EOS>\"]]\n",
        "        tgt=tgt[:60]\n",
        "        tgt+=[0]*(60-len(tgt))\n",
        "\n",
        "        return torch.tensor(src_ids), torch.tensor(tgt)\n",
        "\n",
        "train_data, val_data = train_test_split(dataset, test_size=0.15, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(NL2SQLDataset(train_data), batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(NL2SQLDataset(val_data), batch_size=128)\n",
        "\n",
        "# ============================================================\n",
        "# MODEL (REDUCED CAPACITY)\n",
        "# ============================================================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,v):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(v,192,padding_idx=0)\n",
        "        layer=nn.TransformerEncoderLayer(192,6,768,batch_first=True)\n",
        "        self.enc=nn.TransformerEncoder(layer,3)\n",
        "\n",
        "    def forward(self,x):\n",
        "        mask=(x==0)\n",
        "        e=self.emb(x)\n",
        "        e=nn.functional.dropout(e,0.1,self.training)\n",
        "        return self.enc(e,src_key_padding_mask=mask)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,v):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(v,192)\n",
        "        layer=nn.TransformerDecoderLayer(192,6,768,dropout=0.2,batch_first=True)\n",
        "        self.dec=nn.TransformerDecoder(layer,3)\n",
        "        self.fc=nn.Linear(192,v)\n",
        "\n",
        "    def forward(self,y,mem,mask):\n",
        "        L=y.size(1)\n",
        "        causal=torch.triu(torch.ones(L,L,device=y.device),1).bool()\n",
        "        e=self.emb(y)\n",
        "        o=self.dec(e,mem,tgt_mask=causal,memory_key_padding_mask=mask)\n",
        "        return self.fc(o)\n",
        "\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "enc,dec=Encoder(len(ENC_VOCAB)).to(device),Decoder(len(DEC_VOCAB)).to(device)\n",
        "\n",
        "opt=optim.Adam(list(enc.parameters())+list(dec.parameters()),lr=1e-4)\n",
        "loss_fn=nn.CrossEntropyLoss(ignore_index=0,label_smoothing=0.1)\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING + VALIDATION\n",
        "# ============================================================\n",
        "\n",
        "def eval_loss():\n",
        "    enc.eval(); dec.eval(); tot=0\n",
        "    with torch.no_grad():\n",
        "        for x,y in val_loader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            mem=enc(x)\n",
        "            out=dec(y[:,:-1],mem,(x==0))\n",
        "            loss=loss_fn(out.reshape(-1,len(DEC_VOCAB)),y[:,1:].reshape(-1))\n",
        "            tot+=loss.item()\n",
        "    return tot/len(val_loader)\n",
        "\n",
        "print(\"ðŸš€ Training\")\n",
        "best=1e9\n",
        "for e in range(2):\n",
        "    enc.train(); dec.train()\n",
        "    for x,y in train_loader:\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        opt.zero_grad()\n",
        "        mem=enc(x)\n",
        "        out=dec(y[:,:-1],mem,(x==0))\n",
        "        loss=loss_fn(out.reshape(-1,len(DEC_VOCAB)),y[:,1:].reshape(-1))\n",
        "        loss.backward(); opt.step()\n",
        "    v=eval_loss()\n",
        "    print(f\"Epoch {e+1} | Val Loss {v:.4f}\")\n",
        "    if v>best: break\n",
        "    best=v\n",
        "\n",
        "# ============================================================\n",
        "# SAVE MODEL\n",
        "# ============================================================\n",
        "\n",
        "torch.save({\n",
        "    \"enc\":enc.state_dict(),\n",
        "    \"dec\":dec.state_dict(),\n",
        "    \"ENC_VOCAB\":ENC_VOCAB,\n",
        "    \"DEC_VOCAB\":DEC_VOCAB\n",
        "},\"nl2sql_fixed.pt\")\n",
        "\n",
        "print(\"âœ… Model saved: nl2sql_fixed.pt\")\n"
      ],
      "metadata": {
        "id": "1_6u0ezDSTiL",
        "outputId": "844f5c42-de83-4e30-f867-af494ba678bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training\n",
            "Epoch 1 | Val Loss 0.7518\n",
            "Epoch 2 | Val Loss 0.7433\n",
            "âœ… Model saved: nl2sql_fixed.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# NL â†’ SQL INFERENCE WITH PICARD\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---------------- LOAD MODEL ----------------\n",
        "ckpt = torch.load(\"nl2sql_fixed.pt\", map_location=\"cpu\")\n",
        "\n",
        "ENC_VOCAB = ckpt[\"ENC_VOCAB\"]\n",
        "DEC_VOCAB = ckpt[\"DEC_VOCAB\"]\n",
        "inv_dec_vocab = {v: k for k, v in DEC_VOCAB.items()}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------------- MODEL DEFINITIONS ----------------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, 192, padding_idx=0)\n",
        "        layer = nn.TransformerEncoderLayer(192, 6, 768, batch_first=True)\n",
        "        self.enc = nn.TransformerEncoder(layer, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = (x == 0)\n",
        "        return self.enc(self.emb(x), src_key_padding_mask=mask)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, 192)\n",
        "        layer = nn.TransformerDecoderLayer(\n",
        "            192, 6, 768, dropout=0.2, batch_first=True\n",
        "        )\n",
        "        self.dec = nn.TransformerDecoder(layer, 3)\n",
        "        self.fc = nn.Linear(192, vocab)\n",
        "\n",
        "    def forward(self, y, mem, mem_mask):\n",
        "        L = y.size(1)\n",
        "        causal = torch.triu(torch.ones(L, L, device=y.device), 1).bool()\n",
        "        out = self.dec(self.emb(y), mem,\n",
        "                       tgt_mask=causal,\n",
        "                       memory_key_padding_mask=mem_mask)\n",
        "        return self.fc(out)\n",
        "\n",
        "# ---------------- INIT MODELS ----------------\n",
        "enc = Encoder(len(ENC_VOCAB)).to(device)\n",
        "dec = Decoder(len(DEC_VOCAB)).to(device)\n",
        "enc.load_state_dict(ckpt[\"enc\"])\n",
        "dec.load_state_dict(ckpt[\"dec\"])\n",
        "enc.eval(); dec.eval()\n",
        "\n",
        "# ============================================================\n",
        "# ðŸ”’ PICARD CONSTRAINTS\n",
        "# ============================================================\n",
        "\n",
        "SQL_KEYWORDS = {\"select\", \"from\", \"where\", \"join\", \"group\", \"by\"}\n",
        "\n",
        "def is_valid_next_token(prev_tokens, next_token):\n",
        "    pt = [t.lower() for t in prev_tokens]\n",
        "    nt = next_token.lower()\n",
        "\n",
        "    # Must start with SELECT\n",
        "    if len(pt) == 0:\n",
        "        return nt == \"select\"\n",
        "\n",
        "    # FROM must come after SELECT\n",
        "    if nt == \"from\" and \"select\" not in pt:\n",
        "        return False\n",
        "\n",
        "    # WHERE / GROUP must come after FROM\n",
        "    if nt in {\"where\", \"group\"} and \"from\" not in pt:\n",
        "        return False\n",
        "\n",
        "    # BY must follow GROUP\n",
        "    if nt == \"by\" and (len(pt) == 0 or pt[-1] != \"group\"):\n",
        "        return False\n",
        "\n",
        "    # JOIN must come after FROM\n",
        "    if nt == \"join\" and \"from\" not in pt:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# ============================================================\n",
        "# ðŸ”® INFERENCE FUNCTION (PICARD)\n",
        "# ============================================================\n",
        "\n",
        "def infer_sql(question, schema, max_len=60):\n",
        "    # ---- encoder input ----\n",
        "    tokens = question.lower().split() + [\"<SEP>\"] + [\n",
        "        f\"{t}.{c}\" for t, cols in schema.items() for c in cols\n",
        "    ]\n",
        "\n",
        "    src_ids = [ENC_VOCAB.get(t, ENC_VOCAB[\"<UNK>\"]) for t in tokens][:80]\n",
        "    src_ids += [ENC_VOCAB[\"<PAD>\"]] * (80 - len(src_ids))\n",
        "\n",
        "    x = torch.tensor(src_ids).unsqueeze(0).to(device)\n",
        "    src_mask = (x == ENC_VOCAB[\"<PAD>\"])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        memory = enc(x)\n",
        "\n",
        "        y = torch.tensor([[DEC_VOCAB[\"<BOS>\"]]], device=device)\n",
        "        generated = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            logits = dec(y, memory, src_mask)\n",
        "            probs = logits[:, -1].softmax(dim=-1)\n",
        "\n",
        "            sorted_ids = torch.argsort(probs, descending=True)[0]\n",
        "            next_id = None\n",
        "\n",
        "            # ðŸ”¥ PICARD FILTERING\n",
        "            for tid in sorted_ids:\n",
        "                tok = inv_dec_vocab[tid.item()]\n",
        "                if tok in {\"<PAD>\", \"<BOS>\"}:\n",
        "                    continue\n",
        "                if is_valid_next_token(generated, tok):\n",
        "                    next_id = tid\n",
        "                    break\n",
        "\n",
        "            if next_id is None:\n",
        "                next_id = sorted_ids[0]\n",
        "\n",
        "            token = inv_dec_vocab[next_id.item()]\n",
        "            if token == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "            generated.append(token)\n",
        "            y = torch.cat([y, next_id.view(1,1)], dim=1)\n",
        "\n",
        "    return \" \".join(generated)\n",
        "\n",
        "# ============================================================\n",
        "# ðŸ”¥ TEST EXAMPLES\n",
        "# ============================================================\n",
        "\n",
        "schema1 = {\n",
        "    \"employees\": [\"id\",\"name\",\"salary\",\"dept_id\"],\n",
        "    \"departments\": [\"id\",\"name\"]\n",
        "}\n",
        "\n",
        "schema2 = {\n",
        "    \"students\": [\"id\",\"name\",\"marks\",\"class_id\"],\n",
        "    \"classes\": [\"id\",\"name\"]\n",
        "}\n",
        "\n",
        "print(infer_sql(\"show name of employees\", schema1))\n",
        "print(infer_sql(\"show salary sum from employees\", schema1))\n",
        "print(infer_sql(\"get name from students where marks > 20\", schema2))\n",
        "print(infer_sql(\"show students and classes names\", schema2))\n"
      ],
      "metadata": {
        "id": "7tbFCRTLFR05",
        "outputId": "73a3e326-a1a1-467f-b084-b3d09a6c2485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "select dept_id from employees\n",
            "select sum(salary) from employees\n",
            "select marks from students where marks > 20\n",
            "select students.name, classes.name from students join classes on students.class_id = classes.id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# NL â†’ SQL STRUCTURE â†’ SQL (TRUE SEMANTIC PARSING)\n",
        "# TRANSFORMER BUILT FROM SCRATCH â€” SINGLE CELL\n",
        "# ============================================================\n",
        "\n",
        "import random, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from dataclasses import dataclass\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============================================================\n",
        "# SCHEMA DEFINITIONS\n",
        "# ============================================================\n",
        "\n",
        "SCHEMAS = [\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"employees\": [\"id\",\"name\",\"salary\",\"dept_id\"],\n",
        "            \"departments\": [\"id\",\"name\"]\n",
        "        },\n",
        "        \"join\": (\"employees\",\"departments\",\"dept_id\",\"id\")\n",
        "    },\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"students\": [\"id\",\"name\",\"marks\",\"class_id\"],\n",
        "            \"classes\": [\"id\",\"name\"]\n",
        "        },\n",
        "        \"join\": (\"students\",\"classes\",\"class_id\",\"id\")\n",
        "    }\n",
        "]\n",
        "\n",
        "AGGS = [\"NONE\",\"COUNT\",\"SUM\",\"AVG\",\"MAX\",\"MIN\"]\n",
        "OPS  = [\"NONE\",\">\",\"<\",\"=\"]\n",
        "\n",
        "# ============================================================\n",
        "# SQL AST\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class SQLQuery:\n",
        "    table:str\n",
        "    select_col:str\n",
        "    agg:str\n",
        "    where_col:str\n",
        "    op:str\n",
        "    value:str\n",
        "    join_table:str\n",
        "\n",
        "def render_sql(q:SQLQuery):\n",
        "    sel = q.select_col if q.agg==\"NONE\" else f\"{q.agg}({q.select_col})\"\n",
        "    sql = f\"SELECT {sel} FROM {q.table}\"\n",
        "    if q.join_table:\n",
        "        sql += f\" JOIN {q.join_table}\"\n",
        "    if q.op!=\"NONE\":\n",
        "        sql += f\" WHERE {q.where_col} {q.op} {q.value}\"\n",
        "    return sql\n",
        "\n",
        "# ============================================================\n",
        "# DATASET GENERATION (STRUCTURE LABELS)\n",
        "# ============================================================\n",
        "\n",
        "def generate_example():\n",
        "    db = random.choice(SCHEMAS)\n",
        "    schema = db[\"tables\"]\n",
        "    main = list(schema.keys())[0]\n",
        "    cols = schema[main]\n",
        "\n",
        "    intent = random.choices(\n",
        "        [\"SELECT\",\"AGG\",\"WHERE\",\"GROUP\",\"JOIN\"],\n",
        "        weights=[0.30,0.25,0.20,0.15,0.10]\n",
        "    )[0]\n",
        "\n",
        "    agg=\"NONE\"; where_col=\"NONE\"; op=\"NONE\"; val=\"NONE\"; join=\"NONE\"\n",
        "\n",
        "    if intent==\"SELECT\":\n",
        "        col=random.choice(cols)\n",
        "        q=f\"show {col} of {main}\"\n",
        "\n",
        "    elif intent==\"AGG\":\n",
        "        agg=random.choice(AGGS[1:])\n",
        "        col=random.choice(cols)\n",
        "        q=f\"get {agg.lower()} {col} from {main}\"\n",
        "\n",
        "    elif intent==\"WHERE\":\n",
        "        col=random.choice(cols)\n",
        "        val=str(random.choice([10,20,50,100]))\n",
        "        op=\">\"\n",
        "        q=f\"get {col} from {main} where {col} > {val}\"\n",
        "        where_col=col\n",
        "\n",
        "    elif intent==\"GROUP\":\n",
        "        agg=random.choice([\"COUNT\",\"AVG\"])\n",
        "        col=random.choice(cols)\n",
        "        q=f\"get {agg.lower()} {col} grouped by {col}\"\n",
        "\n",
        "    else:\n",
        "        t1,t2,c1,c2 = db[\"join\"]\n",
        "        col=\"name\"\n",
        "        join=t2\n",
        "        q=f\"show {t1} and {t2} names\"\n",
        "\n",
        "    return {\n",
        "        \"question\":q,\n",
        "        \"schema\":schema,\n",
        "        \"label\":{\n",
        "            \"table\":main,\n",
        "            \"select_col\":col,\n",
        "            \"agg\":agg,\n",
        "            \"where_col\":where_col,\n",
        "            \"op\":op,\n",
        "            \"value\":val,\n",
        "            \"join\":join\n",
        "        }\n",
        "    }\n",
        "\n",
        "DATA=[generate_example() for _ in range(50000)]\n",
        "\n",
        "# ============================================================\n",
        "# VOCAB + LABEL MAPS\n",
        "# ============================================================\n",
        "\n",
        "ENC_VOCAB={\"<PAD>\":0,\"<UNK>\":1,\"<SEP>\":2}\n",
        "def add(tok):\n",
        "    if tok not in ENC_VOCAB:\n",
        "        ENC_VOCAB[tok]=len(ENC_VOCAB)\n",
        "\n",
        "TABLES=set(); COLS=set()\n",
        "\n",
        "for d in DATA:\n",
        "    for t in d[\"question\"].lower().split(): add(t)\n",
        "    for t,cs in d[\"schema\"].items():\n",
        "        TABLES.add(t)\n",
        "        for c in cs:\n",
        "            COLS.add(c)\n",
        "            add(f\"{t}.{c}\")\n",
        "\n",
        "TABLES=list(TABLES)\n",
        "COLS=list(COLS)\n",
        "\n",
        "table2id={t:i for i,t in enumerate(TABLES)}\n",
        "col2id={c:i for i,c in enumerate(COLS)}\n",
        "agg2id={a:i for i,a in enumerate(AGGS)}\n",
        "op2id={o:i for i,o in enumerate(OPS)}\n",
        "\n",
        "# ============================================================\n",
        "# DATASET CLASS\n",
        "# ============================================================\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "    def __init__(self,data):\n",
        "        self.data=data\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        ex=self.data[i]\n",
        "        q=ex[\"question\"].lower().split()\n",
        "\n",
        "        schema_tokens=[f\"{t}.{c}\" for t,cs in ex[\"schema\"].items() for c in cs]\n",
        "        random.shuffle(schema_tokens)\n",
        "\n",
        "        src=q+[\"<SEP>\"]+schema_tokens\n",
        "        ids=[ENC_VOCAB.get(t,1) for t in src][:80]\n",
        "        ids+=[0]*(80-len(ids))\n",
        "\n",
        "        y=ex[\"label\"]\n",
        "        labels=(\n",
        "            table2id[y[\"table\"]],\n",
        "            col2id[y[\"select_col\"]],\n",
        "            agg2id[y[\"agg\"]],\n",
        "            col2id[y[\"where_col\"]] if y[\"where_col\"]!=\"NONE\" else 0,\n",
        "            op2id[y[\"op\"]],\n",
        "            table2id[y[\"join\"]] if y[\"join\"]!=\"NONE\" else 0\n",
        "        )\n",
        "        return torch.tensor(ids), torch.tensor(labels)\n",
        "\n",
        "train_data,val_data=train_test_split(DATA,test_size=0.15)\n",
        "train_loader=DataLoader(NL2SQLDataset(train_data),batch_size=128,shuffle=True)\n",
        "val_loader=DataLoader(NL2SQLDataset(val_data),batch_size=128)\n",
        "\n",
        "# ============================================================\n",
        "# TRANSFORMER STRUCTURE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class NL2SQLModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(len(ENC_VOCAB),192,padding_idx=0)\n",
        "        layer=nn.TransformerEncoderLayer(192,6,768,batch_first=True)\n",
        "        self.enc=nn.TransformerEncoder(layer,3)\n",
        "\n",
        "        self.table_head=nn.Linear(192,len(TABLES))\n",
        "        self.col_head=nn.Linear(192,len(COLS))\n",
        "        self.agg_head=nn.Linear(192,len(AGGS))\n",
        "        self.op_head=nn.Linear(192,len(OPS))\n",
        "        self.join_head=nn.Linear(192,len(TABLES))\n",
        "\n",
        "    def forward(self,x):\n",
        "        h=self.enc(self.emb(x))\n",
        "        p=h.mean(dim=1)\n",
        "        return {\n",
        "            \"table\":self.table_head(p),\n",
        "            \"col\":self.col_head(p),\n",
        "            \"agg\":self.agg_head(p),\n",
        "            \"op\":self.op_head(p),\n",
        "            \"join\":self.join_head(p)\n",
        "        }\n",
        "\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model=NL2SQLModel().to(device)\n",
        "\n",
        "opt=optim.Adam(model.parameters(),lr=3e-4)\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING\n",
        "# ============================================================\n",
        "\n",
        "print(\"ðŸš€ Training structure-based model\")\n",
        "\n",
        "for epoch in range(4):\n",
        "    model.train(); tot=0\n",
        "    for x,y in train_loader:\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        opt.zero_grad()\n",
        "        out=model(x)\n",
        "        loss=(\n",
        "            loss_fn(out[\"table\"],y[:,0])+\n",
        "            loss_fn(out[\"col\"],y[:,1])+\n",
        "            loss_fn(out[\"agg\"],y[:,2])+\n",
        "            loss_fn(out[\"op\"],y[:,4])+\n",
        "            loss_fn(out[\"join\"],y[:,5])\n",
        "        )\n",
        "        loss.backward(); opt.step()\n",
        "        tot+=loss.item()\n",
        "    print(f\"Epoch {epoch+1} | Loss {tot/len(train_loader):.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# INFERENCE\n",
        "# ============================================================\n",
        "\n",
        "def infer(question,schema):\n",
        "    tokens=question.lower().split()+[\"<SEP>\"]+[f\"{t}.{c}\" for t,cs in schema.items() for c in cs]\n",
        "    ids=[ENC_VOCAB.get(t,1) for t in tokens][:80]\n",
        "    ids+=[0]*(80-len(ids))\n",
        "    x=torch.tensor(ids).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out=model(x)\n",
        "        t=TABLES[out[\"table\"].argmax().item()]\n",
        "        c=COLS[out[\"col\"].argmax().item()]\n",
        "        agg=AGGS[out[\"agg\"].argmax().item()]\n",
        "        op=OPS[out[\"op\"].argmax().item()]\n",
        "        j=TABLES[out[\"join\"].argmax().item()]\n",
        "\n",
        "    q=SQLQuery(t,c,agg,c,op,\"50\",None if j==t else j)\n",
        "    return render_sql(q)\n",
        "\n",
        "# ============================================================\n",
        "# TEST\n",
        "# ============================================================\n",
        "\n",
        "schema1={\n",
        "    \"employees\":[\"id\",\"name\",\"salary\",\"dept_id\"],\n",
        "    \"departments\":[\"id\",\"name\"]\n",
        "}\n",
        "\n",
        "print(infer(\"show salary sum from employees\",schema1))\n",
        "print(infer(\"get salary from employees where salary > 20\",schema1))\n",
        "print(infer(\"show employees and departments names\",schema1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "gctZvsRe8YAq",
        "outputId": "fdf6efc1-1db6-4a70-ba2c-7c84a4d021a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training structure-based model\n",
            "Epoch 1 | Loss 0.3070\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3528806422.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"join\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         )\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mtot\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} | Loss {tot/len(train_loader):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# NL â†’ SQL STRUCTURE â†’ SQL (TRUE SEMANTIC PARSING)\n",
        "# TRANSFORMER BUILT FROM SCRATCH â€” DUPLICATE FREE â€” SINGLE CELL\n",
        "# ============================================================\n",
        "\n",
        "import random, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from dataclasses import dataclass\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============================================================\n",
        "# SCHEMA DEFINITIONS\n",
        "# ============================================================\n",
        "\n",
        "SCHEMAS = [\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"employees\": [\"id\",\"name\",\"salary\",\"dept_id\"],\n",
        "            \"departments\": [\"id\",\"name\"]\n",
        "        },\n",
        "        \"join\": (\"employees\",\"departments\",\"dept_id\",\"id\")\n",
        "    },\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"students\": [\"id\",\"name\",\"marks\",\"class_id\"],\n",
        "            \"classes\": [\"id\",\"name\"]\n",
        "        },\n",
        "        \"join\": (\"students\",\"classes\",\"class_id\",\"id\")\n",
        "    }\n",
        "]\n",
        "\n",
        "AGGS = [\"NONE\",\"COUNT\",\"SUM\",\"AVG\",\"MAX\",\"MIN\"]\n",
        "OPS  = [\"NONE\",\">\",\"<\",\"=\"]\n",
        "\n",
        "# ============================================================\n",
        "# SQL AST\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class SQLQuery:\n",
        "    table:str\n",
        "    select_col:str\n",
        "    agg:str\n",
        "    where_col:str\n",
        "    op:str\n",
        "    value:str\n",
        "    join_table:str\n",
        "\n",
        "def render_sql(q:SQLQuery):\n",
        "    sel = q.select_col if q.agg==\"NONE\" else f\"{q.agg}({q.select_col})\"\n",
        "    sql = f\"SELECT {sel} FROM {q.table}\"\n",
        "    if q.join_table!=\"NONE\":\n",
        "        sql += f\" JOIN {q.join_table}\"\n",
        "    if q.op!=\"NONE\":\n",
        "        sql += f\" WHERE {q.where_col} {q.op} {q.value}\"\n",
        "    return sql\n",
        "\n",
        "# ============================================================\n",
        "# DATASET GENERATION (DUPLICATE FREE)\n",
        "# ============================================================\n",
        "\n",
        "def generate_example():\n",
        "    db = random.choice(SCHEMAS)\n",
        "    schema = db[\"tables\"]\n",
        "    main = list(schema.keys())[0]\n",
        "    cols = schema[main]\n",
        "\n",
        "    intent = random.choices(\n",
        "        [\"SELECT\",\"AGG\",\"WHERE\",\"GROUP\",\"JOIN\"],\n",
        "        weights=[0.30,0.25,0.20,0.15,0.10]\n",
        "    )[0]\n",
        "\n",
        "    agg=\"NONE\"; where_col=\"NONE\"; op=\"NONE\"; val=\"NONE\"; join=\"NONE\"\n",
        "\n",
        "    if intent==\"SELECT\":\n",
        "        col=random.choice(cols)\n",
        "        q=f\"show {col} of {main}\"\n",
        "\n",
        "    elif intent==\"AGG\":\n",
        "        agg=random.choice(AGGS[1:])\n",
        "        col=random.choice(cols)\n",
        "        q=f\"get {agg.lower()} {col} from {main}\"\n",
        "\n",
        "    elif intent==\"WHERE\":\n",
        "        col=random.choice(cols)\n",
        "        val=str(random.choice([10,20,50,100]))\n",
        "        op=\">\"\n",
        "        q=f\"get {col} from {main} where {col} > {val}\"\n",
        "        where_col=col\n",
        "\n",
        "    elif intent==\"GROUP\":\n",
        "        agg=random.choice([\"COUNT\",\"AVG\"])\n",
        "        col=random.choice(cols)\n",
        "        q=f\"get {agg.lower()} {col} grouped by {col}\"\n",
        "\n",
        "    else:\n",
        "        t1,t2,c1,c2 = db[\"join\"]\n",
        "        col=\"name\"\n",
        "        join=t2\n",
        "        q=f\"show {t1} and {t2} names\"\n",
        "\n",
        "    return {\n",
        "        \"question\":q,\n",
        "        \"schema\":schema,\n",
        "        \"label\":{\n",
        "            \"table\":main,\n",
        "            \"select_col\":col,\n",
        "            \"agg\":agg,\n",
        "            \"where_col\":where_col,\n",
        "            \"op\":op,\n",
        "            \"value\":val,\n",
        "            \"join\":join\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ----------- DUPLICATE FREE GENERATION -----------\n",
        "\n",
        "TARGET_SIZE = 60000\n",
        "DATA = []\n",
        "seen_questions = set()\n",
        "seen_semantics = set()\n",
        "\n",
        "while len(DATA) < TARGET_SIZE:\n",
        "    ex = generate_example()\n",
        "\n",
        "    q_key = ex[\"question\"]\n",
        "    sem_key = (\n",
        "        ex[\"label\"][\"table\"],\n",
        "        ex[\"label\"][\"select_col\"],\n",
        "        ex[\"label\"][\"agg\"],\n",
        "        ex[\"label\"][\"where_col\"],\n",
        "        ex[\"label\"][\"op\"],\n",
        "        ex[\"label\"][\"value\"],\n",
        "        ex[\"label\"][\"join\"]\n",
        "    )\n",
        "\n",
        "    if q_key in seen_questions: continue\n",
        "    if sem_key in seen_semantics: continue\n",
        "\n",
        "    seen_questions.add(q_key)\n",
        "    seen_semantics.add(sem_key)\n",
        "    DATA.append(ex)\n",
        "\n",
        "print(f\"âœ… Dataset built with {len(DATA)} unique examples\")\n",
        "\n",
        "# ============================================================\n",
        "# VOCAB + LABEL MAPS\n",
        "# ============================================================\n",
        "\n",
        "ENC_VOCAB={\"<PAD>\":0,\"<UNK>\":1,\"<SEP>\":2}\n",
        "\n",
        "def add(tok):\n",
        "    if tok not in ENC_VOCAB:\n",
        "        ENC_VOCAB[tok]=len(ENC_VOCAB)\n",
        "\n",
        "TABLES=set(); COLS=set()\n",
        "\n",
        "for d in DATA:\n",
        "    for t in d[\"question\"].lower().split(): add(t)\n",
        "    for t,cs in d[\"schema\"].items():\n",
        "        TABLES.add(t)\n",
        "        for c in cs:\n",
        "            COLS.add(c)\n",
        "            add(f\"{t}.{c}\")\n",
        "\n",
        "TABLES=list(TABLES)\n",
        "COLS=list(COLS)\n",
        "\n",
        "table2id={t:i for i,t in enumerate(TABLES)}\n",
        "col2id={c:i for i,c in enumerate(COLS)}\n",
        "agg2id={a:i for i,a in enumerate(AGGS)}\n",
        "op2id={o:i for i,o in enumerate(OPS)}\n",
        "\n",
        "# ============================================================\n",
        "# DATASET CLASS\n",
        "# ============================================================\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "    def __init__(self,data):\n",
        "        self.data=data\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        ex=self.data[i]\n",
        "        q=ex[\"question\"].lower().split()\n",
        "\n",
        "        schema_tokens=[f\"{t}.{c}\" for t,cs in ex[\"schema\"].items() for c in cs]\n",
        "        random.shuffle(schema_tokens)\n",
        "\n",
        "        src=q+[\"<SEP>\"]+schema_tokens\n",
        "        ids=[ENC_VOCAB.get(t,1) for t in src][:80]\n",
        "        ids+=[0]*(80-len(ids))\n",
        "\n",
        "        y=ex[\"label\"]\n",
        "        labels=(\n",
        "            table2id[y[\"table\"]],\n",
        "            col2id[y[\"select_col\"]],\n",
        "            agg2id[y[\"agg\"]],\n",
        "            col2id[y[\"where_col\"]] if y[\"where_col\"]!=\"NONE\" else 0,\n",
        "            op2id[y[\"op\"]],\n",
        "            table2id[y[\"join\"]] if y[\"join\"]!=\"NONE\" else 0\n",
        "        )\n",
        "\n",
        "        return torch.tensor(ids), torch.tensor(labels)\n",
        "\n",
        "train_data,val_data=train_test_split(DATA,test_size=0.15)\n",
        "train_loader=DataLoader(NL2SQLDataset(train_data),batch_size=128,shuffle=True)\n",
        "val_loader=DataLoader(NL2SQLDataset(val_data),batch_size=128)\n",
        "\n",
        "# ============================================================\n",
        "# TRANSFORMER STRUCTURE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class NL2SQLModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(len(ENC_VOCAB),192,padding_idx=0)\n",
        "        layer=nn.TransformerEncoderLayer(192,6,768,batch_first=True)\n",
        "        self.enc=nn.TransformerEncoder(layer,3)\n",
        "\n",
        "        self.table_head=nn.Linear(192,len(TABLES))\n",
        "        self.col_head=nn.Linear(192,len(COLS))\n",
        "        self.agg_head=nn.Linear(192,len(AGGS))\n",
        "        self.op_head=nn.Linear(192,len(OPS))\n",
        "        self.join_head=nn.Linear(192,len(TABLES))\n",
        "\n",
        "    def forward(self,x):\n",
        "        h=self.enc(self.emb(x))\n",
        "        p=h.mean(dim=1)\n",
        "        return {\n",
        "            \"table\":self.table_head(p),\n",
        "            \"col\":self.col_head(p),\n",
        "            \"agg\":self.agg_head(p),\n",
        "            \"op\":self.op_head(p),\n",
        "            \"join\":self.join_head(p)\n",
        "        }\n",
        "\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model=NL2SQLModel().to(device)\n",
        "\n",
        "opt=optim.Adam(model.parameters(),lr=3e-4)\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING\n",
        "# ============================================================\n",
        "\n",
        "print(\"ðŸš€ Training structure-based model\")\n",
        "\n",
        "for epoch in range(4):\n",
        "    model.train(); tot=0\n",
        "    for x,y in train_loader:\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        opt.zero_grad()\n",
        "        out=model(x)\n",
        "        loss=(\n",
        "            loss_fn(out[\"table\"],y[:,0])+\n",
        "            loss_fn(out[\"col\"],y[:,1])+\n",
        "            loss_fn(out[\"agg\"],y[:,2])+\n",
        "            loss_fn(out[\"op\"],y[:,4])+\n",
        "            loss_fn(out[\"join\"],y[:,5])\n",
        "        )\n",
        "        loss.backward(); opt.step()\n",
        "        tot+=loss.item()\n",
        "    print(f\"Epoch {epoch+1} | Loss {tot/len(train_loader):.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# INFERENCE\n",
        "# ============================================================\n",
        "\n",
        "def infer(question,schema):\n",
        "    tokens=question.lower().split()+[\"<SEP>\"]+[f\"{t}.{c}\" for t,cs in schema.items() for c in cs]\n",
        "    ids=[ENC_VOCAB.get(t,1) for t in tokens][:80]\n",
        "    ids+=[0]*(80-len(ids))\n",
        "    x=torch.tensor(ids).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out=model(x)\n",
        "        t=TABLES[out[\"table\"].argmax().item()]\n",
        "        c=COLS[out[\"col\"].argmax().item()]\n",
        "        agg=AGGS[out[\"agg\"].argmax().item()]\n",
        "        op=OPS[out[\"op\"].argmax().item()]\n",
        "        j=TABLES[out[\"join\"].argmax().item()]\n",
        "\n",
        "    q=SQLQuery(t,c,agg,c,op,\"50\",j)\n",
        "    return render_sql(q)\n",
        "\n",
        "# ============================================================\n",
        "# TEST\n",
        "# ============================================================\n",
        "\n",
        "schema1={\n",
        "    \"employees\":[\"id\",\"name\",\"salary\",\"dept_id\"],\n",
        "    \"departments\":[\"id\",\"name\"]\n",
        "}\n",
        "\n",
        "print(infer(\"show salary sum from employees\",schema1))\n",
        "print(infer(\"get salary from employees where salary > 20\",schema1))\n",
        "print(infer(\"show employees and departments names\",schema1))\n"
      ],
      "metadata": {
        "id": "SA1bPtTRBl2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}