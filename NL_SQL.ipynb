{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhwaN63aNinw78JU5wpBz7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/NL_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data\n",
        "!cd data\n",
        "\n",
        "!wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2\n",
        "\n",
        "!tar -xvf data.tar.bz2\n",
        "\n",
        "!ls\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I58gZrPg5sCm",
        "outputId": "0e27c7bf-1e7f-45a6-9ff9-6226be5a1b4c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-27 10:01:46--  https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/salesforce/WikiSQL/master/data.tar.bz2 [following]\n",
            "--2026-01-27 10:01:46--  https://raw.githubusercontent.com/salesforce/WikiSQL/master/data.tar.bz2\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26164664 (25M) [application/octet-stream]\n",
            "Saving to: ‘data.tar.bz2’\n",
            "\n",
            "data.tar.bz2        100%[===================>]  24.95M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2026-01-27 10:01:47 (304 MB/s) - ‘data.tar.bz2’ saved [26164664/26164664]\n",
            "\n",
            "data/\n",
            "data/train.jsonl\n",
            "data/test.tables.jsonl\n",
            "data/test.db\n",
            "data/dev.tables.jsonl\n",
            "data/dev.db\n",
            "data/test.jsonl\n",
            "data/train.tables.jsonl\n",
            "data/train.db\n",
            "data/dev.jsonl\n",
            "data  data.tar.bz2  sample_data  Spider_dataset.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Spider_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcEumuCN5sGR",
        "outputId": "be11e83b-d6d7-41f7-efc0-20fa2393da17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Spider_dataset.zip\n",
            "  inflating: spider/README.txt       \n",
            "  inflating: spider/database/academic/academic.sqlite  \n",
            "  inflating: spider/database/academic/schema.sql  \n",
            "  inflating: spider/database/activity_1/activity_1.sqlite  \n",
            "  inflating: spider/database/activity_1/schema.sql  \n",
            "  inflating: spider/database/aircraft/aircraft.sqlite  \n",
            "  inflating: spider/database/aircraft/schema.sql  \n",
            "  inflating: spider/database/allergy_1/allergy_1.sqlite  \n",
            "  inflating: spider/database/allergy_1/schema.sql  \n",
            "  inflating: spider/database/apartment_rentals/apartment_rentals.sqlite  \n",
            "  inflating: spider/database/apartment_rentals/schema.sql  \n",
            "  inflating: spider/database/architecture/architecture.sqlite  \n",
            "  inflating: spider/database/architecture/schema.sql  \n",
            "  inflating: spider/database/assets_maintenance/assets_maintenance.sqlite  \n",
            "  inflating: spider/database/assets_maintenance/schema.sql  \n",
            "  inflating: spider/database/baseball_1/baseball_1.sqlite  \n",
            "  inflating: spider/database/baseball_1/schema.sql  \n",
            "  inflating: spider/database/battle_death/battle_death.sqlite  \n",
            "  inflating: spider/database/battle_death/schema.sql  \n",
            "  inflating: spider/database/behavior_monitoring/behavior_monitoring.sqlite  \n",
            "  inflating: spider/database/behavior_monitoring/schema.sql  \n",
            "  inflating: spider/database/bike_1/bike_1.sqlite  \n",
            "  inflating: spider/database/bike_1/schema.sql  \n",
            "  inflating: spider/database/body_builder/body_builder.sqlite  \n",
            "  inflating: spider/database/body_builder/schema.sql  \n",
            "  inflating: spider/database/book_2/book_2.sqlite  \n",
            "  inflating: spider/database/book_2/schema.sql  \n",
            "  inflating: spider/database/browser_web/browser_web.sqlite  \n",
            "  inflating: spider/database/browser_web/schema.sql  \n",
            "  inflating: spider/database/candidate_poll/candidate_poll.sqlite  \n",
            "  inflating: spider/database/candidate_poll/schema.sql  \n",
            "  inflating: spider/database/car_1/annotation.json  \n",
            "  inflating: spider/database/car_1/car_1.json  \n",
            "  inflating: spider/database/car_1/car_1.sql  \n",
            "  inflating: spider/database/car_1/car_1.sqlite  \n",
            "  inflating: spider/database/car_1/data_csv/README.CARS.TXT  \n",
            "  inflating: spider/database/car_1/data_csv/car-makers.csv  \n",
            "  inflating: spider/database/car_1/data_csv/car-names.csv  \n",
            "  inflating: spider/database/car_1/data_csv/cars-data.csv  \n",
            "  inflating: spider/database/car_1/data_csv/cars.desc  \n",
            "  inflating: spider/database/car_1/data_csv/continents.csv  \n",
            "  inflating: spider/database/car_1/data_csv/countries.csv  \n",
            "  inflating: spider/database/car_1/data_csv/model-list.csv  \n",
            "  inflating: spider/database/car_1/link.txt  \n",
            "  inflating: spider/database/car_1/q.txt  \n",
            "  inflating: spider/database/chinook_1/annotation.json  \n",
            "  inflating: spider/database/chinook_1/chinook_1.sqlite  \n",
            "  inflating: spider/database/cinema/cinema.sqlite  \n",
            "  inflating: spider/database/cinema/schema.sql  \n",
            "  inflating: spider/database/city_record/city_record.sqlite  \n",
            "  inflating: spider/database/city_record/schema.sql  \n",
            "  inflating: spider/database/climbing/climbing.sqlite  \n",
            "  inflating: spider/database/climbing/schema.sql  \n",
            "  inflating: spider/database/club_1/club_1.sqlite  \n",
            "  inflating: spider/database/club_1/schema.sql  \n",
            "  inflating: spider/database/coffee_shop/coffee_shop.sqlite  \n",
            "  inflating: spider/database/coffee_shop/schema.sql  \n",
            "  inflating: spider/database/college_1/TinyCollege.sql  \n",
            "  inflating: spider/database/college_1/college_1.sqlite  \n",
            "  inflating: spider/database/college_1/link.txt  \n",
            "  inflating: spider/database/college_2/TextBookExampleSchema.sql  \n",
            "  inflating: spider/database/college_2/college_2.sqlite  \n",
            "  inflating: spider/database/college_2/link.txt  \n",
            "  inflating: spider/database/college_3/college_3.sqlite  \n",
            "  inflating: spider/database/college_3/schema.sql  \n",
            "  inflating: spider/database/company_1/company_1.sqlite  \n",
            "  inflating: spider/database/company_1/link.txt  \n",
            "  inflating: spider/database/company_employee/company_employee.sqlite  \n",
            "  inflating: spider/database/company_employee/schema.sql  \n",
            "  inflating: spider/database/company_office/company_office.sqlite  \n",
            "  inflating: spider/database/company_office/schema.sql  \n",
            "  inflating: spider/database/concert_singer/concert_singer.sqlite  \n",
            "  inflating: spider/database/concert_singer/schema.sql  \n",
            "  inflating: spider/database/county_public_safety/county_public_safety.sqlite  \n",
            "  inflating: spider/database/county_public_safety/schema.sql  \n",
            "  inflating: spider/database/course_teach/course_teach.sqlite  \n",
            "  inflating: spider/database/course_teach/schema.sql  \n",
            "  inflating: spider/database/cre_Doc_Control_Systems/cre_Doc_Control_Systems.sqlite  \n",
            "  inflating: spider/database/cre_Doc_Control_Systems/schema.sql  \n",
            "  inflating: spider/database/cre_Doc_Template_Mgt/cre_Doc_Template_Mgt.sqlite  \n",
            "  inflating: spider/database/cre_Doc_Template_Mgt/schema.sql  \n",
            "  inflating: spider/database/cre_Doc_Tracking_DB/cre_Doc_Tracking_DB.sqlite  \n",
            "  inflating: spider/database/cre_Doc_Tracking_DB/schema.sql  \n",
            "  inflating: spider/database/cre_Docs_and_Epenses/cre_Docs_and_Epenses.sqlite  \n",
            "  inflating: spider/database/cre_Docs_and_Epenses/schema.sql  \n",
            "  inflating: spider/database/cre_Drama_Workshop_Groups/cre_Drama_Workshop_Groups.sqlite  \n",
            "  inflating: spider/database/cre_Drama_Workshop_Groups/schema.sql  \n",
            "  inflating: spider/database/cre_Theme_park/cre_Theme_park.sqlite  \n",
            "  inflating: spider/database/cre_Theme_park/schema.sql  \n",
            "  inflating: spider/database/csu_1/csu_1.sqlite  \n",
            "  inflating: spider/database/csu_1/schema.sql  \n",
            "  inflating: spider/database/culture_company/culture_company.sqlite  \n",
            "  inflating: spider/database/culture_company/schema.sql  \n",
            "  inflating: spider/database/customer_complaints/customer_complaints.sqlite  \n",
            "  inflating: spider/database/customer_complaints/schema.sql  \n",
            "  inflating: spider/database/customer_deliveries/customer_deliveries.sqlite  \n",
            "  inflating: spider/database/customer_deliveries/schema.sql  \n",
            "  inflating: spider/database/customers_and_addresses/customers_and_addresses.sqlite  \n",
            "  inflating: spider/database/customers_and_addresses/schema.sql  \n",
            "  inflating: spider/database/customers_and_invoices/customers_and_invoices.sqlite  \n",
            "  inflating: spider/database/customers_and_invoices/schema.sql  \n",
            "  inflating: spider/database/customers_and_products_contacts/customers_and_products_contacts.sqlite  \n",
            "  inflating: spider/database/customers_and_products_contacts/schema.sql  \n",
            "  inflating: spider/database/customers_campaigns_ecommerce/customers_campaigns_ecommerce.sqlite  \n",
            "  inflating: spider/database/customers_campaigns_ecommerce/schema.sql  \n",
            "  inflating: spider/database/customers_card_transactions/customers_card_transactions.sqlite  \n",
            "  inflating: spider/database/customers_card_transactions/schema.sql  \n",
            "  inflating: spider/database/debate/debate.sqlite  \n",
            "  inflating: spider/database/debate/schema.sql  \n",
            "  inflating: spider/database/decoration_competition/decoration_competition.sqlite  \n",
            "  inflating: spider/database/decoration_competition/schema.sql  \n",
            "  inflating: spider/database/department_management/department_management.sqlite  \n",
            "  inflating: spider/database/department_management/schema.sql  \n",
            "  inflating: spider/database/department_store/department_store.sqlite  \n",
            "  inflating: spider/database/department_store/schema.sql  \n",
            "  inflating: spider/database/device/device.sqlite  \n",
            "  inflating: spider/database/device/schema.sql  \n",
            "  inflating: spider/database/document_management/document_management.sqlite  \n",
            "  inflating: spider/database/document_management/schema.sql  \n",
            "  inflating: spider/database/dog_kennels/dog_kennels.sqlite  \n",
            "  inflating: spider/database/dog_kennels/schema.sql  \n",
            "  inflating: spider/database/dorm_1/dorm_1.sqlite  \n",
            "  inflating: spider/database/dorm_1/schema.sql  \n",
            "  inflating: spider/database/driving_school/driving_school.sqlite  \n",
            "  inflating: spider/database/driving_school/schema.sql  \n",
            "  inflating: spider/database/e_government/e_government.sqlite  \n",
            "  inflating: spider/database/e_government/schema.sql  \n",
            "  inflating: spider/database/e_learning/e_learning.sqlite  \n",
            "  inflating: spider/database/e_learning/schema.sql  \n",
            "  inflating: spider/database/election/election.sqlite  \n",
            "  inflating: spider/database/election/schema.sql  \n",
            "  inflating: spider/database/election_representative/election_representative.sqlite  \n",
            "  inflating: spider/database/election_representative/schema.sql  \n",
            "  inflating: spider/database/employee_hire_evaluation/employee_hire_evaluation.sqlite  \n",
            "  inflating: spider/database/employee_hire_evaluation/schema.sql  \n",
            "  inflating: spider/database/entertainment_awards/entertainment_awards.sqlite  \n",
            "  inflating: spider/database/entertainment_awards/schema.sql  \n",
            "  inflating: spider/database/entrepreneur/entrepreneur.sqlite  \n",
            "  inflating: spider/database/entrepreneur/schema.sql  \n",
            "  inflating: spider/database/epinions_1/epinions_1.sqlite  \n",
            "  inflating: spider/database/farm/farm.sqlite  \n",
            "  inflating: spider/database/farm/schema.sql  \n",
            "  inflating: spider/database/film_rank/film_rank.sqlite  \n",
            "  inflating: spider/database/film_rank/schema.sql  \n",
            "  inflating: spider/database/flight_1/flight_1.sqlite  \n",
            "  inflating: spider/database/flight_1/schema.sql  \n",
            "  inflating: spider/database/flight_2/annotation.json  \n",
            "  inflating: spider/database/flight_2/data_csv/README.AIRLINES.txt  \n",
            "  inflating: spider/database/flight_2/data_csv/airlines.csv  \n",
            "  inflating: spider/database/flight_2/data_csv/airports100.csv  \n",
            "  inflating: spider/database/flight_2/data_csv/flights.csv  \n",
            "  inflating: spider/database/flight_2/flight_2.json  \n",
            "  inflating: spider/database/flight_2/flight_2.sql  \n",
            "  inflating: spider/database/flight_2/flight_2.sqlite  \n",
            "  inflating: spider/database/flight_2/link.txt  \n",
            "  inflating: spider/database/flight_2/q.txt  \n",
            "  inflating: spider/database/flight_4/flight_4.sqlite  \n",
            "  inflating: spider/database/flight_4/link.txt  \n",
            "  inflating: spider/database/flight_4/sql.txt  \n",
            "  inflating: spider/database/flight_company/flight_company.sqlite  \n",
            "  inflating: spider/database/flight_company/schema.sql  \n",
            "  inflating: spider/database/formula_1/annotation.json  \n",
            "  inflating: spider/database/formula_1/data_csv/circuits.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/constructorResults.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/constructorStandings.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/constructors.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/driverStandings.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/drivers.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/lapTimes.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/pitStops.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/qualifying.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/races.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/results.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/seasons.csv  \n",
            "  inflating: spider/database/formula_1/data_csv/status.csv  \n",
            "  inflating: spider/database/formula_1/formula_1.splite  \n",
            "  inflating: spider/database/formula_1/formula_1.sql  \n",
            "  inflating: spider/database/formula_1/formula_1.sqlite  \n",
            "  inflating: spider/database/game_1/game_1.sqlite  \n",
            "  inflating: spider/database/game_1/schema.sql  \n",
            "  inflating: spider/database/game_injury/game_injury.sqlite  \n",
            "  inflating: spider/database/game_injury/schema.sql  \n",
            "  inflating: spider/database/gas_company/gas_company.sqlite  \n",
            "  inflating: spider/database/gas_company/schema.sql  \n",
            "  inflating: spider/database/geo/geo.sqlite  \n",
            "  inflating: spider/database/geo/schema.sql  \n",
            "  inflating: spider/database/gymnast/gymnast.sqlite  \n",
            "  inflating: spider/database/gymnast/schema.sql  \n",
            "  inflating: spider/database/hospital_1/hospital_1.sqlite  \n",
            "  inflating: spider/database/hospital_1/schema.sql  \n",
            "  inflating: spider/database/hr_1/hr_1.sqlite  \n",
            "  inflating: spider/database/hr_1/schema.sql  \n",
            "  inflating: spider/database/icfp_1/icfp_1.sqlite  \n",
            "  inflating: spider/database/icfp_1/link.txt  \n",
            "  inflating: spider/database/icfp_1/q.txt  \n",
            "  inflating: spider/database/imdb/imdb.sqlite  \n",
            "  inflating: spider/database/imdb/schema.sql  \n",
            "  inflating: spider/database/inn_1/annotation.json  \n",
            "  inflating: spider/database/inn_1/change_date.py  \n",
            "  inflating: spider/database/inn_1/data_csv/README.INN.TXT  \n",
            "  inflating: spider/database/inn_1/data_csv/Reservations.csv  \n",
            "  inflating: spider/database/inn_1/data_csv/Reservations_t.csv  \n",
            "  inflating: spider/database/inn_1/data_csv/Rooms.csv  \n",
            "  inflating: spider/database/inn_1/inn_1.sql  \n",
            "  inflating: spider/database/inn_1/inn_1.sqlite  \n",
            "  inflating: spider/database/inn_1/link.txt  \n",
            "  inflating: spider/database/inn_1/q.txt  \n",
            "  inflating: spider/database/insurance_and_eClaims/insurance_and_eClaims.sqlite  \n",
            "  inflating: spider/database/insurance_and_eClaims/schema.sql  \n",
            "  inflating: spider/database/insurance_fnol/insurance_fnol.sqlite  \n",
            "  inflating: spider/database/insurance_fnol/schema.sql  \n",
            "  inflating: spider/database/insurance_policies/insurance_policies.sqlite  \n",
            "  inflating: spider/database/insurance_policies/schema.sql  \n",
            "  inflating: spider/database/journal_committee/journal_committee.sqlite  \n",
            "  inflating: spider/database/journal_committee/schema.sql  \n",
            "  inflating: spider/database/loan_1/loan_1.sqlite  \n",
            "  inflating: spider/database/loan_1/schema.sql  \n",
            "  inflating: spider/database/local_govt_and_lot/local_govt_and_lot.sqlite  \n",
            "  inflating: spider/database/local_govt_and_lot/schema.sql  \n",
            "  inflating: spider/database/local_govt_in_alabama/local_govt_in_alabama.sqlite  \n",
            "  inflating: spider/database/local_govt_in_alabama/schema.sql  \n",
            "  inflating: spider/database/local_govt_mdm/local_govt_mdm.sqlite  \n",
            "  inflating: spider/database/local_govt_mdm/schema.sql  \n",
            "  inflating: spider/database/machine_repair/machine_repair.sqlite  \n",
            "  inflating: spider/database/machine_repair/schema.sql  \n",
            "  inflating: spider/database/manufactory_1/manufactory_1.sqlite  \n",
            "  inflating: spider/database/manufactory_1/schema.sql  \n",
            "  inflating: spider/database/manufacturer/manufacturer.sqlite  \n",
            "  inflating: spider/database/manufacturer/schema.sql  \n",
            "  inflating: spider/database/match_season/match_season.sqlite  \n",
            "  inflating: spider/database/match_season/schema.sql  \n",
            "  inflating: spider/database/medicine_enzyme_interaction/medicine_enzyme_interaction.sqlite  \n",
            "  inflating: spider/database/medicine_enzyme_interaction/schema.sql  \n",
            "  inflating: spider/database/mountain_photos/mountain_photos.sqlite  \n",
            "  inflating: spider/database/mountain_photos/schema.sql  \n",
            "  inflating: spider/database/movie_1/movie_1.sqlite  \n",
            "  inflating: spider/database/movie_1/schema.sql  \n",
            "  inflating: spider/database/museum_visit/museum_visit.sqlite  \n",
            "  inflating: spider/database/museum_visit/schema.sql  \n",
            "  inflating: spider/database/music_1/music_1.sqlite  \n",
            "  inflating: spider/database/music_1/schema.sql  \n",
            "  inflating: spider/database/music_2/music_2.sqlite  \n",
            "  inflating: spider/database/music_2/schema.sql  \n",
            "  inflating: spider/database/music_4/music_4.sqlite  \n",
            "  inflating: spider/database/music_4/schema.sql  \n",
            "  inflating: spider/database/musical/musical.sqlite  \n",
            "  inflating: spider/database/musical/schema.sql  \n",
            "  inflating: spider/database/network_1/network_1.sqlite  \n",
            "  inflating: spider/database/network_1/schema.sql  \n",
            "  inflating: spider/database/network_2/network_2.sqlite  \n",
            "  inflating: spider/database/network_2/schema.sql  \n",
            "  inflating: spider/database/news_report/news_report.sqlite  \n",
            "  inflating: spider/database/news_report/schema.sql  \n",
            "  inflating: spider/database/orchestra/orchestra.sqlite  \n",
            "  inflating: spider/database/orchestra/schema.sql  \n",
            "  inflating: spider/database/party_host/party_host.sqlite  \n",
            "  inflating: spider/database/party_host/schema.sql  \n",
            "  inflating: spider/database/party_people/party_people.sqlite  \n",
            "  inflating: spider/database/party_people/schema.sql  \n",
            "  inflating: spider/database/performance_attendance/performance_attendance.sqlite  \n",
            "  inflating: spider/database/performance_attendance/schema.sql  \n",
            "  inflating: spider/database/perpetrator/perpetrator.sqlite  \n",
            "  inflating: spider/database/perpetrator/schema.sql  \n",
            "  inflating: spider/database/pets_1/pets_1.sqlite  \n",
            "  inflating: spider/database/pets_1/schema.sql  \n",
            "  inflating: spider/database/phone_1/phone_1.sqlite  \n",
            "  inflating: spider/database/phone_1/schema.sql  \n",
            "  inflating: spider/database/phone_market/phone_market.sqlite  \n",
            "  inflating: spider/database/phone_market/schema.sql  \n",
            "  inflating: spider/database/pilot_record/pilot_record.sqlite  \n",
            "  inflating: spider/database/pilot_record/schema.sql  \n",
            "  inflating: spider/database/poker_player/poker_player.sqlite  \n",
            "  inflating: spider/database/poker_player/schema.sql  \n",
            "  inflating: spider/database/product_catalog/product_catalog.sqlite  \n",
            "  inflating: spider/database/product_catalog/schema.sql  \n",
            "  inflating: spider/database/products_for_hire/products_for_hire.sqlite  \n",
            "  inflating: spider/database/products_for_hire/schema.sql  \n",
            "  inflating: spider/database/products_gen_characteristics/products_gen_characteristics.sqlite  \n",
            "  inflating: spider/database/products_gen_characteristics/schema.sql  \n",
            "  inflating: spider/database/program_share/program_share.sqlite  \n",
            "  inflating: spider/database/program_share/schema.sql  \n",
            "  inflating: spider/database/protein_institute/protein_institute.sqlite  \n",
            "  inflating: spider/database/protein_institute/schema.sql  \n",
            "  inflating: spider/database/race_track/race_track.sqlite  \n",
            "  inflating: spider/database/race_track/schema.sql  \n",
            "  inflating: spider/database/railway/railway.sqlite  \n",
            "  inflating: spider/database/railway/schema.sql  \n",
            "  inflating: spider/database/real_estate_properties/real_estate_properties.sqlite  \n",
            "  inflating: spider/database/real_estate_properties/schema.sql  \n",
            "  inflating: spider/database/restaurant_1/restaurant_1.sqlite  \n",
            "  inflating: spider/database/restaurant_1/schema.sql  \n",
            "  inflating: spider/database/restaurants/restaurants.sqlite  \n",
            "  inflating: spider/database/restaurants/schema.sql  \n",
            "  inflating: spider/database/riding_club/riding_club.sqlite  \n",
            "  inflating: spider/database/riding_club/schema.sql  \n",
            "  inflating: spider/database/roller_coaster/roller_coaster.sqlite  \n",
            "  inflating: spider/database/roller_coaster/schema.sql  \n",
            "  inflating: spider/database/sakila_1/sakila_1.sqlite  \n",
            "  inflating: spider/database/sakila_1/schema.sql  \n",
            "  inflating: spider/database/scholar/schema.sql  \n",
            "  inflating: spider/database/scholar/scholar.sqlite  \n",
            "  inflating: spider/database/school_bus/schema.sql  \n",
            "  inflating: spider/database/school_bus/school_bus.sqlite  \n",
            "  inflating: spider/database/school_finance/schema.sql  \n",
            "  inflating: spider/database/school_finance/school_finance.sqlite  \n",
            "  inflating: spider/database/school_player/schema.sql  \n",
            "  inflating: spider/database/school_player/school_player.sqlite  \n",
            "  inflating: spider/database/scientist_1/schema.sql  \n",
            "  inflating: spider/database/scientist_1/scientist_1.sqlite  \n",
            "  inflating: spider/database/ship_1/schema.sql  \n",
            "  inflating: spider/database/ship_1/ship_1.sqlite  \n",
            "  inflating: spider/database/ship_mission/schema.sql  \n",
            "  inflating: spider/database/ship_mission/ship_mission.sqlite  \n",
            "  inflating: spider/database/shop_membership/schema.sql  \n",
            "  inflating: spider/database/shop_membership/shop_membership.sqlite  \n",
            "  inflating: spider/database/singer/schema.sql  \n",
            "  inflating: spider/database/singer/singer.sqlite  \n",
            "  inflating: spider/database/small_bank_1/small_bank_1.sqlite  \n",
            "  inflating: spider/database/soccer_1/schema.sql  \n",
            "  inflating: spider/database/soccer_1/soccer_1.sqlite  \n",
            "  inflating: spider/database/soccer_2/schema.sql  \n",
            "  inflating: spider/database/soccer_2/soccer_2.sqlite  \n",
            "  inflating: spider/database/solvency_ii/schema.sql  \n",
            "  inflating: spider/database/solvency_ii/solvency_ii.sqlite  \n",
            "  inflating: spider/database/sports_competition/schema.sql  \n",
            "  inflating: spider/database/sports_competition/sports_competition.sqlite  \n",
            "  inflating: spider/database/station_weather/schema.sql  \n",
            "  inflating: spider/database/station_weather/station_weather.sqlite  \n",
            "  inflating: spider/database/store_1/schema.sql  \n",
            "  inflating: spider/database/store_1/store_1.sqlite  \n",
            "  inflating: spider/database/store_product/schema.sql  \n",
            "  inflating: spider/database/store_product/store_product.sqlite  \n",
            "  inflating: spider/database/storm_record/schema.sql  \n",
            "  inflating: spider/database/storm_record/storm_record.sqlite  \n",
            "  inflating: spider/database/student_1/annotation.json  \n",
            "  inflating: spider/database/student_1/data_csv/README.STUDENTS.TXT  \n",
            "  inflating: spider/database/student_1/data_csv/list.csv  \n",
            "  inflating: spider/database/student_1/data_csv/teachers.csv  \n",
            "  inflating: spider/database/student_1/link.txt  \n",
            "  inflating: spider/database/student_1/q.txt  \n",
            "  inflating: spider/database/student_1/student_1.sql  \n",
            "  inflating: spider/database/student_1/student_1.sqlite  \n",
            "  inflating: spider/database/student_assessment/schema.sql  \n",
            "  inflating: spider/database/student_assessment/student_assessment.sqlite  \n",
            "  inflating: spider/database/student_transcripts_tracking/schema.sql  \n",
            "  inflating: spider/database/student_transcripts_tracking/student_transcripts_tracking.sqlite  \n",
            "  inflating: spider/database/swimming/schema.sql  \n",
            "  inflating: spider/database/swimming/swimming.sqlite  \n",
            "  inflating: spider/database/theme_gallery/schema.sql  \n",
            "  inflating: spider/database/theme_gallery/theme_gallery.sqlite  \n",
            "  inflating: spider/database/tracking_grants_for_research/schema.sql  \n",
            "  inflating: spider/database/tracking_grants_for_research/tracking_grants_for_research.sqlite  \n",
            "  inflating: spider/database/tracking_orders/schema.sql  \n",
            "  inflating: spider/database/tracking_orders/tracking_orders.sqlite  \n",
            "  inflating: spider/database/tracking_share_transactions/schema.sql  \n",
            "  inflating: spider/database/tracking_share_transactions/tracking_share_transactions.sqlite  \n",
            "  inflating: spider/database/tracking_software_problems/schema.sql  \n",
            "  inflating: spider/database/tracking_software_problems/tracking_software_problems.sqlite  \n",
            "  inflating: spider/database/train_station/schema.sql  \n",
            "  inflating: spider/database/train_station/train_station.sqlite  \n",
            "  inflating: spider/database/tvshow/schema.sql  \n",
            "  inflating: spider/database/tvshow/tvshow.sqlite  \n",
            "  inflating: spider/database/twitter_1/queries/oracle-dialects.xml  \n",
            "  inflating: spider/database/twitter_1/queries/postgres-dialects.xml  \n",
            "  inflating: spider/database/twitter_1/queries/sqlserver-dialects.xml  \n",
            "  inflating: spider/database/twitter_1/twitter_1.sqlite  \n",
            "  inflating: spider/database/university_basketball/schema.sql  \n",
            "  inflating: spider/database/university_basketball/university_basketball.sqlite  \n",
            "  inflating: spider/database/voter_1/voter_1.sqlite  \n",
            "  inflating: spider/database/voter_2/schema.sql  \n",
            "  inflating: spider/database/voter_2/voter_2.sqlite  \n",
            "  inflating: spider/database/wedding/schema.sql  \n",
            "  inflating: spider/database/wedding/wedding.sqlite  \n",
            "  inflating: spider/database/wine_1/annotation.json  \n",
            "  inflating: spider/database/wine_1/data_csv/README.WINE.txt  \n",
            "  inflating: spider/database/wine_1/data_csv/appellations.csv  \n",
            "  inflating: spider/database/wine_1/data_csv/grapes.csv  \n",
            "  inflating: spider/database/wine_1/data_csv/wine.csv  \n",
            "  inflating: spider/database/wine_1/link.txt  \n",
            "  inflating: spider/database/wine_1/q.txt  \n",
            "  inflating: spider/database/wine_1/wine_1.sql  \n",
            "  inflating: spider/database/wine_1/wine_1.sqlite  \n",
            "  inflating: spider/database/workshop_paper/schema.sql  \n",
            "  inflating: spider/database/workshop_paper/workshop_paper.sqlite  \n",
            "  inflating: spider/database/world_1/world_1.json  \n",
            "  inflating: spider/database/world_1/world_1.sqlite  \n",
            "  inflating: spider/database/wrestler/schema.sql  \n",
            "  inflating: spider/database/wrestler/wrestler.sqlite  \n",
            "  inflating: spider/database/wta_1/wta_1.sql  \n",
            "  inflating: spider/database/wta_1/wta_1.sqlite  \n",
            "  inflating: spider/database/yelp/schema.sql  \n",
            "  inflating: spider/database/yelp/yelp.sqlite  \n",
            "  inflating: spider/dev.json         \n",
            "  inflating: spider/dev_gold.sql     \n",
            "  inflating: spider/tables.json      \n",
            "  inflating: spider/train_gold.sql   \n",
            "  inflating: spider/train_others.json  \n",
            "  inflating: spider/train_spider.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sqlparse transformers datasets torch nltk\n",
        "MAX_SCHEMA_COLS = 128\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otniB5795sIm",
        "outputId": "88f62b21-495e-4fb9-95b3-69b7315bc5e8"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.12/dist-packages (0.5.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import sqlparse\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "NEfZj7bT5sMH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDBUtgdc6tho",
        "outputId": "60bd81e9-4caa-4d35-ee5f-87272dba3e7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "wiki_train = load_jsonl(\"/content/data/train.jsonl\")\n",
        "wiki_val   = load_jsonl(\"/content/data/dev.jsonl\")\n",
        "\n",
        "wiki_train_tables = load_jsonl(\"/content/data/train.tables.jsonl\")\n",
        "wiki_dev_tables   = load_jsonl(\"/content/data/dev.tables.jsonl\")\n",
        "\n",
        "wiki_tables = wiki_train_tables + wiki_dev_tables\n",
        "\n",
        "wiki_table_map = {t[\"id\"]: t for t in wiki_tables}\n",
        "\n",
        "print(\"Wiki train:\", len(wiki_train))\n",
        "print(\"Wiki val:\", len(wiki_val))\n",
        "print(\"Wiki tables:\", len(wiki_table_map))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkMTuPml6tj2",
        "outputId": "bd04d5b3-667c-4e22-c598-db2a62125d78"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wiki train: 56355\n",
            "Wiki val: 8421\n",
            "Wiki tables: 21301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/spider/train_spider.json\") as f:\n",
        "    spider_train = json.load(f)\n",
        "\n",
        "with open(\"/content/spider/dev.json\") as f:\n",
        "    spider_val = json.load(f)\n",
        "\n",
        "with open(\"/content/spider/tables.json\") as f:\n",
        "    spider_tables = json.load(f)\n",
        "\n",
        "spider_table_map = {t[\"db_id\"]: t for t in spider_tables}\n",
        "\n",
        "print(\"Spider train:\", len(spider_train))\n",
        "print(\"Spider val:\", len(spider_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-QQkqPZ6tot",
        "outputId": "35c78aae-bc2a-42b8-da70-4624f770139e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spider train: 7000\n",
            "Spider val: 1034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AGG_OPS = [\"\", \"MAX\", \"MIN\", \"COUNT\", \"SUM\", \"AVG\"]\n",
        "COND_OPS = [\"=\", \">\", \"<\", \"!=\"]\n",
        "\n",
        "def wiksql_to_sql(item, table):\n",
        "    sel = item[\"sql\"][\"sel\"]\n",
        "    agg = item[\"sql\"][\"agg\"]\n",
        "    conds = item[\"sql\"][\"conds\"]\n",
        "\n",
        "    col = table[\"header\"][sel]\n",
        "\n",
        "    if agg == 0:\n",
        "        select_clause = f\"SELECT {col}\"\n",
        "    else:\n",
        "        select_clause = f\"SELECT {AGG_OPS[agg]}({col})\"\n",
        "\n",
        "    from_clause = f\"FROM {table['id']}\"\n",
        "\n",
        "    if not conds:\n",
        "        return f\"{select_clause} {from_clause}\"\n",
        "\n",
        "    where = []\n",
        "    for c, o, v in conds:\n",
        "        op = COND_OPS[o]\n",
        "        col_name = table[\"header\"][c]\n",
        "        v = f\"'{v}'\" if isinstance(v, str) else str(v)\n",
        "        where.append(f\"{col_name} {op} {v}\")\n",
        "\n",
        "    return f\"{select_clause} {from_clause} WHERE \" + \" AND \".join(where)\n"
      ],
      "metadata": {
        "id": "SZdfdTaE7d6F"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sql_to_ast_tokens(sql):\n",
        "    sql = sql.lower()\n",
        "\n",
        "    tokens = [\"<QUERY>\"]\n",
        "\n",
        "    if \"select\" in sql: tokens.append(\"<SELECT>\")\n",
        "    if \"from\" in sql: tokens.append(\"<FROM>\")\n",
        "    if \"join\" in sql: tokens.append(\"<JOIN>\")\n",
        "    if \"where\" in sql: tokens.append(\"<WHERE>\")\n",
        "    if \"group by\" in sql: tokens.append(\"<GROUP_BY>\")\n",
        "    if \"having\" in sql: tokens.append(\"<HAVING>\")\n",
        "    if \"union\" in sql: tokens.append(\"<UNION>\")\n",
        "    if \"intersect\" in sql: tokens.append(\"<INTERSECT>\")\n",
        "    if \"except\" in sql: tokens.append(\"<EXCEPT>\")\n",
        "\n",
        "    if sql.count(\"select\") > 1:\n",
        "        tokens.append(\"<SUBQUERY>\")\n",
        "\n",
        "    tokens.append(\"</QUERY>\")\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "-8jR79m67Ktf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_action_vocab(wiki, spider):\n",
        "    vocab = {\"<PAD>\":0, \"<BOS>\":1, \"<EOS>\":2}\n",
        "    idx = 3\n",
        "\n",
        "    def add(sql):\n",
        "        nonlocal idx\n",
        "        tokens = sql_to_ast_tokens(sql)\n",
        "        for t in tokens:\n",
        "            if t not in vocab:\n",
        "                vocab[t] = idx\n",
        "                idx += 1\n",
        "\n",
        "    for x in wiki[:50000]:\n",
        "        table = wiki_table_map[x[\"table_id\"]]\n",
        "        add(wiksql_to_sql(x, table))\n",
        "\n",
        "    for x in spider[:20000]:\n",
        "        add(x[\"query\"])\n",
        "\n",
        "    return vocab\n",
        "\n",
        "ACTION_VOCAB = build_action_vocab(wiki_train, spider_train)\n",
        "INV_ACTION_VOCAB = {v:k for k,v in ACTION_VOCAB.items()}\n",
        "\n",
        "print(\"Action vocab size:\", len(ACTION_VOCAB))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMselK3E7KxH",
        "outputId": "2d8ce9f4-f64e-4bab-d042-41be684198b3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action vocab size: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
      ],
      "metadata": {
        "id": "az2fymKe7KzG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NL2SQLDataset(Dataset):\n",
        "    def __init__(self, data, table_map, is_spider=False):\n",
        "        self.data = data\n",
        "        self.table_map = table_map\n",
        "        self.is_spider = is_spider\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        if self.is_spider:\n",
        "            question = item[\"question\"]\n",
        "            table = self.table_map[item[\"db_id\"]]\n",
        "            schema = \" | \".join([col for _, col in table[\"column_names_original\"]])\n",
        "            sql = item[\"query\"]\n",
        "\n",
        "        else:\n",
        "            question = item[\"question\"]\n",
        "            table = self.table_map[item[\"db_id\"]]\n",
        "            schema = \" | \".join(sum(table[\"column_names_original\"], []))\n",
        "            sql = item[\"query\"]\n",
        "\n",
        "        text = f\"question: {question} schema: {schema}\"\n",
        "\n",
        "        enc = tokenizer(\n",
        "            text, padding=\"max_length\", truncation=True,\n",
        "            max_length=256, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        tgt = [ACTION_VOCAB[\"<BOS>\"]] + \\\n",
        "              [ACTION_VOCAB[t] for t in sql_to_ast_tokens(sql)] + \\\n",
        "              [ACTION_VOCAB[\"<EOS>\"]]\n",
        "\n",
        "        return enc[\"input_ids\"].squeeze(0), enc[\"attention_mask\"].squeeze(0), torch.tensor(tgt)\n"
      ],
      "metadata": {
        "id": "4iIA1sA77K2n"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    ids, masks, tgts = zip(*batch)\n",
        "\n",
        "    ids = torch.stack(ids)\n",
        "    masks = torch.stack(masks)\n",
        "\n",
        "    max_len = max(len(t) for t in tgts)\n",
        "    padded = torch.zeros(len(tgts), max_len, dtype=torch.long)\n",
        "\n",
        "    for i,t in enumerate(tgts):\n",
        "        padded[i,:len(t)] = t\n",
        "\n",
        "    return ids, masks, padded\n"
      ],
      "metadata": {
        "id": "9vquXqIe7K9H"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    def forward(self, ids, mask):\n",
        "        return self.model(ids, attention_mask=mask).last_hidden_state\n"
      ],
      "metadata": {
        "id": "GC3tPBcu7K_d"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden, vocab):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, hidden)\n",
        "        self.lstm = nn.LSTM(hidden, hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, vocab)\n",
        "\n",
        "    def forward(self, tgt):\n",
        "        emb = self.emb(tgt)\n",
        "        out,_ = self.lstm(emb)\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "k4e1MOFK7LDA"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NL2SQL(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder(384, vocab)\n",
        "\n",
        "    def forward(self, ids, mask, tgt):\n",
        "        enc = self.encoder(ids, mask)\n",
        "        return self.decoder(tgt)\n"
      ],
      "metadata": {
        "id": "i8Cyp7nw-o15"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = NL2SQLDataset(wiki_train, wiki_table_map, False)\n",
        "val_ds   = NL2SQLDataset(wiki_val, wiki_table_map, False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "hK4T6Qrf-o4c"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NL2SQL(len(ACTION_VOCAB)).to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n"
      ],
      "metadata": {
        "id": "3EBG2-5--o63"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for ids, mask, tgt in tqdm(loader):\n",
        "        ids, mask, tgt = ids.to(device), mask.to(device), tgt.to(device)\n",
        "\n",
        "        logits = model(ids, mask, tgt[:,:-1])\n",
        "\n",
        "        loss = criterion(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            tgt[:,1:].reshape(-1)\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "WwGf9e2U-o8_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_epoch(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ids, mask, tgt in loader:\n",
        "            ids, mask, tgt = ids.to(device), mask.to(device), tgt.to(device)\n",
        "            logits = model(ids, mask, tgt[:,:-1])\n",
        "\n",
        "            loss = criterion(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                tgt[:,1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "fY_wcZKr-o-8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FullNL2SQL(nn.Module):\n",
        "    def __init__(self, base_model, hidden):\n",
        "        super().__init__()\n",
        "        self.encoder = base_model.encoder\n",
        "        self.structure_decoder = base_model.decoder\n",
        "\n",
        "        self.column_predictor = ColumnPredictor(hidden)\n",
        "        self.table_predictor = TablePredictor(hidden)\n",
        "\n",
        "    def forward(self, ids, mask, tgt):\n",
        "        enc = self.encoder(ids, mask)\n",
        "        struct_logits = self.structure_decoder(tgt)\n",
        "\n",
        "        return enc, struct_logits\n"
      ],
      "metadata": {
        "id": "UkZUq0ijMJuA"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    train_loss = train_epoch(train_loader)\n",
        "    val_loss = val_epoch(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JELHXaXp-pBl",
        "outputId": "7654a56f-9003-4179-b366-934bf4546ed3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3523/3523 [04:30<00:00, 13.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train: 0.0116 | Val: 0.0120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3523/3523 [04:30<00:00, 13.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Train: 0.0115 | Val: 0.0119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3523/3523 [04:29<00:00, 13.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Train: 0.0115 | Val: 0.0119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3523/3523 [04:33<00:00, 12.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Train: 0.0114 | Val: 0.0119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3523/3523 [04:30<00:00, 13.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Train: 0.0114 | Val: 0.0118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spider_train_ds = NL2SQLDataset(spider_train, spider_table_map, True)\n",
        "spider_loader   = DataLoader(spider_train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "for epoch in range(3):\n",
        "    loss = train_epoch(spider_loader)\n",
        "    print(f\"Spider Epoch {epoch+1} | Loss {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9I1eV6e-pFG",
        "outputId": "23c730c6-9c51-4dc5-dbf0-4d871052e717"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 875/875 [00:36<00:00, 24.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spider Epoch 1 | Loss 0.4555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 875/875 [00:36<00:00, 24.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spider Epoch 2 | Loss 0.3843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 875/875 [00:35<00:00, 24.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spider Epoch 3 | Loss 0.3817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_model = FullNL2SQL(model, hidden=384).to(device)\n",
        "optimizer = torch.optim.AdamW(full_model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "wEMuOGwBP3dS"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColumnPredictor(nn.Module):\n",
        "    def __init__(self, hidden):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, col_emb):\n",
        "        # col_emb → (batch, num_cols, hidden)\n",
        "        return self.linear(col_emb).squeeze(-1)\n"
      ],
      "metadata": {
        "id": "_F2O3coYMJnP"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TablePredictor(nn.Module):\n",
        "    def __init__(self, hidden):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, table_emb):\n",
        "        return self.linear(table_emb).squeeze(-1)\n"
      ],
      "metadata": {
        "id": "6T_P8ktfMJpT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_values(question):\n",
        "    numbers = re.findall(r'\\d+', question)\n",
        "    strings = re.findall(r\"'(.*?)'\", question)\n",
        "    return numbers + strings\n"
      ],
      "metadata": {
        "id": "Nec-uJIhMJrv"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_schema_embeddings(enc_out, schema_token_positions):\n",
        "    return enc_out[:, schema_token_positions, :]\n"
      ],
      "metadata": {
        "id": "WXvW0hw7MJwR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wiki_column_labels(item, table):\n",
        "    labels = [0] * len(table[\"header\"])\n",
        "\n",
        "    labels[item[\"sql\"][\"sel\"]] = 1\n",
        "\n",
        "    for col, _, _ in item[\"sql\"][\"conds\"]:\n",
        "        labels[col] = 1\n",
        "\n",
        "    return labels\n"
      ],
      "metadata": {
        "id": "_cS2MNv5MJz0"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spider_column_labels(item, table):\n",
        "    cols = [c for _, c in table[\"column_names_original\"]][:MAX_SCHEMA_COLS]\n",
        "    labels = [0] * len(cols)\n",
        "\n",
        "    used_cols = item[\"query\"].lower()\n",
        "\n",
        "    for i, col in enumerate(cols):\n",
        "        if col.lower() in used_cols:\n",
        "            labels[i] = 1\n",
        "\n",
        "    return labels\n"
      ],
      "metadata": {
        "id": "k8ldurFwMY-b"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_loss_fn = nn.BCEWithLogitsLoss()\n",
        "def train_column_epoch(loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for ids, mask, tgt, col_labels in tqdm(loader):\n",
        "        ids, mask = ids.to(device), mask.to(device)\n",
        "        col_labels = col_labels.to(device)\n",
        "\n",
        "        enc = model.encoder(ids, mask)\n",
        "\n",
        "        num_cols = col_labels.shape[1]\n",
        "        col_emb = enc[:, -num_cols:, :]\n",
        "\n",
        "\n",
        "        logits = model.column_predictor(col_emb)\n",
        "\n",
        "        loss = column_loss_fn(logits, col_labels.float())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "k1OVVBwjMZBN"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_full_sql(question, table):\n",
        "    model.eval()\n",
        "\n",
        "    schema = \" | \".join(table[\"header\"] if \"header\" in table else\n",
        "                         [c for _,c in table[\"column_names_original\"]])\n",
        "\n",
        "    text = f\"question: {question} schema: {schema}\"\n",
        "    enc = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Structure\n",
        "    tgt = torch.tensor([[ACTION_VOCAB[\"<BOS>\"]]]).to(device)\n",
        "    for _ in range(50):\n",
        "        logits = model(enc[\"input_ids\"], enc[\"attention_mask\"], tgt)\n",
        "        next_tok = logits[:,-1].argmax(-1, keepdim=True)\n",
        "        tgt = torch.cat([tgt, next_tok], dim=1)\n",
        "        if next_tok.item() == ACTION_VOCAB[\"<EOS>\"]:\n",
        "            break\n",
        "\n",
        "    struct_tokens = [INV_ACTION_VOCAB[t.item()] for t in tgt[0]]\n",
        "\n",
        "    # Column Prediction\n",
        "    enc_out = model.encoder(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
        "\n",
        "    col_emb = enc_out[:, -len(schema.split(\" | \")): , :]\n",
        "    col_scores = model.column_predictor(col_emb)\n",
        "    col_ids = (torch.sigmoid(col_scores) > 0.5).squeeze(0)\n",
        "\n",
        "    columns = [schema.split(\" | \")[i] for i in range(len(col_ids)) if col_ids[i]]\n",
        "\n",
        "    values = extract_values(question)\n",
        "\n",
        "    return struct_tokens, columns, values\n"
      ],
      "metadata": {
        "id": "-7MWYulAMZEy"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NL2SQLSlotDataset(Dataset):\n",
        "    def __init__(self, data, table_map, is_spider=False):\n",
        "        self.data = data\n",
        "        self.table_map = table_map\n",
        "        self.is_spider = is_spider\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        if not self.is_spider:\n",
        "            question = item[\"question\"]\n",
        "            table = self.table_map[item[\"table_id\"]]\n",
        "            schema_cols = table[\"header\"]\n",
        "            sql = wiksql_to_sql(item, table)\n",
        "\n",
        "            col_labels = get_wiki_column_labels(item, table)\n",
        "            table_labels = [1]  # single table always\n",
        "\n",
        "        else:\n",
        "            question = item[\"question\"]\n",
        "            table = self.table_map[item[\"db_id\"]]\n",
        "            schema_cols = [c for _, c in table[\"column_names_original\"]][:MAX_SCHEMA_COLS]\n",
        "\n",
        "            sql = item[\"query\"]\n",
        "\n",
        "            col_labels = get_spider_column_labels(item, table)\n",
        "            table_labels = [1] * len(set([t for t,_ in table[\"column_names_original\"]]))\n",
        "\n",
        "        schema = \" | \".join(schema_cols)\n",
        "        text = f\"question: {question} schema: {schema}\"\n",
        "\n",
        "        enc = tokenizer(text, padding=\"max_length\", truncation=True,\n",
        "                        max_length=256, return_tensors=\"pt\")\n",
        "\n",
        "        tgt = [ACTION_VOCAB[\"<BOS>\"]] + \\\n",
        "              [ACTION_VOCAB[t] for t in sql_to_ast_tokens(sql)] + \\\n",
        "              [ACTION_VOCAB[\"<EOS>\"]]\n",
        "\n",
        "        return (\n",
        "            enc[\"input_ids\"].squeeze(0),\n",
        "            enc[\"attention_mask\"].squeeze(0),\n",
        "            torch.tensor(tgt),\n",
        "            torch.tensor(col_labels, dtype=torch.float),\n",
        "            torch.tensor(table_labels, dtype=torch.float)\n",
        "        )\n"
      ],
      "metadata": {
        "id": "F1zR-5S1MZHV"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_slot_fn(batch):\n",
        "    ids, masks, tgts, col_labels, table_labels = zip(*batch)\n",
        "\n",
        "    ids = torch.stack(ids)\n",
        "    masks = torch.stack(masks)\n",
        "\n",
        "    max_len = max(len(t) for t in tgts)\n",
        "    tgt_pad = torch.zeros(len(tgts), max_len, dtype=torch.long)\n",
        "\n",
        "    for i,t in enumerate(tgts):\n",
        "        tgt_pad[i,:len(t)] = t\n",
        "\n",
        "    col_labels = torch.nn.utils.rnn.pad_sequence(col_labels, batch_first=True)\n",
        "    table_labels = torch.nn.utils.rnn.pad_sequence(table_labels, batch_first=True)\n",
        "\n",
        "    return ids, masks, tgt_pad, col_labels, table_labels\n"
      ],
      "metadata": {
        "id": "t7zXiu_eMZKe"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slot_train_ds = NL2SQLSlotDataset(spider_train, spider_table_map, True)\n",
        "slot_train_loader = DataLoader(\n",
        "    slot_train_ds, batch_size=8, shuffle=True, collate_fn=collate_slot_fn\n",
        ")\n"
      ],
      "metadata": {
        "id": "6uIUU7hmMZOA"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_loss_fn = nn.BCEWithLogitsLoss()\n",
        "table_loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def train_slot_epoch(loader):\n",
        "    full_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for ids, mask, tgt, col_labels, table_labels in tqdm(loader):\n",
        "        ids = ids.to(device)\n",
        "        mask = mask.to(device)\n",
        "        col_labels = col_labels.to(device)\n",
        "\n",
        "        enc = full_model.encoder(ids, mask)\n",
        "\n",
        "        col_emb = enc[:, -col_labels.shape[1]:, :]\n",
        "        col_logits = full_model.column_predictor(col_emb)\n",
        "\n",
        "        loss = col_loss_fn(col_logits, col_labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "hE08h7xLNd4K"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    loss = train_slot_epoch(slot_train_loader)\n",
        "    print(f\"SLOT Epoch {epoch+1} | Loss {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfkpbajJNd6_",
        "outputId": "76c67193-14fc-4123-9abe-9412c3d5ef95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 875/875 [01:54<00:00,  7.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLOT Epoch 1 | Loss 0.2666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 875/875 [01:52<00:00,  7.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLOT Epoch 2 | Loss 0.2580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 875/875 [01:52<00:00,  7.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLOT Epoch 3 | Loss 0.2535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 875/875 [01:52<00:00,  7.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLOT Epoch 4 | Loss 0.2503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 52/875 [00:06<01:45,  7.80it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"Which employees earn more than 50000?\"\n",
        "table = spider_table_map[\"employee_db\"]\n",
        "\n",
        "struct, cols, values = generate_full_sql(q, table)\n",
        "\n",
        "print(\"Structure:\", struct)\n",
        "print(\"Columns:\", cols)\n",
        "print(\"Values:\", values)\n"
      ],
      "metadata": {
        "id": "YOz7O-cNNd9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7L0D8SxdNeAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "48O1Cp5qNeDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3mivzm7NeHN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}