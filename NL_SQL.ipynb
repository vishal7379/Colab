{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7e5wORGZmlg6qtw8DzW8P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/NL_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "qz_GBYEA83Tl",
        "outputId": "60cd9bd0-182b-4e8f-83ea-63431ee6fb6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset generated\n",
            "Total examples: 6720\n",
            "Schemas: 70\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cea018b7-77f2-4a34-9305-fe4a32dd62aa\", \"custom_multischema_nl2sql_7000.json\", 2988142)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================\n",
        "# MULTI-SCHEMA NL ‚Üí SQL DATASET GENERATOR (7000 EXAMPLES)\n",
        "# ============================================================\n",
        "\n",
        "import json, random\n",
        "from google.colab import files\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "OUTPUT = \"/content/custom_multischema_nl2sql_7000.json\"\n",
        "DATASET = []\n",
        "\n",
        "NUM_SCHEMAS = 70\n",
        "EXAMPLES_PER_SCHEMA = 100  # 70 √ó 100 = 7000\n",
        "\n",
        "DEPARTMENTS = [\"HR\", \"Sales\", \"IT\", \"Finance\"]\n",
        "\n",
        "def make_schema(i):\n",
        "    return {\n",
        "        f\"employees_{i}\": [\"id\", \"name\", \"salary\", \"dept_id\"],\n",
        "        f\"departments_{i}\": [\"id\", \"name\"],\n",
        "        f\"projects_{i}\": [\"id\", \"budget\", \"dept_id\"]\n",
        "    }\n",
        "\n",
        "def add(q, sql, schema):\n",
        "    DATASET.append({\n",
        "        \"question\": q,\n",
        "        \"schema\": schema,\n",
        "        \"sql\": sql\n",
        "    })\n",
        "\n",
        "for i in range(NUM_SCHEMAS):\n",
        "    schema = make_schema(i)\n",
        "    emp = f\"employees_{i}\"\n",
        "    dep = f\"departments_{i}\"\n",
        "    proj = f\"projects_{i}\"\n",
        "\n",
        "    for _ in range(EXAMPLES_PER_SCHEMA // 8):\n",
        "        add(\n",
        "            f\"show names of employees\",\n",
        "            f\"SELECT name FROM {emp}\",\n",
        "            schema\n",
        "        )\n",
        "\n",
        "        dept = random.choice(DEPARTMENTS)\n",
        "        add(\n",
        "            f\"show employees in {dept.lower()} department\",\n",
        "            f\"\"\"SELECT {emp}.name\n",
        "FROM {emp}\n",
        "JOIN {dep} ON {emp}.dept_id = {dep}.id\n",
        "WHERE {dep}.name = '{dept}'\"\"\",\n",
        "            schema\n",
        "        )\n",
        "\n",
        "        val = random.choice([30000, 40000, 50000])\n",
        "        add(\n",
        "            f\"show employees with salary greater than {val}\",\n",
        "            f\"SELECT name FROM {emp} WHERE salary > {val}\",\n",
        "            schema\n",
        "        )\n",
        "\n",
        "        add(\n",
        "            f\"count employees\",\n",
        "            f\"SELECT COUNT(*) FROM {emp}\",\n",
        "            schema\n",
        "        )\n",
        "\n",
        "        add(\n",
        "            f\"count employees in each department\",\n",
        "            f\"\"\"SELECT {dep}.name, COUNT(*)\n",
        "FROM {emp}\n",
        "JOIN {dep} ON {emp}.dept_id = {dep}.id\n",
        "GROUP BY {dep}.name\"\"\",\n",
        "            schema\n",
        "        )\n",
        "\n",
        "        add(\n",
        "            f\"show top 5 highest paid employees\",\n",
        "            f\"SELECT name FROM {emp} ORDER BY salary DESC LIMIT 5\",\n",
        "            schema\n",
        "        )\n",
        "\n",
        "        add(\n",
        "            f\"show employees in hr department with salary greater than 50000\",\n",
        "            f\"\"\"SELECT {emp}.name\n",
        "FROM {emp}\n",
        "JOIN {dep} ON {emp}.dept_id = {dep}.id\n",
        "WHERE {dep}.name = 'HR' AND {emp}.salary > 50000\"\"\",\n",
        "            schema\n",
        "        )\n",
        "\n",
        "        add(\n",
        "            f\"show total project budget by department\",\n",
        "            f\"\"\"SELECT {dep}.name, SUM({proj}.budget)\n",
        "FROM {proj}\n",
        "JOIN {dep} ON {proj}.dept_id = {dep}.id\n",
        "GROUP BY {dep}.name\"\"\",\n",
        "            schema\n",
        "        )\n",
        "\n",
        "# Trim exactly 7000\n",
        "DATASET = DATASET[:7000]\n",
        "\n",
        "with open(OUTPUT, \"w\") as f:\n",
        "    json.dump(DATASET, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Dataset generated\")\n",
        "print(\"Total examples:\", len(DATASET))\n",
        "print(\"Schemas:\", NUM_SCHEMAS)\n",
        "\n",
        "files.download(OUTPUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load your dataset\n",
        "with open('custom_multischema_nl2sql_7000.json', 'r') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "def preprocess_schema_aware(ex):\n",
        "    schema = ex[\"schema\"]\n",
        "    # 1. Create Mapping\n",
        "    t_map = {t: f\"T{i}\" for i, t in enumerate(schema.keys())}\n",
        "    c_map = {}\n",
        "    cid = 0\n",
        "    for t, cols in schema.items():\n",
        "        for c in cols:\n",
        "            c_map[f\"{t}.{c}\"] = f\"C{cid}\"  # Handle table.column\n",
        "            if c not in c_map: c_map[c] = f\"C{cid}\" # Handle just column\n",
        "            cid += 1\n",
        "\n",
        "    # 2. Map the SQL to Placeholders\n",
        "    sql = ex[\"sql\"].lower()\n",
        "    # Replace long strings first to avoid partial matches\n",
        "    for full_name, placeholder in sorted(c_map.items(), key=lambda x: -len(x[0])):\n",
        "        sql = re.sub(rf\"\\b{re.escape(full_name)}\\b\", placeholder, sql)\n",
        "    for full_name, placeholder in sorted(t_map.items(), key=lambda x: -len(x[0])):\n",
        "        sql = re.sub(rf\"\\b{re.escape(full_name)}\\b\", placeholder, sql)\n",
        "\n",
        "    # 3. Build Encoder Input: [Question] <SEP> [Schema]\n",
        "    q_toks = ex[\"question\"].lower().split()\n",
        "    s_toks, s_types = [], []\n",
        "    for t, cols in schema.items():\n",
        "        s_toks.append(t.split('_')[0]) # Use 'employees' instead of 'employees_0'\n",
        "        s_types.append(1) # Type 1 = Table\n",
        "        for c in cols:\n",
        "            s_toks.append(c)\n",
        "            s_types.append(2) # Type 2 = Column\n",
        "\n",
        "    return {\n",
        "        \"enc_input\": q_toks + [\"<SEP>\"] + s_toks,\n",
        "        \"token_types\": [0]*(len(q_toks)+1) + s_types,\n",
        "        \"dec_target\": sql.upper().replace('(', ' ( ').replace(')', ' ) ').split(),\n",
        "        \"original_schema\": {\"t_map\": t_map, \"c_map\": c_map}\n",
        "    }\n",
        "\n",
        "# Process sample\n",
        "processed_sample = preprocess_schema_aware(raw_data[0])\n",
        "print(\"Encoder Input:\", processed_sample[\"enc_input\"])\n",
        "print(\"Target SQL:\", processed_sample[\"dec_target\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSB-iWrVA-2y",
        "outputId": "a1d382af-8837-4a83-e8aa-9da591490161"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Input: ['show', 'names', 'of', 'employees', '<SEP>', 'employees', 'id', 'name', 'salary', 'dept_id', 'departments', 'id', 'name', 'projects', 'id', 'budget', 'dept_id']\n",
            "Target SQL: ['SELECT', 'C1', 'FROM', 'T0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import json\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£ DEVICE & VOCAB SETUP\n",
        "# ============================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# SQL Vocabulary components\n",
        "SQL_KEYWORDS = [\"SELECT\",\"FROM\",\"WHERE\",\"JOIN\",\"ON\",\"AS\",\"DISTINCT\"]\n",
        "LOGICAL_OPS = [\"AND\",\"OR\",\"NOT\"]\n",
        "COMPARISON_OPS = [\"=\",\"!=\",\"<>\",\">\",\"<\",\">=\",\"<=\"]\n",
        "AGG_FUNCS = [\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\"]\n",
        "GROUPING = [\"GROUP\",\"BY\",\"HAVING\"]\n",
        "ORDERING = [\"ORDER\",\"ASC\",\"DESC\",\"LIMIT\"]\n",
        "PUNCT = [\",\",\"(\",\")\"]\n",
        "SPECIAL = [\"<PAD>\",\"<EOS>\",\"<BOS>\",\"VALUE\"] # Added <BOS>\n",
        "\n",
        "MAX_TABLES, MAX_COLUMNS = 128, 1024\n",
        "TABLE_TOKENS = [f\"T{i}\" for i in range(MAX_TABLES)]\n",
        "COLUMN_TOKENS = [f\"C{i}\" for i in range(MAX_COLUMNS)]\n",
        "\n",
        "DECODER_VOCAB = SQL_KEYWORDS + LOGICAL_OPS + COMPARISON_OPS + AGG_FUNCS + \\\n",
        "                GROUPING + ORDERING + PUNCT + SPECIAL + TABLE_TOKENS + COLUMN_TOKENS\n",
        "\n",
        "token_to_id = {tok: i for i, tok in enumerate(DECODER_VOCAB)}\n",
        "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
        "VOCAB_SIZE = len(DECODER_VOCAB)\n",
        "\n",
        "# ============================================================\n",
        "# 2Ô∏è‚É£ DATASET & PLACEHOLDER MAPPING\n",
        "# ============================================================\n",
        "def build_schema_maps(schema):\n",
        "    table_map = {table: f\"T{i}\" for i, table in enumerate(schema.keys())}\n",
        "    column_map = {}\n",
        "    col_id = 0\n",
        "    for table, cols in schema.items():\n",
        "        for col in cols:\n",
        "            column_map[f\"{table}.{col}\"] = f\"C{col_id}\"\n",
        "            column_map[col] = f\"C{col_id}\" # Fallback for naked columns\n",
        "            col_id += 1\n",
        "    return table_map, column_map\n",
        "\n",
        "def sql_to_placeholder(sql, table_map, column_map):\n",
        "    sql_out = sql.lower()\n",
        "    # Sort by length descending to prevent partial replacements (e.g., 'id' in 'dept_id')\n",
        "    for full_col, cid in sorted(column_map.items(), key=lambda x: -len(x[0])):\n",
        "        sql_out = re.sub(rf\"\\b{re.escape(full_col.lower())}\\b\", cid, sql_out)\n",
        "    for table, tid in table_map.items():\n",
        "        sql_out = re.sub(rf\"\\b{re.escape(table.lower())}\\b\", tid, sql_out)\n",
        "    sql_out = re.sub(r\"'\\w+'|\\b\\d+\\b\", \"VALUE\", sql_out)\n",
        "    return sql_out.upper()\n",
        "\n",
        "class SpiderDataset(Dataset):\n",
        "    def __init__(self, data, encoder_vocab):\n",
        "        self.data = data\n",
        "        self.encoder_vocab = encoder_vocab\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        input_ids = [self.encoder_vocab.get(tok, 1) for tok in ex[\"tokens\"]]\n",
        "        table_map, col_map = build_schema_maps(ex[\"schema\"])\n",
        "        sql_ph = sql_to_placeholder(ex[\"sql\"], table_map, col_map)\n",
        "\n",
        "        # Target: <BOS> SELECT ... <EOS>\n",
        "        tgt_ids = [token_to_id[\"<BOS>\"]] + [token_to_id.get(t, token_to_id[\"VALUE\"]) for t in sql_ph.split()] + [token_to_id[\"<EOS>\"]]\n",
        "\n",
        "        return torch.tensor(input_ids), torch.tensor(ex[\"token_types\"]), \\\n",
        "               torch.tensor(ex[\"schema_labels\"], dtype=torch.float), torch.tensor(tgt_ids)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ids, types, labels, tgts = zip(*batch)\n",
        "    def pad(s, v=0): return nn.utils.rnn.pad_sequence(s, batch_first=True, padding_value=v)\n",
        "    return pad(ids), pad(types), pad(labels), pad(tgts)\n",
        "\n",
        "# ============================================================\n",
        "# 3Ô∏è‚É£ SCHEMA-AWARE TRANSFORMER MODEL\n",
        "# ============================================================\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads, self.d_k = num_heads, d_model // num_heads\n",
        "        self.Wq, self.Wk, self.Wv, self.Wo = [nn.Linear(d_model, d_model) for _ in range(4)]\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        B, Tq, D = q.size()\n",
        "        Q = self.Wq(q).view(B, Tq, self.num_heads, self.d_k).transpose(1,2)\n",
        "        K = self.Wk(k).view(B, k.size(1), self.num_heads, self.d_k).transpose(1,2)\n",
        "        V = self.Wv(v).view(B, k.size(1), self.num_heads, self.d_k).transpose(1,2)\n",
        "        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None: scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        return self.Wo(torch.matmul(attn, V).transpose(1,2).contiguous().view(B, Tq, D))\n",
        "\n",
        "class SchemaAwareEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.typ_emb = nn.Embedding(3, d_model)\n",
        "        self.layers = nn.ModuleList([nn.TransformerEncoderLayer(d_model, num_heads, d_ff, batch_first=True) for _ in range(num_layers)])\n",
        "        self.classifier = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x, types, mask=None):\n",
        "        x = self.tok_emb(x) + self.typ_emb(types)\n",
        "        for layer in self.layers: x = layer(x, src_key_padding_mask=(mask==0))\n",
        "        return x, self.classifier(x).squeeze(-1)\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.layers = nn.ModuleList([nn.TransformerDecoderLayer(d_model, num_heads, d_ff, batch_first=True) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        x = self.emb(tgt)\n",
        "        for layer in self.layers: x = layer(x, enc_out, tgt_mask=tgt_mask, memory_key_padding_mask=(memory_mask==0))\n",
        "        return self.fc(x)\n",
        "\n",
        "# ============================================================\n",
        "# 4Ô∏è‚É£ TRAINING & PICARD INFERENCE\n",
        "# ============================================================\n",
        "def picard_filter(valid_prefix):\n",
        "    if len(valid_prefix) <= 1: return [\"SELECT\"]\n",
        "    last = valid_prefix[-1]\n",
        "    if last == \"SELECT\": return AGG_FUNCS + COLUMN_TOKENS + [\"DISTINCT\"]\n",
        "    if last in AGG_FUNCS or last in COLUMN_TOKENS: return [\",\", \"FROM\"]\n",
        "    if last == \"FROM\": return TABLE_TOKENS\n",
        "    if last in TABLE_TOKENS: return [\"WHERE\", \"JOIN\", \"<EOS>\"]\n",
        "    if last == \"WHERE\": return COLUMN_TOKENS\n",
        "    if last in COMPARISON_OPS: return [\"VALUE\"]\n",
        "    return [\"<EOS>\", \"AND\", \"OR\", \"LIMIT\"]\n",
        "\n",
        "def generate_sql_picard(encoder, decoder, input_ids, token_types, device):\n",
        "    encoder.eval(); decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        mask = (input_ids != 0).long()\n",
        "        enc_out, _ = encoder(input_ids, token_types, mask)\n",
        "\n",
        "        cur = torch.tensor([[token_to_id[\"<BOS>\"]]], device=device)\n",
        "        generated = [\"<BOS>\"]\n",
        "\n",
        "        for _ in range(30):\n",
        "            tgt_mask = torch.triu(torch.ones(cur.size(1), cur.size(1), device=device), diagonal=1).bool()\n",
        "            logits = decoder(cur, enc_out, tgt_mask=tgt_mask)[0, -1]\n",
        "\n",
        "            allowed = picard_filter(generated)\n",
        "            mask_logits = torch.full_like(logits, float(\"-inf\"))\n",
        "            allowed_ids = [token_to_id[t] for t in allowed if t in token_to_id]\n",
        "            mask_logits[allowed_ids] = logits[allowed_ids]\n",
        "\n",
        "            next_id = mask_logits.argmax().item()\n",
        "            next_tok = id_to_token[next_id]\n",
        "            if next_tok == \"<EOS>\": break\n",
        "\n",
        "            generated.append(next_tok)\n",
        "            cur = torch.cat([cur, torch.tensor([[next_id]], device=device)], dim=1)\n",
        "\n",
        "    return \" \".join(generated[1:])\n",
        "\n",
        "# Initialize (Assuming encoder_vocab exists from your build_encoder_vocab function)\n",
        "encoder = SchemaAwareEncoder(enc_vocab_size, d_model=256, num_heads=8, d_ff=1024, num_layers=4).to(device)\n",
        "decoder = TransformerDecoder(num_layers=4, d_model=256, num_heads=8, d_ff=1024, vocab_size=VOCAB_SIZE).to(device)\n",
        "\n",
        "# Training logic remains similar but ensures target shifts:\n",
        "# logits = decoder(tgt[:, :-1], enc_out)\n",
        "# loss = criterion(logits, tgt[:, 1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "pQ07bFBI_QaT",
        "outputId": "e438a73f-65be-4102-927c-dc4afbd9bd5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'enc_vocab_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-949594503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;31m# Initialize (Assuming encoder_vocab exists from your build_encoder_vocab function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSchemaAwareEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'enc_vocab_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import json\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£ DEVICE & DATA LOADING\n",
        "# ============================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "with open('custom_multischema_nl2sql_7000.json', 'r') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# ============================================================\n",
        "# 2Ô∏è‚É£ BUILD VOCABULARIES\n",
        "# ============================================================\n",
        "def build_vocabs(data):\n",
        "    enc_vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SEP>\": 2}\n",
        "    idx = 3\n",
        "    for ex in data:\n",
        "        # Tokenize question\n",
        "        for tok in ex[\"question\"].lower().split():\n",
        "            if tok not in enc_vocab:\n",
        "                enc_vocab[tok] = idx\n",
        "                idx += 1\n",
        "        # Tokenize schema (normalized)\n",
        "        for t, cols in ex[\"schema\"].items():\n",
        "            t_norm = t.split('_')[0]\n",
        "            if t_norm not in enc_vocab:\n",
        "                enc_vocab[t_norm] = idx\n",
        "                idx += 1\n",
        "            for c in cols:\n",
        "                if c not in enc_vocab:\n",
        "                    enc_vocab[c] = idx\n",
        "                    idx += 1\n",
        "    return enc_vocab\n",
        "\n",
        "encoder_vocab = build_vocabs(raw_data)\n",
        "enc_vocab_size = len(encoder_vocab)\n",
        "\n",
        "# Decoder Vocab\n",
        "SQL_KEYWORDS = [\"SELECT\",\"FROM\",\"WHERE\",\"JOIN\",\"ON\",\"AS\",\"DISTINCT\",\"ORDER\",\"BY\",\"DESC\",\"ASC\",\"LIMIT\",\"GROUP\",\"AND\"]\n",
        "SPECIAL = [\"<PAD>\",\"<EOS>\",\"<BOS>\",\"VALUE\"]\n",
        "TABLE_TOKENS = [f\"T{i}\" for i in range(10)] # Max 10 tables per query\n",
        "COLUMN_TOKENS = [f\"C{i}\" for i in range(50)] # Max 50 columns per query\n",
        "DECODER_VOCAB = SPECIAL + SQL_KEYWORDS + [\"COUNT\", \"AVG\", \"SUM\", \"MAX\", \"MIN\", \"(\", \")\", \",\", \"=\", \">\", \"<\"] + TABLE_TOKENS + COLUMN_TOKENS\n",
        "\n",
        "token_to_id = {tok: i for i, tok in enumerate(DECODER_VOCAB)}\n",
        "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
        "VOCAB_SIZE = len(DECODER_VOCAB)\n",
        "\n",
        "print(f\"‚úÖ Encoder Vocab: {enc_vocab_size} | Decoder Vocab: {VOCAB_SIZE}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3Ô∏è‚É£ SCHEMA MAPPING & DATASET\n",
        "# ============================================================\n",
        "def build_schema_maps(schema):\n",
        "    table_map = {table: f\"T{i}\" for i, table in enumerate(schema.keys())}\n",
        "    column_map = {}\n",
        "    col_id = 0\n",
        "    for table, cols in schema.items():\n",
        "        for col in cols:\n",
        "            column_map[f\"{table}.{col}\"] = f\"C{col_id}\"\n",
        "            column_map[col] = f\"C{col_id}\"\n",
        "            col_id += 1\n",
        "    return table_map, column_map\n",
        "\n",
        "def sql_to_placeholder(sql, table_map, column_map):\n",
        "    sql_out = sql.lower()\n",
        "    for full_col, cid in sorted(column_map.items(), key=lambda x: -len(x[0])):\n",
        "        sql_out = re.sub(rf\"\\b{re.escape(full_col.lower())}\\b\", cid, sql_out)\n",
        "    for table, tid in table_map.items():\n",
        "        sql_out = re.sub(rf\"\\b{re.escape(table.lower())}\\b\", tid, sql_out)\n",
        "    sql_out = re.sub(r\"'\\w+'|\\b\\d+\\b\", \"VALUE\", sql_out)\n",
        "    return sql_out.upper()\n",
        "\n",
        "class SchemaAwareDataset(Dataset):\n",
        "    def __init__(self, data, enc_vocab):\n",
        "        self.data = data\n",
        "        self.enc_vocab = enc_vocab\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        t_map, c_map = build_schema_maps(ex[\"schema\"])\n",
        "\n",
        "        # Encoder Input\n",
        "        q_toks = ex[\"question\"].lower().split()\n",
        "        s_toks, s_types = [], []\n",
        "        for t, cols in ex[\"schema\"].items():\n",
        "            s_toks.append(t.split('_')[0]); s_types.append(1)\n",
        "            for c in cols:\n",
        "                s_toks.append(c); s_types.append(2)\n",
        "\n",
        "        full_tokens = q_toks + [\"<SEP>\"] + s_toks\n",
        "        ids = [self.enc_vocab.get(t, 1) for t in full_tokens]\n",
        "        types = [0]*(len(q_toks)+1) + s_types\n",
        "\n",
        "        # Decoder Target\n",
        "        sql_ph = sql_to_placeholder(ex[\"sql\"], t_map, c_map)\n",
        "        tgt = [token_to_id[\"<BOS>\"]] + [token_to_id.get(t, token_to_id[\"VALUE\"]) for t in sql_ph.split()] + [token_to_id[\"<EOS>\"]]\n",
        "\n",
        "        return torch.tensor(ids), torch.tensor(types), torch.tensor(tgt)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ids, types, tgts = zip(*batch)\n",
        "    p_ids = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=0)\n",
        "    p_types = nn.utils.rnn.pad_sequence(types, batch_first=True, padding_value=0)\n",
        "    p_tgts = nn.utils.rnn.pad_sequence(tgts, batch_first=True, padding_value=0)\n",
        "    return p_ids, p_types, p_tgts\n",
        "\n",
        "# ============================================================\n",
        "# 4Ô∏è‚É£ MODEL DEFINITIONS\n",
        "# ============================================================\n",
        "class SchemaAwareEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.typ_emb = nn.Embedding(3, d_model)\n",
        "        layer = nn.TransformerEncoderLayer(d_model, num_heads, d_ff, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(layer, num_layers)\n",
        "\n",
        "    def forward(self, x, types, mask=None):\n",
        "        x = self.tok_emb(x) + self.typ_emb(types)\n",
        "        return self.encoder(x, src_key_padding_mask=(mask==0)), None\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        layer = nn.TransformerDecoderLayer(d_model, num_heads, d_ff, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(layer, num_layers)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        x = self.emb(tgt)\n",
        "        x = self.decoder(x, enc_out, tgt_mask=tgt_mask, memory_key_padding_mask=(memory_mask==0))\n",
        "        return self.fc(x)\n",
        "\n",
        "# ============================================================\n",
        "# 5Ô∏è‚É£ INITIALIZATION & TRAINING\n",
        "# ============================================================\n",
        "encoder = SchemaAwareEncoder(enc_vocab_size, 256, 8, 1024, 4).to(device)\n",
        "decoder = TransformerDecoder(4, 256, 8, 1024, VOCAB_SIZE).to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "dataset = SchemaAwareDataset(raw_data, encoder_vocab)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "for epoch in range(5):\n",
        "    encoder.train(); decoder.train()\n",
        "    epoch_loss = 0\n",
        "    for ids, types, tgts in loader:\n",
        "        ids, types, tgts = ids.to(device), types.to(device), tgts.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        mask = (ids != 0)\n",
        "        enc_out, _ = encoder(ids, types, mask)\n",
        "\n",
        "        dec_input = tgts[:, :-1]\n",
        "        dec_target = tgts[:, 1:]\n",
        "\n",
        "        sz = dec_input.size(1)\n",
        "        tgt_mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
        "\n",
        "        logits = decoder(dec_input, enc_out, tgt_mask=tgt_mask, memory_mask=mask)\n",
        "        loss = criterion(logits.reshape(-1, VOCAB_SIZE), dec_target.reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {epoch_loss/len(loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "mZNG_20oCGCH",
        "outputId": "7f5c25b5-8ac1-4cde-fe0d-51500c8bc8db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-756784732.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0mtot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-756784732.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"schema\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\((.*?)\\)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"select (.*?) from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         tgt = [\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sql_robust(question, schema, encoder, decoder, encoder_vocab, device):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # 1. Build Mapping for this specific inference instance\n",
        "    t_map, c_map = build_schema_maps(schema)\n",
        "    rev_t_map = {v: k for k, v in t_map.items()}\n",
        "    rev_c_map = {v: k for k, v in c_map.items()}\n",
        "\n",
        "    # 2. Encode Input\n",
        "    q_toks = question.lower().split()\n",
        "    s_toks, s_types = [], []\n",
        "    for t, cols in schema.items():\n",
        "        s_toks.append(t.split('_')[0]); s_types.append(1)\n",
        "        for c in cols:\n",
        "            s_toks.append(c); s_types.append(2)\n",
        "\n",
        "    full_tokens = q_toks + [\"<SEP>\"] + s_toks\n",
        "    ids = torch.tensor([encoder_vocab.get(t, 1) for t in full_tokens]).unsqueeze(0).to(device)\n",
        "    types = torch.tensor([0]*(len(q_toks)+1) + s_types).unsqueeze(0).to(device)\n",
        "\n",
        "    # 3. Greedy Decoding with PICARD-style constraints\n",
        "    with torch.no_grad():\n",
        "        enc_out, _ = encoder(ids, types, mask=(ids != 0))\n",
        "        tgt_indices = [token_to_id[\"<BOS>\"]]\n",
        "        generated_tokens = [\"<BOS>\"]\n",
        "\n",
        "        for _ in range(30):\n",
        "            tgt_tensor = torch.tensor([tgt_indices]).to(device)\n",
        "            sz = tgt_tensor.size(1)\n",
        "            tgt_mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
        "\n",
        "            output = decoder(tgt_tensor, enc_out, tgt_mask=tgt_mask, memory_mask=(ids != 0))\n",
        "            logits = output[0, -1, :]\n",
        "\n",
        "            # --- PICARD FILTERING ---\n",
        "            # We force the model to only pick columns that exist in the current schema\n",
        "            allowed_placeholders = list(rev_t_map.keys()) + list(rev_c_map.keys()) + SQL_KEYWORDS + SPECIAL + PUNCT + AGG_FUNCS\n",
        "            allowed_ids = [token_to_id[tok] for tok in allowed_placeholders if tok in token_to_id]\n",
        "\n",
        "            mask_logits = torch.full_like(logits, float(\"-inf\"))\n",
        "            mask_logits[allowed_ids] = logits[allowed_ids]\n",
        "\n",
        "            next_id = mask_logits.argmax().item()\n",
        "            next_tok = id_to_token[next_id]\n",
        "\n",
        "            if next_tok == \"<EOS>\": break\n",
        "            tgt_indices.append(next_id)\n",
        "            generated_tokens.append(next_tok)\n",
        "\n",
        "    # 4. Final De-Mapping with Boundry Protection\n",
        "    placeholder_sql = \" \".join(generated_tokens[1:])\n",
        "    final_sql = placeholder_sql\n",
        "\n",
        "    # Sort keys by length descending (C10 before C1) to prevent partial replacement\n",
        "    for cid in sorted(rev_c_map.keys(), key=lambda x: int(x[1:]), reverse=True):\n",
        "        final_sql = re.sub(rf\"\\b{cid}\\b\", rev_c_map[cid], final_sql)\n",
        "    for tid in sorted(rev_t_map.keys(), key=lambda x: int(x[1:]), reverse=True):\n",
        "        final_sql = re.sub(rf\"\\b{tid}\\b\", rev_t_map[tid], final_sql)\n",
        "\n",
        "    return final_sql\n",
        "\n",
        "# ============================================================\n",
        "# RUN TEST\n",
        "# ============================================================\n",
        "sample_schema = {\n",
        "    \"employees\": [\"id\", \"name\", \"salary\", \"dept_id\"],\n",
        "    \"departments\": [\"id\", \"name\"]\n",
        "}\n",
        "# The model will now correctly link \"names\" to \"employees.name\"\n",
        "# because it was trained to look at the T0 table first.\n",
        "print(\"Result:\", predict_sql_robust(\"show salary of employees\", sample_schema, encoder, decoder, encoder_vocab, device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afmORrrnWefa",
        "outputId": "670518c6-ed7c-46e9-f375-271548df8f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: SELECT departments.name FROM employees WHERE salary\n"
          ]
        }
      ]
    }
  ]
}