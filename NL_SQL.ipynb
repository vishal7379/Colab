{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/NL_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker\n",
        "\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from faker import Faker\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wacCB09hFijs",
        "outputId": "4d32ce57-96ed-4607-bc66-9a4428acec52"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.12/dist-packages (40.4.0)\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SQL_PATTERNS = (\n",
        "    [\"join\"] * 5 +\n",
        "    [\"join_where\"] * 4 +        # ⭐ NEW\n",
        "    [\"group_by\"] * 3 +\n",
        "    [\"having\"] * 2 +\n",
        "    [\"where\"] * 2 +\n",
        "    [\"aggregation\"] * 2 +\n",
        "    [\"order_by\"] * 3 +         # ⭐ NEW\n",
        "    [\"limit\"] * 2 +            # ⭐ NEW\n",
        "    [\"order_by_limit\"] * 3 +   # ⭐ VERY IMPORTANT\n",
        "    [\"multi_select\"] +\n",
        "    [\"simple_select\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "SMYdGF_-FimD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_COLUMNS = [\n",
        "    \"name\",\"email\",\"age\",\"salary\",\n",
        "    \"department\",\"city\",\"country\",\n",
        "    \"price\",\"amount\",\"quantity\",\n",
        "    \"created_at\",\"updated_at\"\n",
        "]\n",
        "\n",
        "FK_COLUMNS = [\n",
        "    \"user_id\",\n",
        "    \"order_id\",\n",
        "    \"product_id\",\n",
        "    \"customer_id\"\n",
        "]"
      ],
      "metadata": {
        "id": "pZOjhLZ6Fioa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COLUMN_TYPES = {\n",
        "    \"numeric\":[\n",
        "        \"age\",\"salary\",\"price\",\n",
        "        \"amount\",\"quantity\"\n",
        "    ],\n",
        "    \"text\":[\n",
        "        \"name\",\"email\",\n",
        "        \"department\",\"city\",\"country\"\n",
        "    ],\n",
        "    \"date\":[\n",
        "        \"created_at\",\"updated_at\"\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "0dqcpa4NMvJ0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_relational_schema():\n",
        "\n",
        "    num_tables = random.randint(2,5)\n",
        "\n",
        "    fake.unique.clear()\n",
        "\n",
        "    tables = {}\n",
        "    relationships = {}\n",
        "\n",
        "    table_names = [fake.unique.word() for _ in range(num_tables)]\n",
        "\n",
        "    for t in table_names:\n",
        "\n",
        "        cols = {\n",
        "            \"numeric\": random.sample(COLUMN_TYPES[\"numeric\"],\n",
        "                                     random.randint(1,3)),\n",
        "\n",
        "            \"text\": random.sample(COLUMN_TYPES[\"text\"],\n",
        "                                  random.randint(1,3)),\n",
        "\n",
        "            \"date\": random.sample(COLUMN_TYPES[\"date\"],\n",
        "                                  random.randint(0,1))\n",
        "        }\n",
        "\n",
        "        # flatten\n",
        "        flat_cols = [\"id\"]\n",
        "\n",
        "        for v in cols.values():\n",
        "            flat_cols.extend(v)\n",
        "\n",
        "        tables[t] = {\n",
        "            \"all\": flat_cols,\n",
        "            \"numeric\": cols[\"numeric\"],\n",
        "            \"text\": cols[\"text\"],\n",
        "            \"date\": cols[\"date\"]\n",
        "        }\n",
        "\n",
        "    relationships_list = []\n",
        "\n",
        "    for i in range(1, num_tables):\n",
        "\n",
        "        parent = table_names[i-1]\n",
        "        child = table_names[i]\n",
        "\n",
        "        fk = f\"{parent}_id\"\n",
        "\n",
        "        tables[child][\"all\"].append(fk)\n",
        "\n",
        "        relationships_list.append(\n",
        "            (child, fk, parent, \"id\")\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"tables\": tables,\n",
        "        \"relationships\": relationships_list\n",
        "    }"
      ],
      "metadata": {
        "id": "FP1HPZB5Fiqm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def schema_to_text(schema):\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    for table, col_dict in schema[\"tables\"].items():\n",
        "\n",
        "        cols = col_dict[\"all\"]\n",
        "\n",
        "        col_tokens = \" \".join([f\"[COL] {c}\" for c in cols])\n",
        "        parts.append(f\"[TABLE] {table} {col_tokens}\")\n",
        "\n",
        "    # relationships\n",
        "    for child, fk, parent, pk in schema[\"relationships\"]:\n",
        "        parts.append(\n",
        "            f\"[REL] {child}.{fk} -> {parent}.{pk}\"\n",
        "        )\n",
        "\n",
        "    return \" \".join(parts)"
      ],
      "metadata": {
        "id": "2J47wv3MFis7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT_TEMPLATES = [\n",
        "    \"show {col} from {table}\",\n",
        "    \"list {col} in {table}\",\n",
        "    \"display {col} from {table}\",\n",
        "    \"what are the {col} in {table}\",\n",
        "]"
      ],
      "metadata": {
        "id": "cdSBBALCFiv_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sql(schema):\n",
        "\n",
        "    pattern = random.choice(SQL_PATTERNS)\n",
        "\n",
        "    tables = schema[\"tables\"]\n",
        "    rels = schema[\"relationships\"]\n",
        "\n",
        "    table = random.choice(list(tables.keys()))\n",
        "\n",
        "    columns = tables[table][\"all\"]\n",
        "    numeric_cols = tables[table][\"numeric\"]\n",
        "    text_cols = tables[table][\"text\"]\n",
        "\n",
        "    # ---------- SIMPLE SELECT ----------\n",
        "    if pattern == \"simple_select\":\n",
        "\n",
        "        selected = random.sample(\n",
        "            columns,\n",
        "            random.randint(1, min(3, len(columns)))\n",
        "        )\n",
        "\n",
        "        question = f\"show {', '.join(selected)} from {table}\"\n",
        "        sql = f\"SELECT {', '.join(selected)} FROM {table}\"\n",
        "\n",
        "\n",
        "    # ---------- MULTI SELECT ----------\n",
        "    elif pattern == \"multi_select\":\n",
        "\n",
        "        selected = random.sample(\n",
        "            columns,\n",
        "            random.randint(2, min(4, len(columns)))\n",
        "        )\n",
        "\n",
        "        question = f\"show {', '.join(selected)} from {table}\"\n",
        "        sql = f\"SELECT {', '.join(selected)} FROM {table}\"\n",
        "\n",
        "\n",
        "    # ---------- WHERE ----------\n",
        "    elif pattern == \"where\":\n",
        "\n",
        "        # Prefer numeric filters\n",
        "        if numeric_cols and random.random() < 0.7:\n",
        "\n",
        "            col = random.choice(numeric_cols)\n",
        "            operator = random.choice([\"=\", \">\", \"<\"])\n",
        "            value = random.randint(1, 100)\n",
        "\n",
        "        else:\n",
        "\n",
        "            col = random.choice(text_cols if text_cols else columns)\n",
        "            operator = \"=\"\n",
        "            value = f\"'{fake.word()}'\"\n",
        "\n",
        "        question = f\"show {col} from {table} where {col} {operator} {value}\"\n",
        "        sql = f\"SELECT {col} FROM {table} WHERE {col} {operator} {value}\"\n",
        "\n",
        "\n",
        "    # ---------- AGGREGATION ----------\n",
        "    elif pattern == \"aggregation\":\n",
        "\n",
        "        agg = random.choice([\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\"])\n",
        "\n",
        "        if agg in [\"SUM\",\"AVG\"]:\n",
        "\n",
        "            if not numeric_cols:\n",
        "                return generate_sql(schema)\n",
        "\n",
        "            col = random.choice(numeric_cols)\n",
        "\n",
        "        elif agg in [\"MIN\",\"MAX\"]:\n",
        "\n",
        "            candidates = numeric_cols + text_cols\n",
        "            if not candidates:\n",
        "                return generate_sql(schema)\n",
        "\n",
        "            col = random.choice(candidates)\n",
        "\n",
        "        else:  # COUNT\n",
        "            col = random.choice(columns)\n",
        "\n",
        "        question = f\"what is the {agg.lower()} of {col} in {table}\"\n",
        "        sql = f\"SELECT {agg}({col}) FROM {table}\"\n",
        "        # ---------- ORDER BY ----------\n",
        "    elif pattern == \"order_by\":\n",
        "\n",
        "        # prefer numeric columns for sorting\n",
        "        if numeric_cols:\n",
        "            col = random.choice(numeric_cols)\n",
        "        else:\n",
        "            col = random.choice(columns)\n",
        "\n",
        "        direction = random.choice([\"ASC\",\"DESC\"])\n",
        "\n",
        "        question = f\"show all records from {table} ordered by {col} {direction.lower()}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "        SELECT *\n",
        "        FROM {table}\n",
        "        ORDER BY {col} {direction}\n",
        "        \"\"\"\n",
        "        # ---------- LIMIT ----------\n",
        "    elif pattern == \"limit\":\n",
        "\n",
        "        limit_val = random.randint(3,20)\n",
        "\n",
        "        question = f\"show first {limit_val} rows from {table}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "SELECT *\n",
        "FROM {table}\n",
        "LIMIT {limit_val}\n",
        "\"\"\"\n",
        "        # ---------- ORDER BY + LIMIT ----------\n",
        "    elif pattern == \"order_by_limit\":\n",
        "\n",
        "        if numeric_cols:\n",
        "            col = random.choice(numeric_cols)\n",
        "        else:\n",
        "            col = random.choice(columns)\n",
        "\n",
        "        direction = random.choice([\"ASC\",\"DESC\"])\n",
        "        limit_val = random.randint(3,15)\n",
        "\n",
        "        question = f\"show top {limit_val} records from {table} ordered by {col}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "SELECT *\n",
        "FROM {table}\n",
        "ORDER BY {col} {direction}\n",
        "LIMIT {limit_val}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    # ---------- GROUP BY ----------\n",
        "    elif pattern == \"group_by\":\n",
        "\n",
        "        group_candidates = text_cols if text_cols else columns\n",
        "        group_col = random.choice(group_candidates)\n",
        "\n",
        "        if numeric_cols:\n",
        "            agg_col = random.choice(numeric_cols)\n",
        "        else:\n",
        "            return generate_sql(schema)\n",
        "\n",
        "        agg = random.choice([\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\"])\n",
        "\n",
        "        question = f\"{agg.lower()} of {agg_col} grouped by {group_col}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "SELECT {group_col}, {agg}({agg_col})\n",
        "FROM {table}\n",
        "GROUP BY {group_col}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    # ---------- HAVING ----------\n",
        "    elif pattern == \"having\":\n",
        "\n",
        "        group_candidates = text_cols if text_cols else columns\n",
        "        group_col = random.choice(group_candidates)\n",
        "\n",
        "        if not numeric_cols:\n",
        "            return generate_sql(schema)\n",
        "\n",
        "        agg_col = random.choice(numeric_cols)\n",
        "        agg = random.choice([\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\"])\n",
        "\n",
        "        operator = random.choice([\">\", \"<\", \"=\"])\n",
        "        value = random.randint(1,50)\n",
        "\n",
        "        question = f\"{agg.lower()} of {agg_col} grouped by {group_col} having value {operator} {value}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "SELECT {group_col}, {agg}({agg_col})\n",
        "FROM {table}\n",
        "GROUP BY {group_col}\n",
        "HAVING {agg}({agg_col}) {operator} {value}\n",
        "\"\"\"\n",
        "    # ---------- JOIN + WHERE ----------\n",
        "    elif pattern == \"join_where\" and rels:\n",
        "\n",
        "        child, fk, parent, pk = random.choice(rels)\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            left, right = parent, child\n",
        "            left_key, right_key = pk, fk\n",
        "        else:\n",
        "            left, right = child, parent\n",
        "            left_key, right_key = fk, pk\n",
        "\n",
        "        right_numeric = tables[right][\"numeric\"]\n",
        "        right_text = tables[right][\"text\"]\n",
        "\n",
        "        if right_numeric and random.random() < 0.7:\n",
        "\n",
        "            where_col = random.choice(right_numeric)\n",
        "            operator = random.choice([\">\",\"<\",\"=\"])\n",
        "            value = random.randint(1,100)\n",
        "\n",
        "        else:\n",
        "\n",
        "            candidates = right_text if right_text else right_numeric\n",
        "            if not candidates:\n",
        "                return generate_sql(schema)\n",
        "\n",
        "            where_col = random.choice(candidates)\n",
        "            operator = \"=\"\n",
        "            value = f\"'{fake.word()}'\"\n",
        "\n",
        "        select_col = random.choice(tables[left][\"all\"])\n",
        "\n",
        "        question = f\"show {select_col} from {left} joined with {right} where {where_col} {operator} {value}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "SELECT {left}.{select_col}\n",
        "FROM {left}\n",
        "JOIN {right}\n",
        "ON {left}.{left_key} = {right}.{right_key}\n",
        "WHERE {right}.{where_col} {operator} {value}\n",
        "\"\"\"\n",
        "\n",
        "    # ---------- JOIN ----------\n",
        "    elif pattern == \"join\" and rels:\n",
        "\n",
        "        child, fk, parent, pk = random.choice(rels)\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            left, right = parent, child\n",
        "            left_key, right_key = pk, fk\n",
        "        else:\n",
        "            left, right = child, parent\n",
        "            left_key, right_key = fk, pk\n",
        "\n",
        "        select_col = random.choice(tables[left][\"all\"])\n",
        "\n",
        "        question = f\"show {select_col} from {left} joined with {right}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "SELECT {left}.{select_col}\n",
        "FROM {left}\n",
        "JOIN {right}\n",
        "ON {left}.{left_key} = {right}.{right_key}\n",
        "\"\"\"\n",
        "\n",
        "    else:\n",
        "        return generate_sql(schema)\n",
        "\n",
        "    return question.strip(), sql.strip()"
      ],
      "metadata": {
        "id": "SxOl0INZFizX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset_by_schema(num_schemas=12000,\n",
        "                            queries_per_schema=8):\n",
        "\n",
        "    schemas = [generate_relational_schema()\n",
        "               for _ in range(num_schemas)]\n",
        "\n",
        "    split = int(0.9 * num_schemas)\n",
        "\n",
        "    train_s = schemas[:split]\n",
        "    test_s = schemas[split:]\n",
        "\n",
        "    def build_examples(schema_list):\n",
        "\n",
        "        data = []\n",
        "\n",
        "        for schema in schema_list:\n",
        "\n",
        "            schema_text = schema_to_text(schema)\n",
        "\n",
        "            for _ in range(queries_per_schema):\n",
        "\n",
        "                q, sql = generate_sql(schema)\n",
        "\n",
        "                model_input = f\"\"\"\n",
        "Schema:\n",
        "{schema_text}\n",
        "\n",
        "Question:\n",
        "{q}\n",
        "\"\"\"\n",
        "\n",
        "                data.append({\n",
        "                    \"input\": model_input.strip(),\n",
        "                    \"output\": sql.strip()\n",
        "                })\n",
        "\n",
        "        return data\n",
        "\n",
        "    return build_examples(train_s), build_examples(test_s)\n",
        "\n",
        "\n",
        "train, test = build_dataset_by_schema()\n",
        "\n",
        "print(\"Train:\", len(train))\n",
        "print(\"Test:\", len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eFqq1tIGfzb",
        "outputId": "32a65b02-4029-45ad-b5bf-dcc2c6fd64d0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 86400\n",
            "Test: 9600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"train.json\", \"w\") as f:\n",
        "    json.dump(train, f)\n",
        "\n",
        "with open(\"test.json\", \"w\") as f:\n",
        "    json.dump(test, f)\n",
        "\n",
        "print(\"✅ Dataset saved as JSON\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdjAeBQmWaxB",
        "outputId": "c2689713-e013-4441-d9fa-849f0b5db6d4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset saved as JSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(data):\n",
        "\n",
        "    counter = Counter()\n",
        "\n",
        "    for row in data:\n",
        "        counter.update(row[\"input\"].split())\n",
        "        counter.update(row[\"output\"].split())\n",
        "\n",
        "    vocab = {w:i+2 for i,(w,_) in enumerate(counter.items())}\n",
        "    vocab[\"<pad>\"] = 0\n",
        "    vocab[\"<unk>\"] = 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train)\n",
        "\n",
        "torch.save(vocab, \"vocab.pt\")"
      ],
      "metadata": {
        "id": "ijPR7psAGf16"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 220\n",
        "\n",
        "def encode(text):\n",
        "\n",
        "    tokens = text.split()\n",
        "\n",
        "    ids = [vocab.get(t,1) for t in tokens][:MAX_LEN]\n",
        "\n",
        "    ids += [0]*(MAX_LEN-len(ids))\n",
        "\n",
        "    return torch.tensor(ids)\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "\n",
        "    def __init__(self,data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        row = self.data[idx]\n",
        "\n",
        "        return encode(row[\"input\"]), encode(row[\"output\"])"
      ],
      "metadata": {
        "id": "-ECMgN75Gf4i"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"train.json\") as f:\n",
        "    train = json.load(f)\n",
        "\n",
        "with open(\"test.json\") as f:\n",
        "    test = json.load(f)\n",
        "\n",
        "print(\"✅ Dataset loaded!\")\n",
        "print(\"Train size:\", len(train))\n"
      ],
      "metadata": {
        "id": "yIyTYqmBZYHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NL2SQLModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=8,\n",
        "            num_encoder_layers=3,\n",
        "            num_decoder_layers=3,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "\n",
        "        src_padding_mask = (src == 0)\n",
        "        tgt_padding_mask = (tgt == 0)\n",
        "\n",
        "        # ⭐⭐⭐ CAUSAL MASK (VERY IMPORTANT)\n",
        "        tgt_seq_len = tgt.size(1)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "            tgt_seq_len\n",
        "        ).to(tgt.device)\n",
        "\n",
        "        src = self.embed(src)\n",
        "        tgt = self.embed(tgt)\n",
        "\n",
        "        out = self.transformer(\n",
        "            src,\n",
        "            tgt,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask\n",
        "        )\n",
        "\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "C-TVvOjHGf61"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "    print(train[i][\"input\"])\n",
        "    print(train[i][\"output\"])\n",
        "    print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vv8PWShLVwS",
        "outputId": "f5de0370-eae5-457e-ced3-12a63eadb2e6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema:\n",
            "[TABLE] continue [COL] id [COL] price [COL] email [TABLE] our [COL] id [COL] age [COL] quantity [COL] price [COL] name [COL] country [COL] continue_id [REL] our.continue_id -> continue.id\n",
            "\n",
            "Question:\n",
            "show price from continue joined with our where quantity > 64\n",
            "SELECT continue.price\n",
            "FROM continue\n",
            "JOIN our\n",
            "ON continue.id = our.continue_id\n",
            "WHERE our.quantity > 64\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] continue [COL] id [COL] price [COL] email [TABLE] our [COL] id [COL] age [COL] quantity [COL] price [COL] name [COL] country [COL] continue_id [REL] our.continue_id -> continue.id\n",
            "\n",
            "Question:\n",
            "show top 10 records from our ordered by quantity\n",
            "SELECT *\n",
            "FROM our\n",
            "ORDER BY quantity DESC\n",
            "LIMIT 10\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] continue [COL] id [COL] price [COL] email [TABLE] our [COL] id [COL] age [COL] quantity [COL] price [COL] name [COL] country [COL] continue_id [REL] our.continue_id -> continue.id\n",
            "\n",
            "Question:\n",
            "count of quantity grouped by name\n",
            "SELECT name, COUNT(quantity)\n",
            "FROM our\n",
            "GROUP BY name\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] continue [COL] id [COL] price [COL] email [TABLE] our [COL] id [COL] age [COL] quantity [COL] price [COL] name [COL] country [COL] continue_id [REL] our.continue_id -> continue.id\n",
            "\n",
            "Question:\n",
            "show top 4 records from our ordered by age\n",
            "SELECT *\n",
            "FROM our\n",
            "ORDER BY age ASC\n",
            "LIMIT 4\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] continue [COL] id [COL] price [COL] email [TABLE] our [COL] id [COL] age [COL] quantity [COL] price [COL] name [COL] country [COL] continue_id [REL] our.continue_id -> continue.id\n",
            "\n",
            "Question:\n",
            "show price from continue where price < 39\n",
            "SELECT price FROM continue WHERE price < 39\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] continue [COL] id [COL] price [COL] email [TABLE] our [COL] id [COL] age [COL] quantity [COL] price [COL] name [COL] country [COL] continue_id [REL] our.continue_id -> continue.id\n",
            "\n",
            "Question:\n",
            "show top 14 records from continue ordered by price\n",
            "SELECT *\n",
            "FROM continue\n",
            "ORDER BY price DESC\n",
            "LIMIT 14\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] continue [COL] id [COL] price [COL] email [TABLE] our [COL] id [COL] age [COL] quantity [COL] price [COL] name [COL] country [COL] continue_id [REL] our.continue_id -> continue.id\n",
            "\n",
            "Question:\n",
            "show id from our joined with continue\n",
            "SELECT our.id\n",
            "FROM our\n",
            "JOIN continue\n",
            "ON our.continue_id = continue.id\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] continue [COL] id [COL] price [COL] email [TABLE] our [COL] id [COL] age [COL] quantity [COL] price [COL] name [COL] country [COL] continue_id [REL] our.continue_id -> continue.id\n",
            "\n",
            "Question:\n",
            "min of price grouped by email\n",
            "SELECT email, MIN(price)\n",
            "FROM continue\n",
            "GROUP BY email\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] poor [COL] id [COL] quantity [COL] name [COL] email [COL] department [COL] created_at [TABLE] senior [COL] id [COL] quantity [COL] price [COL] email [COL] updated_at [COL] poor_id [TABLE] once [COL] id [COL] salary [COL] quantity [COL] name [COL] country [COL] created_at [COL] senior_id [REL] senior.poor_id -> poor.id [REL] once.senior_id -> senior.id\n",
            "\n",
            "Question:\n",
            "show department from poor joined with senior where price = 54\n",
            "SELECT poor.department\n",
            "FROM poor\n",
            "JOIN senior\n",
            "ON poor.id = senior.poor_id\n",
            "WHERE senior.price = 54\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] poor [COL] id [COL] quantity [COL] name [COL] email [COL] department [COL] created_at [TABLE] senior [COL] id [COL] quantity [COL] price [COL] email [COL] updated_at [COL] poor_id [TABLE] once [COL] id [COL] salary [COL] quantity [COL] name [COL] country [COL] created_at [COL] senior_id [REL] senior.poor_id -> poor.id [REL] once.senior_id -> senior.id\n",
            "\n",
            "Question:\n",
            "show all records from senior ordered by price desc\n",
            "SELECT *\n",
            "        FROM senior\n",
            "        ORDER BY price DESC\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] poor [COL] id [COL] quantity [COL] name [COL] email [COL] department [COL] created_at [TABLE] senior [COL] id [COL] quantity [COL] price [COL] email [COL] updated_at [COL] poor_id [TABLE] once [COL] id [COL] salary [COL] quantity [COL] name [COL] country [COL] created_at [COL] senior_id [REL] senior.poor_id -> poor.id [REL] once.senior_id -> senior.id\n",
            "\n",
            "Question:\n",
            "show top 9 records from senior ordered by quantity\n",
            "SELECT *\n",
            "FROM senior\n",
            "ORDER BY quantity DESC\n",
            "LIMIT 9\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] poor [COL] id [COL] quantity [COL] name [COL] email [COL] department [COL] created_at [TABLE] senior [COL] id [COL] quantity [COL] price [COL] email [COL] updated_at [COL] poor_id [TABLE] once [COL] id [COL] salary [COL] quantity [COL] name [COL] country [COL] created_at [COL] senior_id [REL] senior.poor_id -> poor.id [REL] once.senior_id -> senior.id\n",
            "\n",
            "Question:\n",
            "show id from poor joined with senior\n",
            "SELECT poor.id\n",
            "FROM poor\n",
            "JOIN senior\n",
            "ON poor.id = senior.poor_id\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] poor [COL] id [COL] quantity [COL] name [COL] email [COL] department [COL] created_at [TABLE] senior [COL] id [COL] quantity [COL] price [COL] email [COL] updated_at [COL] poor_id [TABLE] once [COL] id [COL] salary [COL] quantity [COL] name [COL] country [COL] created_at [COL] senior_id [REL] senior.poor_id -> poor.id [REL] once.senior_id -> senior.id\n",
            "\n",
            "Question:\n",
            "min of price grouped by email having value > 49\n",
            "SELECT email, MIN(price)\n",
            "FROM senior\n",
            "GROUP BY email\n",
            "HAVING MIN(price) > 49\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] poor [COL] id [COL] quantity [COL] name [COL] email [COL] department [COL] created_at [TABLE] senior [COL] id [COL] quantity [COL] price [COL] email [COL] updated_at [COL] poor_id [TABLE] once [COL] id [COL] salary [COL] quantity [COL] name [COL] country [COL] created_at [COL] senior_id [REL] senior.poor_id -> poor.id [REL] once.senior_id -> senior.id\n",
            "\n",
            "Question:\n",
            "show quantity from poor where quantity > 70\n",
            "SELECT quantity FROM poor WHERE quantity > 70\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] poor [COL] id [COL] quantity [COL] name [COL] email [COL] department [COL] created_at [TABLE] senior [COL] id [COL] quantity [COL] price [COL] email [COL] updated_at [COL] poor_id [TABLE] once [COL] id [COL] salary [COL] quantity [COL] name [COL] country [COL] created_at [COL] senior_id [REL] senior.poor_id -> poor.id [REL] once.senior_id -> senior.id\n",
            "\n",
            "Question:\n",
            "show department from poor joined with senior\n",
            "SELECT poor.department\n",
            "FROM poor\n",
            "JOIN senior\n",
            "ON poor.id = senior.poor_id\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] poor [COL] id [COL] quantity [COL] name [COL] email [COL] department [COL] created_at [TABLE] senior [COL] id [COL] quantity [COL] price [COL] email [COL] updated_at [COL] poor_id [TABLE] once [COL] id [COL] salary [COL] quantity [COL] name [COL] country [COL] created_at [COL] senior_id [REL] senior.poor_id -> poor.id [REL] once.senior_id -> senior.id\n",
            "\n",
            "Question:\n",
            "what is the sum of quantity in senior\n",
            "SELECT SUM(quantity) FROM senior\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] modern [COL] id [COL] quantity [COL] price [COL] city [TABLE] worker [COL] id [COL] age [COL] price [COL] country [COL] department [COL] city [COL] modern_id [TABLE] speak [COL] id [COL] age [COL] quantity [COL] price [COL] department [COL] worker_id [TABLE] actually [COL] id [COL] age [COL] department [COL] city [COL] created_at [COL] speak_id [REL] worker.modern_id -> modern.id [REL] speak.worker_id -> worker.id [REL] actually.speak_id -> speak.id\n",
            "\n",
            "Question:\n",
            "show top 3 records from actually ordered by age\n",
            "SELECT *\n",
            "FROM actually\n",
            "ORDER BY age DESC\n",
            "LIMIT 3\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] modern [COL] id [COL] quantity [COL] price [COL] city [TABLE] worker [COL] id [COL] age [COL] price [COL] country [COL] department [COL] city [COL] modern_id [TABLE] speak [COL] id [COL] age [COL] quantity [COL] price [COL] department [COL] worker_id [TABLE] actually [COL] id [COL] age [COL] department [COL] city [COL] created_at [COL] speak_id [REL] worker.modern_id -> modern.id [REL] speak.worker_id -> worker.id [REL] actually.speak_id -> speak.id\n",
            "\n",
            "Question:\n",
            "show quantity, id, price from speak\n",
            "SELECT quantity, id, price FROM speak\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] modern [COL] id [COL] quantity [COL] price [COL] city [TABLE] worker [COL] id [COL] age [COL] price [COL] country [COL] department [COL] city [COL] modern_id [TABLE] speak [COL] id [COL] age [COL] quantity [COL] price [COL] department [COL] worker_id [TABLE] actually [COL] id [COL] age [COL] department [COL] city [COL] created_at [COL] speak_id [REL] worker.modern_id -> modern.id [REL] speak.worker_id -> worker.id [REL] actually.speak_id -> speak.id\n",
            "\n",
            "Question:\n",
            "show price, department from speak\n",
            "SELECT price, department FROM speak\n",
            "-----\n",
            "Schema:\n",
            "[TABLE] modern [COL] id [COL] quantity [COL] price [COL] city [TABLE] worker [COL] id [COL] age [COL] price [COL] country [COL] department [COL] city [COL] modern_id [TABLE] speak [COL] id [COL] age [COL] quantity [COL] price [COL] department [COL] worker_id [TABLE] actually [COL] id [COL] age [COL] department [COL] city [COL] created_at [COL] speak_id [REL] worker.modern_id -> modern.id [REL] speak.worker_id -> worker.id [REL] actually.speak_id -> speak.id\n",
            "\n",
            "Question:\n",
            "max of quantity grouped by city\n",
            "SELECT city, MAX(quantity)\n",
            "FROM modern\n",
            "GROUP BY city\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    NL2SQLDataset(train),\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    NL2SQLDataset(test),\n",
        "    batch_size=64,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model = NL2SQLModel(len(vocab)).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "metadata": {
        "id": "5PESpD8FGf9I"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"nl2sql_checkpoint.pt\"\n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, loss):\n",
        "\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"loss\": loss\n",
        "    }, CHECKPOINT_PATH)\n",
        "\n",
        "def load_checkpoint(model, optimizer):\n",
        "\n",
        "    if os.path.exists(CHECKPOINT_PATH):\n",
        "\n",
        "        ckpt = torch.load(CHECKPOINT_PATH,\n",
        "                          map_location=device)\n",
        "\n",
        "        model.load_state_dict(ckpt[\"model_state\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "\n",
        "        print(\"Resuming from epoch:\",\n",
        "              ckpt[\"epoch\"]+1)\n",
        "\n",
        "        return ckpt[\"epoch\"] + 1\n",
        "\n",
        "    return 0"
      ],
      "metadata": {
        "id": "pi6TnwL9Gf_d"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 15\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "start_epoch = load_checkpoint(model, optimizer)\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "\n",
        "    ###################\n",
        "    # TRAIN\n",
        "    ###################\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for x,y in train_loader:\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(x, y[:,:-1])\n",
        "\n",
        "        loss = loss_fn(\n",
        "            output.reshape(-1, len(vocab)),\n",
        "            y[:,1:].reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "\n",
        "    ###################\n",
        "    # VALIDATION\n",
        "    ###################\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for x,y in val_loader:\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            output = model(x, y[:,:-1])\n",
        "\n",
        "            loss = loss_fn(\n",
        "                output.reshape(-1, len(vocab)),\n",
        "                y[:,1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch} | Train Loss {train_loss:.4f} | Val Loss {val_loss:.4f}\")\n",
        "\n",
        "    save_checkpoint(epoch, model, optimizer, val_loss)\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(\"⭐ BEST MODEL SAVED\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d62paSTGgB6",
        "outputId": "b4ce45b3-84f7-480b-d7bc-411d4c04262b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(\n",
        "    NL2SQLDataset(test),\n",
        "    batch_size=64,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for x,y in test_loader:\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        output = model(x, y[:,:-1])\n",
        "\n",
        "        loss = loss_fn(\n",
        "            output.reshape(-1, len(vocab)),\n",
        "            y[:,1:].reshape(-1)\n",
        "        )\n",
        "\n",
        "        test_loss += loss.item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "print(\"✅ FINAL TEST LOSS:\", test_loss)\n"
      ],
      "metadata": {
        "id": "G10AQ6FzgRFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- LOAD VOCAB ----------\n",
        "vocab = torch.load(\"vocab.pt\")\n",
        "inv_vocab = {i:w for w,i in vocab.items()}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# ---------- REBUILD MODEL ----------\n",
        "model = NL2SQLModel(len(vocab)).to(device)\n",
        "\n",
        "model.load_state_dict(\n",
        "    torch.load(\"best_model.pt\", map_location=device)\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Best model loaded!\")\n"
      ],
      "metadata": {
        "id": "88bvEFs3GgET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 220\n",
        "\n",
        "def encode(text):\n",
        "\n",
        "    tokens = text.split()\n",
        "\n",
        "    ids = [vocab.get(t,1) for t in tokens][:MAX_LEN]\n",
        "\n",
        "    ids += [0]*(MAX_LEN-len(ids))\n",
        "\n",
        "    return torch.tensor(ids)\n"
      ],
      "metadata": {
        "id": "5cnMD7NEGgIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_query(model, text, max_len=220):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    src = encode(text).unsqueeze(0).to(device)\n",
        "\n",
        "    tgt = torch.zeros((1,1), dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "            tgt.size(1)\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            output = model.transformer(\n",
        "                model.embed(src),\n",
        "                model.embed(tgt),\n",
        "                tgt_mask=tgt_mask,\n",
        "                src_key_padding_mask=(src==0),\n",
        "                tgt_key_padding_mask=(tgt==0)\n",
        "            )\n",
        "\n",
        "            logits = model.fc(output)\n",
        "\n",
        "        next_token = logits.argmax(-1)[:,-1].unsqueeze(0)\n",
        "\n",
        "        tgt = torch.cat([tgt, next_token], dim=1)\n",
        "\n",
        "        if next_token.item() == 0:\n",
        "            break\n",
        "\n",
        "\n",
        "    tokens = [\n",
        "        inv_vocab.get(i,\"\")\n",
        "        for i in tgt.squeeze().tolist()\n",
        "    ]\n",
        "\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "YeGVvn7RIgUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbZc46UCIgbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJ4cwf1OIges"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}