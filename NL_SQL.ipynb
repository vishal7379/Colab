{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/NL_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker sqlparse\n",
        "\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from faker import Faker\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uKTfHehe_V_",
        "outputId": "9503a13a-e226-48be-ddfd-ec2b96d89e15"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.12/dist-packages (40.4.0)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.12/dist-packages (0.5.5)\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SQL_PATTERNS = (\n",
        "    [\"join\"] * 6 +          # MOST important\n",
        "    [\"group_by\"] * 3 +\n",
        "    [\"having\"] * 2 +\n",
        "    [\"where\"] * 2 +\n",
        "    [\"aggregation\"] * 2 +\n",
        "    [\"multi_select\"] * 1 +\n",
        "    [\"simple_select\"] * 1\n",
        ")"
      ],
      "metadata": {
        "id": "BHbGaOAie_Yv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COLUMN_POOL = [\n",
        "    \"id\",\"user_id\",\"order_id\",\"product_id\",\n",
        "    \"name\",\"email\",\"age\",\"salary\",\n",
        "    \"department\",\"city\",\"country\",\n",
        "    \"price\",\"amount\",\"quantity\",\n",
        "    \"created_at\",\"updated_at\"\n",
        "]"
      ],
      "metadata": {
        "id": "eCNnZ31ve_a9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_schema():\n",
        "\n",
        "    num_tables = random.randint(2,6)\n",
        "    schema = {}\n",
        "\n",
        "    fake.unique.clear()\n",
        "\n",
        "    for _ in range(num_tables):\n",
        "\n",
        "        table = fake.unique.word()\n",
        "\n",
        "        cols = random.sample(\n",
        "            COLUMN_POOL,\n",
        "            random.randint(4,10)\n",
        "        )\n",
        "\n",
        "        if \"id\" not in cols:\n",
        "            cols.append(\"id\")\n",
        "\n",
        "        schema[table] = cols\n",
        "\n",
        "    return schema"
      ],
      "metadata": {
        "id": "9B7rbHkSe_dV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def schema_to_text(schema):\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    for table, cols in schema.items():\n",
        "\n",
        "        col_tokens = \" \".join([f\"[COL] {c}\" for c in cols])\n",
        "        parts.append(f\"[TABLE] {table} {col_tokens}\")\n",
        "\n",
        "    return \" \".join(parts)"
      ],
      "metadata": {
        "id": "c_Hg5tAXfTkl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT_TEMPLATES = [\n",
        "    \"show {col} from {table}\",\n",
        "    \"list {col} in {table}\",\n",
        "    \"display {col} from {table}\",\n",
        "    \"what are the {col} in {table}\",\n",
        "    \"give me the {col} of {table}\"\n",
        "]"
      ],
      "metadata": {
        "id": "2OPVGN22fTnL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sql(schema):\n",
        "\n",
        "    pattern = random.choice(SQL_PATTERNS)\n",
        "\n",
        "    table = random.choice(list(schema.keys()))\n",
        "    cols = schema[table]\n",
        "    col1 = random.choice(cols)\n",
        "\n",
        "    # SIMPLE\n",
        "    if pattern == \"simple_select\":\n",
        "\n",
        "        question = random.choice(SELECT_TEMPLATES).format(\n",
        "            col=col1, table=table\n",
        "        )\n",
        "\n",
        "        sql = f\"SELECT {col1} FROM {table}\"\n",
        "\n",
        "    # WHERE\n",
        "    elif pattern == \"where\":\n",
        "\n",
        "        question = f\"{random.choice(SELECT_TEMPLATES).format(col=col1, table=table)} where {col1} is not null\"\n",
        "\n",
        "        sql = f\"SELECT {col1} FROM {table} WHERE {col1} IS NOT NULL\"\n",
        "\n",
        "    # MULTI\n",
        "    elif pattern == \"multi_select\":\n",
        "\n",
        "        selected = random.sample(cols, min(2,len(cols)))\n",
        "\n",
        "        question = f\"show {', '.join(selected)} from {table}\"\n",
        "\n",
        "        sql = f\"SELECT {', '.join(selected)} FROM {table}\"\n",
        "\n",
        "    # AGG\n",
        "    elif pattern == \"aggregation\":\n",
        "\n",
        "        question = f\"what is the average {col1} in {table}\"\n",
        "\n",
        "        sql = f\"SELECT AVG({col1}) FROM {table}\"\n",
        "\n",
        "    # GROUP\n",
        "    elif pattern == \"group_by\":\n",
        "\n",
        "        col2 = random.choice(cols)\n",
        "\n",
        "        question = f\"count {col1} grouped by {col2} in {table}\"\n",
        "\n",
        "        sql = f\"SELECT {col2}, COUNT({col1}) FROM {table} GROUP BY {col2}\"\n",
        "\n",
        "    # HAVING\n",
        "    elif pattern == \"having\":\n",
        "\n",
        "        col2 = random.choice(cols)\n",
        "\n",
        "        question = f\"show {col2} having count of {col1} greater than 5\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "SELECT {col2}, COUNT({col1})\n",
        "FROM {table}\n",
        "GROUP BY {col2}\n",
        "HAVING COUNT({col1}) > 5\n",
        "\"\"\"\n",
        "\n",
        "    # JOIN (Most Important)\n",
        "    elif pattern == \"join\":\n",
        "\n",
        "        tables = list(schema.keys())\n",
        "\n",
        "        if len(tables) < 2:\n",
        "            return generate_sql(schema)\n",
        "\n",
        "        t2 = random.choice([t for t in tables if t != table])\n",
        "\n",
        "        if \"id\" not in schema[table] or \"id\" not in schema[t2]:\n",
        "            return generate_sql(schema)\n",
        "\n",
        "        question = f\"join {table} and {t2} and show {table}.{col1}\"\n",
        "\n",
        "        sql = f\"\"\"\n",
        "SELECT {table}.{col1}\n",
        "FROM {table}\n",
        "JOIN {t2}\n",
        "ON {table}.id = {t2}.id\n",
        "\"\"\"\n",
        "\n",
        "    return question.strip(), sql.strip()"
      ],
      "metadata": {
        "id": "vZvcB-ZPfTph"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset_by_schema(num_schemas=10000, queries_per_schema=6):\n",
        "\n",
        "    schemas = [generate_schema() for _ in range(num_schemas)]\n",
        "\n",
        "    split = int(0.9 * num_schemas)\n",
        "\n",
        "    train_schemas = schemas[:split]\n",
        "    test_schemas = schemas[split:]\n",
        "\n",
        "    def build_examples(schema_list):\n",
        "\n",
        "        data = []\n",
        "\n",
        "        for schema in schema_list:\n",
        "\n",
        "            schema_text = schema_to_text(schema)\n",
        "\n",
        "            for _ in range(queries_per_schema):\n",
        "\n",
        "                question, sql = generate_sql(schema)\n",
        "\n",
        "                model_input = f\"\"\"\n",
        "Schema:\n",
        "{schema_text}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "                data.append({\n",
        "                    \"input\": model_input.strip(),\n",
        "                    \"output\": sql.strip()\n",
        "                })\n",
        "\n",
        "        return data\n",
        "\n",
        "    return build_examples(train_schemas), build_examples(test_schemas)\n",
        "\n",
        "\n",
        "train, test = build_dataset_by_schema()\n",
        "\n",
        "print(\"Train size:\", len(train))\n",
        "print(\"Test size:\", len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIggsSuAfTrk",
        "outputId": "32e30a73-9756-4120-d2a9-ff60fdcd86c1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 54000\n",
            "Test size: 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(data):\n",
        "\n",
        "    counter = Counter()\n",
        "\n",
        "    for row in data:\n",
        "        counter.update(row[\"input\"].split())\n",
        "        counter.update(row[\"output\"].split())\n",
        "\n",
        "    vocab = {w:i+2 for i,(w,_) in enumerate(counter.items())}\n",
        "    vocab[\"<pad>\"] = 0\n",
        "    vocab[\"<unk>\"] = 1\n",
        "\n",
        "    inv_vocab = {i:w for w,i in vocab.items()}\n",
        "\n",
        "    return vocab, inv_vocab\n",
        "\n",
        "vocab, inv_vocab = build_vocab(train)\n",
        "\n",
        "torch.save(vocab, \"vocab.pt\")"
      ],
      "metadata": {
        "id": "-1Lh0xHXfTuF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 160\n",
        "\n",
        "def encode(text):\n",
        "    tokens = text.split()\n",
        "    ids = [vocab.get(t,1) for t in tokens][:MAX_LEN]\n",
        "    ids += [0]*(MAX_LEN-len(ids))\n",
        "    return torch.tensor(ids)\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "\n",
        "    def __init__(self,data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        row = self.data[idx]\n",
        "        return encode(row[\"input\"]), encode(row[\"output\"])"
      ],
      "metadata": {
        "id": "DYlR9BhyfTwy"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NL2SQLModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=8,\n",
        "            num_encoder_layers=3,\n",
        "            num_decoder_layers=3,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "\n",
        "        src_mask = (src == 0)\n",
        "        tgt_mask = (tgt == 0)\n",
        "\n",
        "        src = self.embed(src)\n",
        "        tgt = self.embed(tgt)\n",
        "\n",
        "        out = self.transformer(\n",
        "            src,\n",
        "            tgt,\n",
        "            src_key_padding_mask=src_mask,\n",
        "            tgt_key_padding_mask=tgt_mask\n",
        "        )\n",
        "\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "g7uXv0rQfT0H"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    NL2SQLDataset(train),\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "model = NL2SQLModel(len(vocab)).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "metadata": {
        "id": "549eZRpggPTY"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"nl2sql_checkpoint.pt\"\n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, loss):\n",
        "\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"loss\": loss\n",
        "    }, CHECKPOINT_PATH)\n",
        "\n",
        "def load_checkpoint(model, optimizer):\n",
        "\n",
        "    if os.path.exists(CHECKPOINT_PATH):\n",
        "\n",
        "        ckpt = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "\n",
        "        model.load_state_dict(ckpt[\"model_state\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "\n",
        "        print(\"Resuming from epoch:\", ckpt[\"epoch\"]+1)\n",
        "        return ckpt[\"epoch\"] + 1\n",
        "\n",
        "    return 0"
      ],
      "metadata": {
        "id": "xyzEuJ2se_gr"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "start_epoch = load_checkpoint(model, optimizer)\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x,y in train_loader:\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(x, y[:,:-1])\n",
        "\n",
        "        loss = loss_fn(\n",
        "            output.reshape(-1, len(vocab)),\n",
        "            y[:,1:].reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch} | Loss {avg_loss}\")\n",
        "\n",
        "    save_checkpoint(epoch, model, optimizer, avg_loss)\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(\"⭐ BEST MODEL SAVED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esej7cwSivbU",
        "outputId": "37bfde57-2320-450f-9f62-cca3993602c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss 1.9909309304156009\n",
            "⭐ BEST MODEL SAVED\n",
            "Epoch 1 | Loss 0.7649803956026008\n",
            "⭐ BEST MODEL SAVED\n",
            "Epoch 2 | Loss 0.6061771394711394\n",
            "⭐ BEST MODEL SAVED\n",
            "Epoch 3 | Loss 0.4908064574144463\n",
            "⭐ BEST MODEL SAVED\n",
            "Epoch 4 | Loss 0.40722163312878656\n",
            "⭐ BEST MODEL SAVED\n",
            "Epoch 5 | Loss 0.35079582590804\n",
            "⭐ BEST MODEL SAVED\n",
            "Epoch 6 | Loss 0.311383879015231\n",
            "⭐ BEST MODEL SAVED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yU5c_Ibivd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wT5qJWL2ivgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rppsiP5eivh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4m12ri_uivlT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}