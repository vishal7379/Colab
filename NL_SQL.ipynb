{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMenq+5v/OX4SY1Lf85dIC9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal7379/Colab/blob/main/NL_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "# ---------------- SCHEMAS ----------------\n",
        "SCHEMAS = [\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"employees\": [\"id\", \"name\", \"salary\", \"dept_id\"],\n",
        "            \"departments\": [\"id\", \"name\"]\n",
        "        },\n",
        "        \"numeric\": {\n",
        "            \"employees\": [\"id\", \"salary\", \"dept_id\"]\n",
        "        },\n",
        "        \"text\": {\n",
        "            \"employees\": [\"name\"]\n",
        "        },\n",
        "        \"join\": (\"employees\", \"departments\", \"dept_id\", \"id\")\n",
        "    },\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"students\": [\"id\", \"name\", \"marks\", \"class_id\"],\n",
        "            \"classes\": [\"id\", \"name\"]\n",
        "        },\n",
        "        \"numeric\": {\n",
        "            \"students\": [\"id\", \"marks\", \"class_id\"]\n",
        "        },\n",
        "        \"text\": {\n",
        "            \"students\": [\"name\"]\n",
        "        },\n",
        "        \"join\": (\"students\", \"classes\", \"class_id\", \"id\")\n",
        "    }\n",
        "]\n",
        "\n",
        "AGGS = [\"COUNT\", \"SUM\", \"AVG\", \"MAX\", \"MIN\"]\n",
        "\n",
        "# ---------------- GENERATOR ----------------\n",
        "def generate_example():\n",
        "    db = random.choice(SCHEMAS)\n",
        "    schema = db[\"tables\"]\n",
        "\n",
        "    main_table = list(schema.keys())[0]\n",
        "    cols = schema[main_table]\n",
        "\n",
        "    intent = random.choices(\n",
        "        [\"SELECT\", \"AGG\", \"WHERE\", \"GROUP\", \"JOIN\"],\n",
        "        weights=[0.30, 0.25, 0.20, 0.15, 0.10]\n",
        "    )[0]\n",
        "\n",
        "    # ---------- SELECT ----------\n",
        "    if intent == \"SELECT\":\n",
        "        col = random.choice(cols)\n",
        "        question = random.choice([\n",
        "            f\"show {col} of {main_table}\",\n",
        "            f\"get {col} from {main_table}\",\n",
        "            f\"list {col} in {main_table}\",\n",
        "            f\"display {col} for {main_table}\"\n",
        "        ])\n",
        "        sql = f\"SELECT {col} FROM {main_table}\"\n",
        "\n",
        "    # ---------- AGGREGATION ----------\n",
        "    elif intent == \"AGG\":\n",
        "        agg = random.choice(AGGS)\n",
        "\n",
        "        if agg == \"COUNT\":\n",
        "            col = random.choice(cols)\n",
        "        else:\n",
        "            col = random.choice(db[\"numeric\"][main_table])\n",
        "\n",
        "        question = random.choice([\n",
        "            f\"show {agg.lower()} of {col} of {main_table}\",\n",
        "            f\"what is the {agg.lower()} {col} in {main_table}\",\n",
        "            f\"give me the {agg.lower()} {col} from {main_table}\"\n",
        "        ])\n",
        "        sql = f\"SELECT {agg}({col}) FROM {main_table}\"\n",
        "\n",
        "    # ---------- WHERE ----------\n",
        "    elif intent == \"WHERE\":\n",
        "        col = random.choice(db[\"numeric\"][main_table])\n",
        "        val = random.choice([10, 20, 50, 100])\n",
        "        question = random.choice([\n",
        "            f\"show {col} of {main_table} where {col} > {val}\",\n",
        "            f\"list {main_table} with {col} greater than {val}\",\n",
        "            f\"get {col} from {main_table} having {col} above {val}\"\n",
        "        ])\n",
        "        sql = f\"SELECT {col} FROM {main_table} WHERE {col} > {val}\"\n",
        "\n",
        "    # ---------- GROUP BY ----------\n",
        "    elif intent == \"GROUP\":\n",
        "        agg = random.choice([\"COUNT\", \"AVG\"])\n",
        "        group_col = random.choice(db[\"text\"][main_table])\n",
        "        num_col = random.choice(db[\"numeric\"][main_table])\n",
        "\n",
        "        question = random.choice([\n",
        "            f\"show {agg.lower()} of {num_col} per {group_col}\",\n",
        "            f\"get {agg.lower()} {num_col} grouped by {group_col}\",\n",
        "            f\"list {group_col} wise {agg.lower()} {num_col}\"\n",
        "        ])\n",
        "        sql = (\n",
        "            f\"SELECT {group_col}, {agg}({num_col}) \"\n",
        "            f\"FROM {main_table} GROUP BY {group_col}\"\n",
        "        )\n",
        "\n",
        "    # ---------- JOIN ----------\n",
        "    else:\n",
        "        t1, t2, c1, c2 = db[\"join\"]\n",
        "        question = random.choice([\n",
        "            f\"show {t1} name and {t2} name\",\n",
        "            f\"get {t1} names with their {t2}\",\n",
        "            f\"list {t1} and corresponding {t2}\"\n",
        "        ])\n",
        "        sql = (\n",
        "            f\"SELECT {t1}.name, {t2}.name \"\n",
        "            f\"FROM {t1} JOIN {t2} ON {t1}.{c1} = {t2}.{c2}\"\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"schema\": schema,\n",
        "        \"sql\": sql\n",
        "    }\n",
        "\n",
        "# ---------------- DATASET CREATION ----------------\n",
        "def generate_dataset(n=80_000, outfile=\"nl2sql_varied.json\"):\n",
        "    data = []\n",
        "    for i in range(n):\n",
        "        data.append(generate_example())\n",
        "        if (i + 1) % 20_000 == 0:\n",
        "            print(f\"Generated {i+1} examples\")\n",
        "\n",
        "    with open(outfile, \"w\") as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    print(\"âœ… Dataset size:\", len(data))\n",
        "\n",
        "\n",
        "# ðŸ”¥ CHANGE n TO 300_000 IF YOU WANT\n",
        "generate_dataset(n=80_000)\n"
      ],
      "metadata": {
        "id": "ZkCMmK9pe6xl",
        "outputId": "bebb5e86-d928-40a7-f442-2c6c999e2961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 20000 examples\n",
            "Generated 40000 examples\n",
            "Generated 60000 examples\n",
            "Generated 80000 examples\n",
            "âœ… Dataset size: 80000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# with open(\"nl2sql_varied.json\", \"r\") as f:\n",
        "#     dataset = json.load(f)\n",
        "\n",
        "# print(\"Loaded dataset size:\", len(dataset))\n",
        "\n",
        "# from collections import Counter\n",
        "\n",
        "# ENC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<SEP>\":2}\n",
        "# DEC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<BOS>\":2, \"<EOS>\":3}\n",
        "\n",
        "\n",
        "# def add(vocab, tok):\n",
        "#     if tok not in vocab:\n",
        "#         vocab[tok] = len(vocab)\n",
        "\n",
        "# for ex in dataset:\n",
        "#     # NL tokens\n",
        "#     for t in ex[\"question\"].lower().split():\n",
        "#         add(ENC_VOCAB, t)\n",
        "\n",
        "#     # Schema tokens\n",
        "#     for t, cols in ex[\"schema\"].items():\n",
        "#         add(ENC_VOCAB, t)\n",
        "#         for c in cols:\n",
        "#             add(ENC_VOCAB, f\"{t}.{c}\")\n",
        "\n",
        "#     # SQL tokens\n",
        "#     for ex in dataset:\n",
        "#       for t in ex[\"sql\"].lower().split():\n",
        "#           if t not in DEC_VOCAB:\n",
        "#               DEC_VOCAB[t] = len(DEC_VOCAB)\n",
        "\n",
        "\n",
        "# print(\"Encoder vocab:\", len(ENC_VOCAB))\n",
        "# print(\"Decoder vocab:\", len(DEC_VOCAB))\n",
        "import json\n",
        "\n",
        "with open(\"nl2sql_varied.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(\"Loaded dataset size:\", len(dataset))\n",
        "\n",
        "ENC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<SEP>\":2}\n",
        "DEC_VOCAB = {\"<PAD>\":0, \"<UNK>\":1, \"<BOS>\":2, \"<EOS>\":3}\n",
        "\n",
        "def add(vocab, tok):\n",
        "    if tok not in vocab:\n",
        "        vocab[tok] = len(vocab)\n",
        "\n",
        "for ex in dataset:\n",
        "    # -------- Encoder vocab --------\n",
        "    for t in ex[\"question\"].lower().split():\n",
        "        add(ENC_VOCAB, t)\n",
        "\n",
        "    for table, cols in ex[\"schema\"].items():\n",
        "        add(ENC_VOCAB, table)\n",
        "        for col in cols:\n",
        "            add(ENC_VOCAB, f\"{table}.{col}\")\n",
        "\n",
        "    # -------- Decoder vocab --------\n",
        "    for t in ex[\"sql\"].lower().split():\n",
        "        add(DEC_VOCAB, t)\n",
        "\n",
        "print(\"Encoder vocab:\", len(ENC_VOCAB))\n",
        "print(\"Decoder vocab:\", len(DEC_VOCAB))\n"
      ],
      "metadata": {
        "id": "cI1fIGF0e606",
        "outputId": "81319a79-03b4-4adf-e3bb-14a7aab53f68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset size: 80000\n",
            "Encoder vocab: 62\n",
            "Decoder vocab: 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "    def __init__(self, data, enc_vocab, dec_vocab,\n",
        "                 max_src_len=80, max_tgt_len=60):\n",
        "        self.data = data\n",
        "        self.enc_vocab = enc_vocab\n",
        "        self.dec_vocab = dec_vocab\n",
        "        self.max_src_len = max_src_len\n",
        "        self.max_tgt_len = max_tgt_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def encode_src(self, tokens):\n",
        "        ids = [self.enc_vocab.get(t, self.enc_vocab[\"<UNK>\"]) for t in tokens]\n",
        "        ids = ids[:self.max_src_len]\n",
        "        pad_len = self.max_src_len - len(ids)\n",
        "        return ids + [self.enc_vocab[\"<PAD>\"]] * pad_len\n",
        "\n",
        "    def encode_tgt(self, tokens):\n",
        "        ids = [self.dec_vocab[\"<BOS>\"]] + \\\n",
        "              [self.dec_vocab.get(t, self.dec_vocab[\"<UNK>\"]) for t in tokens] + \\\n",
        "              [self.dec_vocab[\"<EOS>\"]]\n",
        "\n",
        "        ids = ids[:self.max_tgt_len]\n",
        "        pad_len = self.max_tgt_len - len(ids)\n",
        "        return ids + [self.dec_vocab[\"<PAD>\"]] * pad_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "\n",
        "        # -------- Encoder input (NL + schema) --------\n",
        "        question_tokens = ex[\"question\"].lower().split()\n",
        "\n",
        "        schema_tokens = [\n",
        "            f\"{table}.{col}\"\n",
        "            for table, cols in ex[\"schema\"].items()\n",
        "            for col in cols\n",
        "        ]\n",
        "\n",
        "        src_tokens = question_tokens + [\"<SEP>\"] + schema_tokens\n",
        "        src_ids = self.encode_src(src_tokens)\n",
        "\n",
        "        # -------- Decoder target (SQL) --------\n",
        "        sql_tokens = ex[\"sql\"].lower().split()\n",
        "        tgt_ids = self.encode_tgt(sql_tokens)\n",
        "\n",
        "        return (\n",
        "            torch.tensor(src_ids, dtype=torch.long),\n",
        "            torch.tensor(tgt_ids, dtype=torch.long)\n",
        "        )\n"
      ],
      "metadata": {
        "id": "wI-4VB83e63W"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, dim_ff=1024, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_ff,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(layer, num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T]\n",
        "        pad_mask = (x == 0)        # True where PAD\n",
        "        emb = self.emb(x)\n",
        "        return self.encoder(emb, src_key_padding_mask=pad_mask)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        d_model=256,\n",
        "        nhead=8,\n",
        "        dim_ff=1024,\n",
        "        num_layers=4,\n",
        "        dropout=0.2\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding with dropout\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_ff,\n",
        "            dropout=dropout,          # ðŸ”¥ IMPORTANT\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.TransformerDecoder(layer, num_layers)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, y, memory, memory_pad_mask):\n",
        "        tgt_len = y.size(1)\n",
        "\n",
        "        # Causal mask\n",
        "        causal_mask = torch.triu(\n",
        "            torch.ones(tgt_len, tgt_len, device=y.device),\n",
        "            diagonal=1\n",
        "        ).bool()\n",
        "\n",
        "        emb = self.emb(y)\n",
        "        emb = self.emb_dropout(emb)   # ðŸ”¥ Dropout applied\n",
        "\n",
        "        out = self.decoder(\n",
        "            emb,\n",
        "            memory,\n",
        "            tgt_mask=causal_mask,\n",
        "            memory_key_padding_mask=memory_pad_mask\n",
        "        )\n",
        "\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "jYZJ6FBce662"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize models (PASS vocab sizes)\n",
        "enc = Encoder(vocab_size=len(ENC_VOCAB)).to(device)\n",
        "dec = Decoder(vocab_size=len(DEC_VOCAB)).to(device)\n",
        "\n",
        "# DataLoader (NO collate_fn)\n",
        "loader = DataLoader(\n",
        "    NL2SQLDataset(\n",
        "        data=dataset,\n",
        "        enc_vocab=ENC_VOCAB,\n",
        "        dec_vocab=DEC_VOCAB,\n",
        "        max_src_len=80,\n",
        "        max_tgt_len=60\n",
        "    ),\n",
        "    batch_size=128,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "opt = optim.Adam(\n",
        "    list(enc.parameters()) + list(dec.parameters()),\n",
        "    lr=1e-4\n",
        ")\n",
        "\n",
        "# Loss (ignore PAD in decoder vocab)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=DEC_VOCAB[\"<PAD>\"])\n",
        "\n",
        "print(\"ðŸš€ Training...\")\n",
        "for epoch in range(3):\n",
        "    enc.train()\n",
        "    dec.train()\n",
        "    total = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Encoder\n",
        "        mem = enc(x)\n",
        "        src_pad_mask = (x == 0)\n",
        "\n",
        "        # Decoder (teacher forcing)\n",
        "        out = dec(\n",
        "            y=y[:, :-1],\n",
        "            memory=mem,\n",
        "            memory_pad_mask=src_pad_mask\n",
        "        )\n",
        "\n",
        "        # Loss\n",
        "        loss = loss_fn(\n",
        "            out.reshape(-1, len(DEC_VOCAB)),\n",
        "            y[:, 1:].reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss {total / len(loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "lOT2bgNzfO22",
        "outputId": "30af2205-f9ed-45bb-eb76-a9e3ff183dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training...\n",
            "Epoch 1 | Loss 0.2043\n",
            "Epoch 2 | Loss 0.0037\n",
            "Epoch 3 | Loss 0.0016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "SAVE_PATH = \"nl2sql_transformer.pt\"\n",
        "\n",
        "torch.save({\n",
        "    \"encoder_state_dict\": enc.state_dict(),\n",
        "    \"decoder_state_dict\": dec.state_dict(),\n",
        "    \"ENC_VOCAB\": ENC_VOCAB,\n",
        "    \"DEC_VOCAB\": DEC_VOCAB,\n",
        "    \"model_config\": {\n",
        "        \"d_model\": 256,\n",
        "        \"nhead\": 8,\n",
        "        \"dim_ff\": 1024,\n",
        "        \"num_layers\": 4,\n",
        "        \"dropout\": 0.2\n",
        "    }\n",
        "}, SAVE_PATH)\n",
        "\n",
        "print(f\"âœ… Model successfully saved to {SAVE_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "u9lDQFagCH-V",
        "outputId": "f573a314-1886-49e1-f311-8e1ea1666a5a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'enc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3087773501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m torch.save({\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m\"encoder_state_dict\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"decoder_state_dict\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"ENC_VOCAB\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mENC_VOCAB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'enc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SQL_KEYWORDS = {\"select\", \"from\", \"where\", \"join\", \"group\", \"by\"}\n",
        "\n",
        "def is_valid_next_token(prev_tokens, next_token):\n",
        "    prev_tokens = [t.lower() for t in prev_tokens]\n",
        "    nt = next_token.lower()\n",
        "\n",
        "    # Must start with SELECT\n",
        "    if len(prev_tokens) == 0:\n",
        "        return nt == \"select\"\n",
        "\n",
        "    # FROM cannot come before SELECT\n",
        "    if nt == \"from\" and \"select\" not in prev_tokens:\n",
        "        return False\n",
        "\n",
        "    # WHERE / GROUP cannot come before FROM\n",
        "    if nt in {\"where\", \"group\"} and \"from\" not in prev_tokens:\n",
        "        return False\n",
        "\n",
        "    # JOIN cannot come before FROM\n",
        "    if nt == \"join\" and \"from\" not in prev_tokens:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "inv_dec_vocab = {v: k for k, v in DEC_VOCAB.items()}\n",
        "\n",
        "def infer_sql(question, schema, max_len=50):\n",
        "    enc.eval()\n",
        "    dec.eval()\n",
        "\n",
        "    tokens = question.lower().split() + [\"<SEP>\"] + [\n",
        "        f\"{t}.{c}\" for t, cols in schema.items() for c in cols\n",
        "    ]\n",
        "\n",
        "    x = torch.tensor([\n",
        "        ENC_VOCAB.get(t, ENC_VOCAB[\"<UNK>\"]) for t in tokens\n",
        "    ]).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        memory = enc(x)\n",
        "        memory_pad_mask = (x == ENC_VOCAB[\"<PAD>\"])\n",
        "\n",
        "        y = torch.tensor([[DEC_VOCAB[\"<BOS>\"]]], device=device)\n",
        "        generated_tokens = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            logits = dec(\n",
        "                y=y,\n",
        "                memory=memory,\n",
        "                memory_pad_mask=memory_pad_mask\n",
        "            )\n",
        "\n",
        "            probs = logits[:, -1].softmax(dim=-1)\n",
        "            sorted_ids = torch.argsort(probs, descending=True)\n",
        "\n",
        "            next_token_id = None\n",
        "\n",
        "            # ðŸ”¥ PICARD-style filtering\n",
        "            for tok_id in sorted_ids[0]:\n",
        "                tok = inv_dec_vocab[tok_id.item()]\n",
        "\n",
        "                if tok in {\"<PAD>\", \"<BOS>\"}:\n",
        "                    continue\n",
        "\n",
        "                if is_valid_next_token(generated_tokens, tok):\n",
        "                    next_token_id = tok_id\n",
        "                    break\n",
        "\n",
        "            # Fallback (should rarely happen)\n",
        "            if next_token_id is None:\n",
        "                next_token_id = sorted_ids[0][0]\n",
        "\n",
        "            y = torch.cat([y, next_token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "            generated_tokens.append(inv_dec_vocab[next_token_id.item()])\n",
        "\n",
        "            if next_token_id.item() == DEC_VOCAB[\"<EOS>\"]:\n",
        "                break\n",
        "\n",
        "    return \" \".join(\n",
        "        t for t in generated_tokens\n",
        "        if t not in {\"<EOS>\", \"<PAD>\"}\n",
        "    )\n"
      ],
      "metadata": {
        "id": "3b0CvC5vfUbx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = {\n",
        "    \"employees\": [\"id\",\"name\",\"salary\",\"dept_id\"],\n",
        "    \"departments\": [\"id\",\"name\"]\n",
        "}\n",
        "\n",
        "print(infer_sql(\"give names of employees\", schema))\n",
        "print(infer_sql(\"get count salary grouped by name\", schema))\n",
        "print(infer_sql(\"get marks from students having marks above 20\", schema))\n",
        "print(infer_sql(\"show students name and classes name\", schema))"
      ],
      "metadata": {
        "id": "hV2HyXhvfVJI",
        "outputId": "692b9888-6203-48c3-c11d-9088b30f9f85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "select name from employees\n",
            "select name, count(salary) from employees group by name\n",
            "select marks from employees where marks > 20\n",
            "select students.name, classes.name from employees join classes on students.class_id = departments.id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# NL â†’ SQL Transformer (OVERFITTING FIXED â€“ SINGLE CELL)\n",
        "# ============================================================\n",
        "\n",
        "import random, json, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============================================================\n",
        "# DATASET GENERATION\n",
        "# ============================================================\n",
        "\n",
        "SCHEMAS = [\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"employees\": [\"id\", \"name\", \"salary\", \"dept_id\"],\n",
        "            \"departments\": [\"id\", \"name\"]\n",
        "        },\n",
        "        \"numeric\": {\"employees\": [\"id\", \"salary\", \"dept_id\"]},\n",
        "        \"text\": {\"employees\": [\"name\"]},\n",
        "        \"join\": (\"employees\", \"departments\", \"dept_id\", \"id\")\n",
        "    },\n",
        "    {\n",
        "        \"tables\": {\n",
        "            \"students\": [\"id\", \"name\", \"marks\", \"class_id\"],\n",
        "            \"classes\": [\"id\", \"name\"]\n",
        "        },\n",
        "        \"numeric\": {\"students\": [\"id\", \"marks\", \"class_id\"]},\n",
        "        \"text\": {\"students\": [\"name\"]},\n",
        "        \"join\": (\"students\", \"classes\", \"class_id\", \"id\")\n",
        "    }\n",
        "]\n",
        "\n",
        "AGGS = [\"COUNT\", \"SUM\", \"AVG\", \"MAX\", \"MIN\"]\n",
        "\n",
        "def generate_example():\n",
        "    db = random.choice(SCHEMAS)\n",
        "    schema = db[\"tables\"]\n",
        "    main = list(schema.keys())[0]\n",
        "    cols = schema[main]\n",
        "\n",
        "    intent = random.choices(\n",
        "        [\"SELECT\",\"AGG\",\"WHERE\",\"GROUP\",\"JOIN\"],\n",
        "        weights=[0.30,0.25,0.20,0.15,0.10]\n",
        "    )[0]\n",
        "\n",
        "    if intent == \"SELECT\":\n",
        "        col = random.choice(cols)\n",
        "        q = f\"show {col} of {main}\"\n",
        "        sql = f\"SELECT {col} FROM {main}\"\n",
        "\n",
        "    elif intent == \"AGG\":\n",
        "        agg = random.choice(AGGS)\n",
        "        col = random.choice(db[\"numeric\"][main]) if agg!=\"COUNT\" else random.choice(cols)\n",
        "        q = f\"get {agg.lower()} {col} from {main}\"\n",
        "        sql = f\"SELECT {agg}({col}) FROM {main}\"\n",
        "\n",
        "    elif intent == \"WHERE\":\n",
        "        col = random.choice(db[\"numeric\"][main])\n",
        "        val = random.choice([10,20,50,100])\n",
        "        q = f\"get {col} from {main} where {col} > {val}\"\n",
        "        sql = f\"SELECT {col} FROM {main} WHERE {col} > {val}\"\n",
        "\n",
        "    elif intent == \"GROUP\":\n",
        "        agg = random.choice([\"COUNT\",\"AVG\"])\n",
        "        g = random.choice(db[\"text\"][main])\n",
        "        n = random.choice(db[\"numeric\"][main])\n",
        "        q = f\"get {agg.lower()} {n} grouped by {g}\"\n",
        "        sql = f\"SELECT {g}, {agg}({n}) FROM {main} GROUP BY {g}\"\n",
        "\n",
        "    else:\n",
        "        t1,t2,c1,c2 = db[\"join\"]\n",
        "        q = f\"show {t1} and {t2} names\"\n",
        "        sql = f\"SELECT {t1}.name, {t2}.name FROM {t1} JOIN {t2} ON {t1}.{c1} = {t2}.{c2}\"\n",
        "\n",
        "    return {\"question\": q, \"schema\": schema, \"sql\": sql}\n",
        "\n",
        "dataset = [generate_example() for _ in range(80000)]\n",
        "\n",
        "# ============================================================\n",
        "# VOCAB BUILDING\n",
        "# ============================================================\n",
        "\n",
        "ENC_VOCAB = {\"<PAD>\":0,\"<UNK>\":1,\"<SEP>\":2}\n",
        "DEC_VOCAB = {\"<PAD>\":0,\"<UNK>\":1,\"<BOS>\":2,\"<EOS>\":3}\n",
        "\n",
        "def add(v,t):\n",
        "    if t not in v: v[t]=len(v)\n",
        "\n",
        "for ex in dataset:\n",
        "    for t in ex[\"question\"].lower().split(): add(ENC_VOCAB,t)\n",
        "    for tb,cs in ex[\"schema\"].items():\n",
        "        add(ENC_VOCAB,tb)\n",
        "        for c in cs: add(ENC_VOCAB,f\"{tb}.{c}\")\n",
        "    for t in ex[\"sql\"].lower().split(): add(DEC_VOCAB,t)\n",
        "\n",
        "# ============================================================\n",
        "# DATASET CLASS (SCHEMA SHUFFLING)\n",
        "# ============================================================\n",
        "\n",
        "class NL2SQLDataset(Dataset):\n",
        "    def __init__(self,data):\n",
        "        self.data=data\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        ex=self.data[i]\n",
        "        q = ex[\"question\"].lower().split()\n",
        "\n",
        "        schema_tokens=[\n",
        "            f\"{t}.{c}\" for t,cs in ex[\"schema\"].items() for c in cs\n",
        "        ]\n",
        "        random.shuffle(schema_tokens)  # ðŸ”¥ CRITICAL\n",
        "\n",
        "        src = q + [\"<SEP>\"] + schema_tokens\n",
        "        src_ids=[ENC_VOCAB.get(t,1) for t in src][:80]\n",
        "        src_ids += [0]*(80-len(src_ids))\n",
        "\n",
        "        tgt=[DEC_VOCAB[\"<BOS>\"]] + \\\n",
        "            [DEC_VOCAB.get(t,1) for t in ex[\"sql\"].lower().split()] + \\\n",
        "            [DEC_VOCAB[\"<EOS>\"]]\n",
        "        tgt=tgt[:60]\n",
        "        tgt+=[0]*(60-len(tgt))\n",
        "\n",
        "        return torch.tensor(src_ids), torch.tensor(tgt)\n",
        "\n",
        "train_data, val_data = train_test_split(dataset, test_size=0.15, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(NL2SQLDataset(train_data), batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(NL2SQLDataset(val_data), batch_size=128)\n",
        "\n",
        "# ============================================================\n",
        "# MODEL (REDUCED CAPACITY)\n",
        "# ============================================================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,v):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(v,192,padding_idx=0)\n",
        "        layer=nn.TransformerEncoderLayer(192,6,768,batch_first=True)\n",
        "        self.enc=nn.TransformerEncoder(layer,3)\n",
        "\n",
        "    def forward(self,x):\n",
        "        mask=(x==0)\n",
        "        e=self.emb(x)\n",
        "        e=nn.functional.dropout(e,0.1,self.training)\n",
        "        return self.enc(e,src_key_padding_mask=mask)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,v):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(v,192)\n",
        "        layer=nn.TransformerDecoderLayer(192,6,768,dropout=0.2,batch_first=True)\n",
        "        self.dec=nn.TransformerDecoder(layer,3)\n",
        "        self.fc=nn.Linear(192,v)\n",
        "\n",
        "    def forward(self,y,mem,mask):\n",
        "        L=y.size(1)\n",
        "        causal=torch.triu(torch.ones(L,L,device=y.device),1).bool()\n",
        "        e=self.emb(y)\n",
        "        o=self.dec(e,mem,tgt_mask=causal,memory_key_padding_mask=mask)\n",
        "        return self.fc(o)\n",
        "\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "enc,dec=Encoder(len(ENC_VOCAB)).to(device),Decoder(len(DEC_VOCAB)).to(device)\n",
        "\n",
        "opt=optim.Adam(list(enc.parameters())+list(dec.parameters()),lr=1e-4)\n",
        "loss_fn=nn.CrossEntropyLoss(ignore_index=0,label_smoothing=0.1)\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING + VALIDATION\n",
        "# ============================================================\n",
        "\n",
        "def eval_loss():\n",
        "    enc.eval(); dec.eval(); tot=0\n",
        "    with torch.no_grad():\n",
        "        for x,y in val_loader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            mem=enc(x)\n",
        "            out=dec(y[:,:-1],mem,(x==0))\n",
        "            loss=loss_fn(out.reshape(-1,len(DEC_VOCAB)),y[:,1:].reshape(-1))\n",
        "            tot+=loss.item()\n",
        "    return tot/len(val_loader)\n",
        "\n",
        "print(\"ðŸš€ Training\")\n",
        "best=1e9\n",
        "for e in range(2):\n",
        "    enc.train(); dec.train()\n",
        "    for x,y in train_loader:\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        opt.zero_grad()\n",
        "        mem=enc(x)\n",
        "        out=dec(y[:,:-1],mem,(x==0))\n",
        "        loss=loss_fn(out.reshape(-1,len(DEC_VOCAB)),y[:,1:].reshape(-1))\n",
        "        loss.backward(); opt.step()\n",
        "    v=eval_loss()\n",
        "    print(f\"Epoch {e+1} | Val Loss {v:.4f}\")\n",
        "    if v>best: break\n",
        "    best=v\n",
        "\n",
        "# ============================================================\n",
        "# SAVE MODEL\n",
        "# ============================================================\n",
        "\n",
        "torch.save({\n",
        "    \"enc\":enc.state_dict(),\n",
        "    \"dec\":dec.state_dict(),\n",
        "    \"ENC_VOCAB\":ENC_VOCAB,\n",
        "    \"DEC_VOCAB\":DEC_VOCAB\n",
        "},\"nl2sql_fixed.pt\")\n",
        "\n",
        "print(\"âœ… Model saved: nl2sql_fixed.pt\")\n"
      ],
      "metadata": {
        "id": "1_6u0ezDSTiL",
        "outputId": "844f5c42-de83-4e30-f867-af494ba678bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training\n",
            "Epoch 1 | Val Loss 0.7518\n",
            "Epoch 2 | Val Loss 0.7433\n",
            "âœ… Model saved: nl2sql_fixed.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# NL â†’ SQL INFERENCE WITH PICARD\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---------------- LOAD MODEL ----------------\n",
        "ckpt = torch.load(\"nl2sql_fixed.pt\", map_location=\"cpu\")\n",
        "\n",
        "ENC_VOCAB = ckpt[\"ENC_VOCAB\"]\n",
        "DEC_VOCAB = ckpt[\"DEC_VOCAB\"]\n",
        "inv_dec_vocab = {v: k for k, v in DEC_VOCAB.items()}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------------- MODEL DEFINITIONS ----------------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, 192, padding_idx=0)\n",
        "        layer = nn.TransformerEncoderLayer(192, 6, 768, batch_first=True)\n",
        "        self.enc = nn.TransformerEncoder(layer, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = (x == 0)\n",
        "        return self.enc(self.emb(x), src_key_padding_mask=mask)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, 192)\n",
        "        layer = nn.TransformerDecoderLayer(\n",
        "            192, 6, 768, dropout=0.2, batch_first=True\n",
        "        )\n",
        "        self.dec = nn.TransformerDecoder(layer, 3)\n",
        "        self.fc = nn.Linear(192, vocab)\n",
        "\n",
        "    def forward(self, y, mem, mem_mask):\n",
        "        L = y.size(1)\n",
        "        causal = torch.triu(torch.ones(L, L, device=y.device), 1).bool()\n",
        "        out = self.dec(self.emb(y), mem,\n",
        "                       tgt_mask=causal,\n",
        "                       memory_key_padding_mask=mem_mask)\n",
        "        return self.fc(out)\n",
        "\n",
        "# ---------------- INIT MODELS ----------------\n",
        "enc = Encoder(len(ENC_VOCAB)).to(device)\n",
        "dec = Decoder(len(DEC_VOCAB)).to(device)\n",
        "enc.load_state_dict(ckpt[\"enc\"])\n",
        "dec.load_state_dict(ckpt[\"dec\"])\n",
        "enc.eval(); dec.eval()\n",
        "\n",
        "# ============================================================\n",
        "# ðŸ”’ PICARD CONSTRAINTS\n",
        "# ============================================================\n",
        "\n",
        "SQL_KEYWORDS = {\"select\", \"from\", \"where\", \"join\", \"group\", \"by\"}\n",
        "\n",
        "def is_valid_next_token(prev_tokens, next_token):\n",
        "    pt = [t.lower() for t in prev_tokens]\n",
        "    nt = next_token.lower()\n",
        "\n",
        "    # Must start with SELECT\n",
        "    if len(pt) == 0:\n",
        "        return nt == \"select\"\n",
        "\n",
        "    # FROM must come after SELECT\n",
        "    if nt == \"from\" and \"select\" not in pt:\n",
        "        return False\n",
        "\n",
        "    # WHERE / GROUP must come after FROM\n",
        "    if nt in {\"where\", \"group\"} and \"from\" not in pt:\n",
        "        return False\n",
        "\n",
        "    # BY must follow GROUP\n",
        "    if nt == \"by\" and (len(pt) == 0 or pt[-1] != \"group\"):\n",
        "        return False\n",
        "\n",
        "    # JOIN must come after FROM\n",
        "    if nt == \"join\" and \"from\" not in pt:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# ============================================================\n",
        "# ðŸ”® INFERENCE FUNCTION (PICARD)\n",
        "# ============================================================\n",
        "\n",
        "def infer_sql(question, schema, max_len=60):\n",
        "    # ---- encoder input ----\n",
        "    tokens = question.lower().split() + [\"<SEP>\"] + [\n",
        "        f\"{t}.{c}\" for t, cols in schema.items() for c in cols\n",
        "    ]\n",
        "\n",
        "    src_ids = [ENC_VOCAB.get(t, ENC_VOCAB[\"<UNK>\"]) for t in tokens][:80]\n",
        "    src_ids += [ENC_VOCAB[\"<PAD>\"]] * (80 - len(src_ids))\n",
        "\n",
        "    x = torch.tensor(src_ids).unsqueeze(0).to(device)\n",
        "    src_mask = (x == ENC_VOCAB[\"<PAD>\"])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        memory = enc(x)\n",
        "\n",
        "        y = torch.tensor([[DEC_VOCAB[\"<BOS>\"]]], device=device)\n",
        "        generated = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            logits = dec(y, memory, src_mask)\n",
        "            probs = logits[:, -1].softmax(dim=-1)\n",
        "\n",
        "            sorted_ids = torch.argsort(probs, descending=True)[0]\n",
        "            next_id = None\n",
        "\n",
        "            # ðŸ”¥ PICARD FILTERING\n",
        "            for tid in sorted_ids:\n",
        "                tok = inv_dec_vocab[tid.item()]\n",
        "                if tok in {\"<PAD>\", \"<BOS>\"}:\n",
        "                    continue\n",
        "                if is_valid_next_token(generated, tok):\n",
        "                    next_id = tid\n",
        "                    break\n",
        "\n",
        "            if next_id is None:\n",
        "                next_id = sorted_ids[0]\n",
        "\n",
        "            token = inv_dec_vocab[next_id.item()]\n",
        "            if token == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "            generated.append(token)\n",
        "            y = torch.cat([y, next_id.view(1,1)], dim=1)\n",
        "\n",
        "    return \" \".join(generated)\n",
        "\n",
        "# ============================================================\n",
        "# ðŸ”¥ TEST EXAMPLES\n",
        "# ============================================================\n",
        "\n",
        "schema1 = {\n",
        "    \"employees\": [\"id\",\"name\",\"salary\",\"dept_id\"],\n",
        "    \"departments\": [\"id\",\"name\"]\n",
        "}\n",
        "\n",
        "schema2 = {\n",
        "    \"students\": [\"id\",\"name\",\"marks\",\"class_id\"],\n",
        "    \"classes\": [\"id\",\"name\"]\n",
        "}\n",
        "\n",
        "print(infer_sql(\"show dept_id of employees\", schema1))\n",
        "print(infer_sql(\"show salary sum from employees\", schema1))\n",
        "print(infer_sql(\"get name from students where marks > 20\", schema2))\n",
        "print(infer_sql(\"show students and classes names\", schema2))\n"
      ],
      "metadata": {
        "id": "7tbFCRTLFR05",
        "outputId": "73a3e326-a1a1-467f-b084-b3d09a6c2485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "select dept_id from employees\n",
            "select sum(salary) from employees\n",
            "select marks from students where marks > 20\n",
            "select students.name, classes.name from students join classes on students.class_id = classes.id\n"
          ]
        }
      ]
    }
  ]
}